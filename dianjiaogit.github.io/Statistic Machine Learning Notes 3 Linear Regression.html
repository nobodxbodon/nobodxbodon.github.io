<h2><a href="https://github.com/dianjiaogit/dianjiaogit.github.io/blob/master/_posts/2021-03-01-SML notes.md">仓库源文</a>，<a href="https://dianjiaogit.github.io/notes/statistic%20mchine%20learning/2021/03/01/SML-notes">站点原文</a></h2>
<hr/>
<p>layout: post
title:  "Statistic Machine Learning Notes 3 Linear Regression"
date:   2021-3-1 07:00:00 +0800</p>
<h2>categories: [Notes, Statistic Mchine Learning]</h2>
<h1>Linear Models for Regression</h1>
<p>y(x, w) = w0 + w1x1 + ... + wDxD<br/>
w: bias parameter<br/>
x: input (independent with each other)</p>
<h2>Occam's Razor</h2>
<p>Being simple.</p>
<p>Goal is to minimize squared errors and analytic solution<br/>
convex losses and regularizers</p>
<h3>Conventions</h3>
<p>See X as a matrix, rows are data points, columns are input dimentions.</p>
<h2>Feature Functions</h2>
<h3>Polunomial Basis Functions</h3>
<p>Infinite magnitude. Extrapolate poorly</p>
<h3>Gaussian Basis Functions</h3>
<p>Magnitude bounded. Would not vanish.</p>
<h3>Sigmoidal Basis Functions</h3>
<p>Hyperbolic tangent.</p>
<h2>Define Likelihood</h2>
<p>beta: percision</p>
<p>Sum-of-squares error function: E_D(w)</p>
<p>Maximise Likelihood function = Minimise Error function</p>
<p>Then find the stationary point.</p>
<p>Batch learning</p>
