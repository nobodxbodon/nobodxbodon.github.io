<h2><a href="https://github.com/caol64/caol64.github.io/blob/master/content/posts/2024/2024-02-27-running-a-large-language-model-locally.en.md">仓库源文</a>，<a href="https://babyno.top/posts/2024/02/27/running-a-large-language-model-locally.en">站点原文</a></h2>
<hr/>
<p>author: caol64
title: Run a large language models locally
slug: running-a-large-language-model-locally
description: This guide shows you how to run a large language model (LLM) directly on your computer! It's free, protects your privacy, and lets you try various open-source models.
date: 2024-02-27 12:22:33
draft: false
ShowToc: true
TocOpen: true
tags:</p>
<ul>
<li>Ollama</li>
<li>AI
categories:</li>
<li>Tutorial</li>
</ul>
<hr/>
<p>With the rise of ChatGPT, LLMs (Large Language Models) have become a hot topic in the field of artificial intelligence and natural language processing. In this article, I will walk you through the process of running a large language model on your own personal computer.</p>
<h2>Pros and Cons</h2>
<p>There are many advantages to running a large language model locally:</p>
<ul>
<li><strong>Privacy protection</strong></li>
<li><strong>No expensive costs</strong></li>
<li>Ignore network problems</li>
<li>Try out various open source models</li>
</ul>
<p>I think the first two advantages are enough for everyone to try. Everyone has some private data that they don't want to send to third parties. If you can use AI to process private data locally or even offline, isn't that perfect? In addition, locally, no matter how much data you run, you don't need to pay for API and token fees. Are you excited?</p>
<p>Of course, there are also disadvantages:</p>
<ul>
<li>Not for beginners</li>
<li>The "intelligence" of open source models is worrying compared to commercial models</li>
<li>Personal computer configuration is weak, it is impossible to run the full power of the model</li>
</ul>
<p>But fortunately, there are many tools in the open source world, and "intelligence" can be slowly "trained" through tools. I believe that as long as you are willing to try, you will be able to train a model that you are satisfied with.</p>
<h2>Tool Selection</h2>
<p>I will mainly introduce 3 tools for running LLMs locally:</p>
<ul>
<li>LMStudio: <a href="https://lmstudio.ai/">https://lmstudio.ai/</a></li>
<li>llamafile: <a href="https://github.com/Mozilla-Ocho/llamafile">https://github.com/Mozilla-Ocho/llamafile</a></li>
<li>Ollama: <a href="https://ollama.com/">https://ollama.com/</a></li>
</ul>
<p>I have used all three of these tools, and the usage methods are very similar. The installation is also very simple. If you are interested, you can try them all. But in terms of stability and convenience, I would recommend <code>Ollama</code>.</p>
<h2>Running Ollama</h2>
<h3>Installation</h3>
<p>The installation of <code>Ollama</code> can be downloaded from the official website. Just download the corresponding installation package according to your operating system. Although the installation package is provided, there is no GUI interface. All operations need to be performed by command line.</p>
<h3>Pull Model</h3>
<p>The first step after installation is to pull a model. The command is very similar to the <code>docker</code> command:</p>
<pre><code class="language-shell"># Install model
ollama pull llama2

# Delete model
ollama rm llama2
</code></pre>
<p>Speaking of models, it's very interesting. First of all, you can go to the official website: <a href="https://ollama.com/library">https://ollama.com/library</a> to find and download the model you like. At present, major companies have more or less open sourced their own large language models. You should have heard of many of them in various news reports. But if you are a beginner, I recommend a few models here:</p>
<ul>
<li>llama2 (a large language model released by Meta, Facebook's parent company)</li>
<li>mistral (a large language model released by a French AI company)</li>
<li>qwen (a large language model released by Alibaba, you have heard of Tongyi Qianwen, right?)</li>
<li>llava (a large language model that can perform image recognition)</li>
</ul>
<p>Why is it interesting? Because these models open sourced by these big companies are not their strongest models, in other words, they are all reserved. Because the strongest models are all reserved for their own commercial use. This means that these models are more or less "retarded" when they are first obtained, and they cannot understand your intentions and give you good answers at first. This is also the reason why I said later that it needs to be "trained".</p>
<h3>Model Parameters</h3>
<p>The model pulled by the command <code>ollama pull llama2</code> is the default parameter. If you have requirements for the parameters, you can click the <code>Tags</code> tag of the model and choose the appropriate parameters yourself.</p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/babyno.top/content/imgs/posts/2024-02-27-running-a-large-language-model-locally/1.webp"/></p>
<p>Here is a brief explanation of the main parameters:</p>
<ul>
<li><p><code>2b, 7b, 13b</code>
- The number of parameters of the model during training, b stands for 100 million. The larger the result, the more accurate, the more resources it will occupy, and the longer it will take to generate the result. 7B requires at least 8GB of memory, and 13B requires at least 16GB of memory.</p>
</li>
<li><p><code>instruct, chat, text</code></p>
<ul>
<li>instruct, chat is more suitable for chatting, text is more suitable for content generation (see [Ollama API](#Ollama Api)).</li>
</ul>
</li>
<li><p><code>q2, q4, q8</code>
- Model quantization value, the larger the more accurate, but the more memory it occupies, The same time to generate the result will also be longer. These parameters can be selected according to your computer configuration. My <code>M1</code> chip MacBook generally chooses <code>7b_q8</code>.</p>
</li>
</ul>
<h3>Running</h3>
<p>After pulling the model, you can use the <code>ollama list</code> command to view all installed models. Then you can run <code>Ollama</code>.</p>
<p>There are two ways to run <code>Ollama</code>. One is the command line mode, enter <code>ollama serve</code> to start the service. The other is to click on the application shortcut to run. This method will display an <code>Ollama</code> icon in the status bar. No matter which method is used, after starting the service, it will occupy port <code>11434</code>.</p>
<p>At this time, you can connect Ollama to any <code>ChatGPT</code> client that supports modifying the API address (because the latest version of <code>Ollama</code> has adapted to <code>OpenAI's API</code>). In addition, some apps can also be directly configured if they are compatible with the <code>Llama API</code> interface:</p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/babyno.top/content/imgs/posts/2024-02-27-running-a-large-language-model-locally/2.webp"/></p>
<h3>Ollama API</h3>
<p>The API documentation is here: <a href="https://github.com/ollama/ollama/blob/main/docs/api.md">https://github.com/ollama/ollama/blob/main/docs/api.md</a>, I will only introduce two here.</p>
<pre><code>POST /api/generate
POST /api/chat
</code></pre>
<p>The first is content generation, and the second is chatting with the model. The two APIs require different parameters. You can refer to the documentation for details. Here, I will mainly talk about the difference between these two interfaces.</p>
<p>The <code>generate</code> interface emphasizes generation, so you need to provide all the prompts to it at once, so that you can generate a more ideal result.</p>
<p>The <code>chat</code> interface emphasizes chatting, which is also the scenario used by most <code>ChatGPT</code> clients. It chats with the model back and forth, and each chat will include all the previous contexts. Therefore, you can gradually give it prompts after chatting with the model a few times.</p>
<h2>Model Testing</h2>
<p>Next, let's write a script to test how different models perform in content generation.</p>
<p>First, install the corresponding package: <code>pip install ollama</code>.</p>
<p>Then write a Python script. This script defines several simple roles:</p>
<pre><code class="language-python">import ollama


roles = {
  'english_translator': 'Translate `%s` to English.',
  'chinese_translator': 'Translate `%s` to Chinese.',
  'supervisor': "Explain `%s` like I'm a 5 year old.",
  'professional': "Explain `%s` as a professional.",
  'generator': "%s",
  'content_creator': 'Expand your writing according to the prompts given: `%s`',
}

models = {
  'mistral': 'mistral:7b-instruct-v0.2-q8_0',
  'qwen': 'qwen:7b-q8_0',
  'gemma': 'gemma:7b-instruct-q8_0',
  'llama2': 'llama2-uncensored:7b-chat-q8_0',
  'llava': 'llava:7b-v1.6-mistral-q8_0',
  'codellama': 'codellama:7b-instruct-q8_0',
}

class Assistant:
  def __init__(self, model_key, role_key) -&gt; None:
    self.role_prompt = roles[role_key]
    self.model = models[model_key]


  def work(self, user_prompt):
    print(user_prompt)
    response = ollama.generate(
      model=self.model,
      # format='json',
      options={'temperature': 0.7},
      prompt=self.role_prompt % user_prompt,
      stream=True
    )

    for chunk in response:
      print(chunk['response'], end='', flush=True)


if __name__ == "__main__":
  assistant = Assistant('llama2', 'chinese_translator')
  assistant.work("Note: It is important to wear protective gear while handling these ingredients as they are highly corrosive and can cause serious burns if not handled properly.")
</code></pre>
<p>This script is very simple, you can run it on your own computer. Through my observation, the conclusion is that most language models do not support Chinese well. Chinese-to-English translation is acceptable, but English-to-Chinese and Japanese-to-Chinese translation are often nonsensical. "Tongyi Qianwen" performs the best in terms of Chinese support. However, the answers from "Tongyi Qianwen" always seem to go off on strange tangents, which can be disappointing.</p>
<p>Here are the answers from 3 models for the prompt "The Four Great Classical Novels of China":</p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/babyno.top/content/imgs/posts/2024-02-27-running-a-large-language-model-locally/1.gif"/></p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/babyno.top/content/imgs/posts/2024-02-27-running-a-large-language-model-locally/2.gif"/></p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/babyno.top/content/imgs/posts/2024-02-27-running-a-large-language-model-locally/3.gif"/></p>
<p>As you can see, it seems like it's time to further train these models!</p>
