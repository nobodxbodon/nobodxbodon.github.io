<h2><a href="https://github.com/caol64/caol64.github.io/blob/master/content/posts/2024/2024-04-19-run-stable-diffusion-locally.md">仓库源文</a>，<a href="https://babyno.top/posts/2024/04/19/run-stable-diffusion-locally">站点原文</a></h2>
<hr/>
<p>author: "路边的阿不"
title: 在本地跑一个AI模型(5) - Stable Diffusion
slug: run-stable-diffusion-locally
description: "Learn how to run Stable Diffusion locally using <code>diffusers</code> and generate stunning images from text prompts. Discover the basics of text-to-image synthesis, including model selection, prompt engineering, and image generation. Get started with this comprehensive guide and unlock the power of AI-generated art."
date: 2024-04-19 10:17:47
draft: false
ShowToc: true
TocOpen: true
tags:</p>
<ul>
<li>Stable-Diffusion</li>
<li>AI
categories:</li>
<li>教程</li>
</ul>
<hr/>
<p>在之前的文章中，我们使用<a href="https://babyno.top/tags/ollama/"><code>ollama</code></a>在本地运行了大语言模型，它可以与你聊天，帮助你理解和生成文本内容。使用<a href="https://babyno.top/tags/coqui/"><code>coqui-tts</code></a>在本地运行了文本转语音模型，它可以将大语言模型生成的文字转换成语音，让你的应用更有趣。今天我们将要介绍<code>Stable Diffusion</code>，一种扩散神经网络的深度学习模型，使用它可以生成各种不可思议的图片。</p>
<p>我们使用的工具是<code>huggingface</code>提供的<code>diffusers</code>，一个在纯<code>python</code>环境下运行的库。废话不多说，我们进入今天的教程。</p>
<h2>安装</h2>
<p><code>diffusers</code>目前不支持<code>python 3.12</code>，因此我们使用虚拟环境来安装。</p>
<pre><code class="language-shell"># 使用3.10版本的python创建venv
/opt/homebrew/opt/python@3.10/libexec/bin/python3 -m venv .venv
# 激活venv
source .venv/bin/activate 
</code></pre>
<p>安装<code>diffusers</code>及其依赖：</p>
<pre><code class="language-shell">pip install diffusers accelerate transformers
</code></pre>
<h2>下载模型</h2>
<p>和之前文章里介绍的一样，模型我们还是选择自己下载。你可以到<code>huggingface</code>网站下载已经训练好的模型，比如<a href="https://huggingface.co/runwayml/stable-diffusion-v1-5"><code>runwayml/stable-diffusion-v1-5</code></a>。</p>
<blockquote>
<p>Tips：你可以使用如下命令下载<code>huggingface</code>上的模型：</p>
</blockquote>
<pre><code class="language-shell">git lfs install
git clone git@hf.co:&lt;MODEL ID&gt; # example: git clone git@hf.co:bigscience/bloom
</code></pre>
<p>此外，<code>diffusers</code>支持<code>AUTOMATIC1111</code>的模型，因此你可以去<a href="https://civitai.com/">Civitai</a>下载各种<code>CheckPoint</code>和<code>LoRA</code>。本文使用的模型就是<code>Civitai</code>的<a href="https://civitai.com/models/112902/dreamshaper-xl">dreamshaper</a>。选择模型时要注意以下几点：</p>
<ul>
<li>根据自己的喜好选择模型的风格，比如“写实”、“动漫”或者“魔幻”</li>
<li>模型有一个属性是“基础模型”，如下图。对应的诸如<code>SD 1.5</code>、<code>SDXL 1.0</code>、<code>SDXL Turbo</code>等等。<code>SD</code>系列只能生成<code>512x512</code>的图片，<code>SDXL</code>系列可以生成<code>1024x1024</code>及以上的图片。而<code>Turbo</code>系列可以将生成所需的时间缩短。根据自己的电脑配置选择合适的模型吧。</li>
</ul>
<p><img alt="Civitai Model" src="/media/wwww/share/study/聚聚/源数据/博客聚合/babyno.top/content/imgs/posts/2024-04-19-run-stable-diffusion-locally/1.jpg"/></p>
<p>选择好模型后，就点击<code>Download</code>下载吧。</p>
<h2>加载模型</h2>
<p>如果你的模型是单个<code>safetensors</code>格式的，使用<code>from_single_file</code>加载，如果是从<code>huggingface</code>下载的预训练模型，使用<code>from_pretrained</code>加载。此外如果你的模型是<code>SDXL</code>，使用<code>StableDiffusionXLPipeline</code>，因此<code>dreamshaperXL</code>模型加载的代码如下：</p>
<pre><code class="language-python">from diffusers import StableDiffusionXLPipeline

pipe = StableDiffusionXLPipeline.from_single_file("your/path/dreamshaperXL_v21TurboDPMSDE.safetensors")
</code></pre>
<h2>使用GPU运行</h2>
<p><code>windows</code>用户可以根据如下代码判断自己的电脑能否进行<code>GPU</code>推理：</p>
<pre><code class="language-python">device = "cuda" if torch.cuda.is_available() else "cpu"
</code></pre>
<p><code>MAC</code>的<code>M1</code>和<code>M2</code>芯片可以使用如下代码：</p>
<pre><code class="language-python">device = 'mps'
</code></pre>
<p>然后：</p>
<pre><code class="language-python">pipe = pipe.to(device)
</code></pre>
<h2>调度器</h2>
<p><code>diffusers</code>的调度器对应的是<code>AUTOMATIC1111</code>中的<code>Sampling method</code>，它对获得高质量的图像至关重要。<code>Sampling method</code>和<code>diffusers</code>的调度器的对应关系可以参照<a href="https://huggingface.co/docs/diffusers/v0.27.2/en/api/schedulers/overview">此处</a>。</p>
<p><img alt="Diffusers Scheduler" src="/media/wwww/share/study/聚聚/源数据/博客聚合/babyno.top/content/imgs/posts/2024-04-19-run-stable-diffusion-locally/2.jpg"/></p>
<p>至于如何选择调度器，在模型的详情页可以找到作者给出的建议，比如：</p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/babyno.top/content/imgs/posts/2024-04-19-run-stable-diffusion-locally/3.jpg"/></p>
<p>这里作者建议的是<code>DPM++ SDE Karras</code>，可以参照上面的对应表找到对应的调度器为<code>DPMSolverSinglestepScheduler</code>，初始化参数为<code>use_karras_sigmas=True</code>。</p>
<p>调度器代码如下：</p>
<pre><code class="language-python">pipe.scheduler = DPMSolverSinglestepScheduler.from_config(pipe.scheduler.config, use_karras_sigmas=True)
</code></pre>
<h2>提示词</h2>
<p>提示词的质量对最终生成的图像质量有很大的影响。提示词的写法这里不做展开，提示词的例子：</p>
<pre><code class="language-python">prompt = "masterpiece, cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney"

negative_prompt = "worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed"
</code></pre>
<p>提示词目前有77个长度的限制，要突破这个限制，可以将提示词向量化，以下是代码例子：</p>
<pre><code class="language-shell">pip install compel
</code></pre>
<pre><code class="language-python">compel = Compel(
    tokenizer=[pipe.tokenizer, pipe.tokenizer_2] ,
    text_encoder=[pipe.text_encoder, pipe.text_encoder_2],
    returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,
    requires_pooled=[False, True]
)

conditioning, pooled = compel(prompt)
negative_prompt_embeds, negative_pooled = compel(negative_prompt)
</code></pre>
<h2>图片生成</h2>
<pre><code class="language-python">image = pipe(
            prompt_embeds = conditioning,
            pooled_prompt_embeds=pooled,
            negative_prompt_embeds = negative_prompt_embeds,
            negative_pooled_prompt_embeds=negative_pooled,
            # height=800,
            # width=512,
            num_inference_steps=6,
            guidance_scale=2,
            strength=0.5
        ).images[0]

image.save("data/out.jpg")
</code></pre>
<p>这里对图片生成质量有影响的几个参数是<code>guidance_scale</code>和<code>num_inference_steps</code>，这两个参数分别对应<code>AUTOMATIC1111</code>里的<code>CFG Scale</code>和<code>Sampling steps</code>。你也可以在模型的详情页找到作者给出的建议：</p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/babyno.top/content/imgs/posts/2024-04-19-run-stable-diffusion-locally/4.jpg"/></p>
<p>至此，运行代码，你应该可以获得模型生成的图片了。</p>
<h2>总结</h2>
<p>本文介绍了使用<code>diffusers</code>在本地运行<code>Stable Diffusion</code>的方法，并进行了一次基本的<code>Text to Image</code>的实践。下篇文章将继续介绍<code>diffusers</code>使用<code>LoRA</code>，<code>ControlNet</code>和<code>Adapter</code>生成高级图片的实践。</p>
