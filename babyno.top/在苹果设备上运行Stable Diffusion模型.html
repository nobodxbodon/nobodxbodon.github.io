<h2><a href="https://github.com/caol64/caol64.github.io/blob/master/content/posts/2024/2024-06-28-run-the-stable-diffusion-model-on-apple-devices.md">仓库源文</a>，<a href="https://babyno.top/posts/2024/06/28/run-the-stable-diffusion-model-on-apple-devices">站点原文</a></h2>
<h2>模型类别</h2>
<p>首先要下载模型，<code>Stable Diffusion</code>模型可以在<a href="https://huggingface.co/models">huggingface</a>或者<a href="https://civitai.com/">Civitai</a>下载到。但是在这两个网站上下载的模型可能会有三种格式。</p>
<h3><code>CoreML</code>格式</h3>
<p>这种类别的模型较少，文件主要以<code>.mlmodelc</code>或<code>.mlmodel</code>为主，其文件结构大致为：</p>
<pre><code>├── TextEncoder.mlmodelc
├── TextEncoder2.mlmodelc
├── Unet.mlmodelc
├── VAEDecoder.mlmodelc
├── merges.txt
└── vocab.json
</code></pre>
<h3><code>Diffusers</code>格式</h3>
<p>在<code>huggingface</code>上下载的模型大多是这种类型，其文件结构大致为：</p>
<pre><code>├── model_index.json
├── scheduler
│   └── scheduler_config.json
├── text_encoder
│   ├── config.json
│   └── pytorch_model.bin
├── tokenizer
│   ├── merges.txt
│   ├── special_tokens_map.json
│   ├── tokenizer_config.json
│   └── vocab.json
├── unet
│   ├── config.json
│   └── diffusion_pytorch_model.bin
└── vae
    ├── config.json
    └── diffusion_pytorch_model.bin
</code></pre>
<h3><code>safetensors</code>格式</h3>
<p>在<code>Civitai</code>网站下载的大多是这种格式，就一个文件，非常方便。</p>
<h2>模型转换</h2>
<p>接下来，需要把下载下来的模型都转成<code>CoreML</code>格式，如果你在第一步下载的模型已经是<code>CoreML</code>格式，那么这一步就可以跳过。</p>
<h3><code>Diffusers</code>格式转<code>CoreML</code>格式</h3>
<p>首先下载该仓库代码：<a href="https://github.com/apple/ml-stable-diffusion">ml-stable-diffusion</a>。查看<code>System Requirements</code>检查自己的设备是否支持。然后安装依赖：</p>
<pre><code class="lang-shell">pip install -r requirements.txt
</code></pre>
<p>找到<code>torch2coreml.py</code>文件，执行以下命令：</p>
<pre><code class="lang-shell">python torch2coreml.py \
--bundle-resources-for-swift-cli \
--xl-version \
--convert-unet \
--convert-text-encoder \
--convert-vae-decoder \
--attention-implementation ORIGINAL \
--model-version /your/model/path \
-o /your/model/output/path
</code></pre>
<p>注意有个参数<code>--xl-version</code>，如果模型是<code>sdxl</code>类型的，就加上，否则把这行删除。另外如果你的模型支持图生图，你可以加上<code>--convert-vae-encoder</code>参数。</p>
<p>运行完该命令，应该在你指定的目录生成了文件，在<code>Resources</code>目录下的文件就是转换好的<code>CoreML</code>格式。</p>
<h3><code>safetensors</code>格式转<code>Diffusers</code>格式</h3>
<p>首先下载该仓库代码：<a href="https://github.com/huggingface/diffusers">Diffusers</a>。然后安装依赖：</p>
<pre><code class="lang-shell">pip install --upgrade diffusers
</code></pre>
<p>找到<code>convert_original_stable_diffusion_to_diffusers.py</code>文件并执行以下命令：</p>
<pre><code class="lang-shell">python convert_original_stable_diffusion_to_diffusers.py \
--checkpoint_path /your/model/path \
--dump_path /your/model/output/path \
--from_safetensors \
--half \
--device mps
</code></pre>
<p>这里<code>--half</code>表示转换时精度为<code>fp16</code>，<code>--device mps</code>表示模型使用<code>mps(GPU)</code>进行推理。</p>
<p>运行完该命令，会生成<code>Diffusers</code>格式的模型，再利用<code>Diffusers</code>格式转<code>CoreML</code>格式的步骤，将模型转换为<code>CoreML</code>格式。</p>
<h2><code>Swift</code>调用<code>Stable Diffusion</code>模型</h2>
<p>使用<code>Huggingface</code>提供的<a href="https://github.com/huggingface/swift-coreml-diffusers">swift-coreml-diffusers</a>库。</p>
<pre><code class="lang-swift">import StableDiffusion
import CoreML

// 初始化CoreML配置
let config = MLModelConfiguration()
// 运行在GPU上（MAC限定）
config.computeUnits = MLComputeUnits.cpuAndGPU
// 初始化pipeline
var pipeline = try StableDiffusionPipeline(resourcesAt: modelDirectory,
                                      controlNet: [],
                                      configuration: config,
                                      reduceMemory: diffusersConfig.reduceMemory)
let pipeline.loadResources()
// 初始化图片推理配置
var pipelineConfig = StableDiffusionPipeline.Configuration(prompt: prompt)
pipelineConfig.stepCount = stepCount
pipelineConfig.guidanceScale = cfgScale
pipelineConfig.schedulerType = scheduler
// 开始图片推理
let images = try pipeline.generateImages(configuration: pipelineConfig,
                                          progressHandler: { progress in
})
</code></pre>
<p><strong><em>我的<a href="https://github.com/caol64/aquarius-ai">AquariusAI</a>项目提供了示例代码。</em></strong></p>
<p><img alt="" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/babyno.top/content/imgs/posts/2024-06-28-run-the-stable-diffusion-model-on-apple-devices/2.webp"/></p>
<h2>最后</h2>
<p>那到底什么是<code>CoreML</code>呢？</p>
<blockquote><p>Core ML 是Apple Silicon芯片产品（包括macOS、iOS、watchOS 和 tvOS）中使用的机器学习框架，用于执行快速预测或推理，在边缘轻松集成预训练的机器学习模型，从而可以对设备上的实时图像或视频进行实时预测。</p>
<p>Core ML 通过利用 CPU、GPU 和 神经网络引擎 ，同时最大程度地减小内存占用空间和功耗，来优化设备端性能。 由于模型严格地在用户设备上，因此无需任何网络连接，这有助于保护用户数据的私密性和 App 的响应速度。</p>
</blockquote>
<p>简而言之，如果你的模型运行在<code>Silicon</code>芯片的苹果设备上，利用<code>Core ML</code>可以获得更快的性能和更低的内存及能耗。</p>
