<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2017/2017-10-12-AI--bi-ji--Week-07-Bayes-Nets.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2017/10/12/AI--bi-ji--Week-07-Bayes-Nets">站点原文</a></h2>
<hr/>
<h2>layout: post
title: "AI 笔记 Week 07 Bayes Nets"
date: "2017-10-12 02:24:19"
categories: 计算机科学
excerpt: "Week 07 This week you should watch Lesson 6, Bayes Nets, and read Chapte..."
auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p><strong>Week 07</strong></p>
<blockquote>
<p>This week you should watch Lesson 6, <a href="https://classroom.udacity.com/courses/ud954/lessons/6381509770/concepts/64119686570923">Bayes Nets</a>, and read Chapter 14 in AIMA (Russell &amp; Norvig).
&gt;
Assignment 3:  Bayes Nets Sampling
Due: October 10 at 11:59PM UTC-12 (<a href="https://www.timeanddate.com/time/zones/aoe">Anywhere on Earth</a> time)</p>
</blockquote>
<h1>Challenge Question</h1>
<p><img alt="Graduation and job offer" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-acc3dc26ae2db0b5.png"/></p>
<ul>
<li>given the Bayes net, what is the probability of P(O2|o1), that is the probability of the students get an offer from company  2 when s/he got an offer from company 1.</li>
</ul>
<p><img alt="Answer" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a0add7bc6201bfc7.png"/></p>
<h1>a simple Bayes net</h1>
<p><img alt="bayes net quiz" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-4e2de29fca94b896.png"/></p>
<ul>
<li>How many parameters (probabilities) are needed to specify the Bayes network above? (answer is 3)</li>
</ul>
<h1>Computing Bayes Rule</h1>
<p><img alt="normalizing pseudo probablility" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-e59aa7e780774276.png"/></p>
<ul>
<li>the Bayes net can be calculated without knowing P(B) which is hard to calculate at first place.</li>
<li>Since we know P(A|B) + P(-A|B) = 1, we can use the pseudo probability method. We only calculate the numerator of the Bayse rule and normalize the result with a normalization factor.</li>
</ul>
<h2>Two Test Cancer example</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-d435df9e41c311a3.png"/></p>
<ul>
<li>note here <code>P(C|+,+) =P(c, +,+)/[P(c,+,+) + P(-c,+,+)] </code></li>
</ul>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-4e882b8d32bbc817.png"/></p>
<ul>
<li>simillary,  <code>P(C|+,-) =P(c, +,-)/[P(c,+,-) + P(-c,+,-)] </code></li>
</ul>
<h1>conditional independence</h1>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-11c8617a9e0ac4fc.png"/></p>
<ul>
<li>When C is known, T1 and T2 are independent.</li>
<li>what if we don't know C?</li>
</ul>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-996ac0d34fea806d.png"/></p>
<ul>
<li>the answer is that not knowing C, T1 and T2 might not be independent.</li>
</ul>
<p><img alt="Quiz on Conditional independece" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-6f492632aa91cecc.png"/></p>
<ul>
<li>the hard part for me is to come up with <strong>P(+&lt;sub&gt;2&lt;/sub&gt;|+&lt;sub&gt;1&lt;/sub&gt;)=P(+&lt;sub&gt;2&lt;/sub&gt;|+&lt;sub&gt;1&lt;/sub&gt;,C)P(c|+&lt;sub&gt;1&lt;/sub&gt;) + P(+&lt;sub&gt;2&lt;/sub&gt;|+&lt;sub&gt;1&lt;/sub&gt;,-C)P(-C|+&lt;sub&gt;1&lt;/sub&gt;)</strong></li>
<li><em>the calculation of P(C|+&lt;sub&gt;1&lt;/sub&gt;) is covered in the lecture of Week 06.</em> <strong>P(C|+&lt;sub&gt;1&lt;/sub&gt;) = P(+&lt;sub&gt;1&lt;/sub&gt;|C)P(C)/P(+&lt;sub&gt;1&lt;/sub&gt;)</strong></li>
</ul>
<h2>the relationship between absolute and conditional independence</h2>
<p><img alt="quiz" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-c0a009151185eed1.png"/></p>
<h2>Independence</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-fb959c8e1d80a707.png"/></p>
<ul>
<li>S and R (Sunny and Raise) are not related in the Bayes Network</li>
</ul>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-f1ecc80bcbf18c19.png"/></p>
<ul>
<li>still the first part of the Bayes rule, I did not get</li>
<li>the point here that the lecture is trying to make is that P(R|S) = P(R) since R and S are independent</li>
<li>this is called <strong>Away effect</strong></li>
</ul>
<p><img alt="quiz: calculate P(R|H)" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-81fa0b547b1ec844.png"/></p>
<ul>
<li>apply Bayes rule so that P(R|H) = P(H|R)*P(R)/P(H), P(H) is known</li>
<li>calculate P(H) is the key here</li>
<li>P(H|R) is easy, the steps are similar to the calculation of P(H)</li>
</ul>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-c3d7beec572d0a85.png"/></p>
<ul>
<li>Similar to the P(R|S,H)</li>
</ul>
<h2>Conditional Dependence</h2>
<p><img alt="Conditional Dependence" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-d33d611211d3cf6d.png"/></p>
<ul>
<li>we know that R and S are independent, but adding the information about H, it created a dependence between the two. E.g. being happy in a not-Sunny day increases the probability of raise.</li>
</ul>
<p><img alt="independence does not imply conditional independence" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-53db00389d1219b7.png"/></p>
<h1>general definition of Bayes Network</h1>
<p><img alt="Bayes Network requires fewer parameters to represent the probability distributions" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-efd22bd3ca72d93e.png"/></p>
<p><img alt="quiz: Number of parameters needed" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-9f7e43d8edcfc3f8.png"/></p>
<p><img alt="quiz: Number of parameters needed 2" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-f13b66b2d857f21d.png"/></p>
<p><img alt="quiz: Value Of A Network 3" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-0f71f6425a9931a3.png"/></p>
<ul>
<li>the quizzes above showed the advantage of Bayes network v.s. unstructured networks. Bayes net is more concise.</li>
</ul>
<h1>D Separation</h1>
<p><img alt="quiz:D Separation1" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-4096b2dd25184b23.png"/></p>
<ul>
<li>A and C are not independent because A can determine B and then determine A through B. But if B is known, then A becauses irrelevant.</li>
<li>C and D is not independent because knowing D can infer the knowledge of A and then can influence C. But if A is known, then D can not inference A or C anymore.</li>
<li>E and C are similar to C and D, they are independent if A, B, or D is known. if A, B, D is unknown, the C and E can be dependent.</li>
</ul>
<p><img alt="Quiz: D Separation2" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-31c4a53909fa38d0.png"/></p>
<ul>
<li>A matters in determine E so does B. Because Both A and B can influence C and C influence E. If C is known, A or B becomes irrelevant to E.</li>
<li>A and B are independent but if C is known, we can use A to infer B or vice versa. they are then not independent anymore in the case that C is known. <strong>This is the explain away effect</strong>.</li>
</ul>
<p><img alt="D Separation Rules" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a9cb571005b7d8f8.png"/></p>
<ul>
<li>Active means dependence of the nodes the unknown nodes.</li>
</ul>
<p><img alt="Quiz: D Separation 3" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-6e05c9243ad600bf.png"/></p>
<ul>
<li>notice that the third one is the application of the last rule in the previous slide.</li>
</ul>
<hr/>
<p>Now the application of Bayes Network:</p>
<h1>Probabilistic Inference</h1>
<p>In the remaining part of this lesson, you will learn about probabilistic inference using Bayes Nets, i.e. how to answer questions that you are interested in, given certain inputs.</p>
<p><img alt="Evidence, query and hidden nodes" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a9245bdccf19d0a8.png"/></p>
<ul>
<li>With Bayes net, we can infer the probability distribution of some nodes given certain input.</li>
<li>the input is called <strong>evidence</strong>, the output is called <strong>Query</strong> and all the other nodes are called <strong>Hidden nodes</strong>.</li>
</ul>
<p><img alt="quiz: what is evidence, hidden and query when trying to solve P(B|M)?" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-761e53f99c102423.png"/></p>
<p>Enumeration</p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-9165521874239af7.png"/></p>
<p><img alt="Enumerate f(+e, +a)" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-b7eab1d643c42c8d.png"/></p>
<p>The calculation becomes reading from the Bayes net table and summarize them. But when the number of nodes becomes larger, this method requires a lot of calculation.</p>
<p><img alt="Technique 1: pulling out terms that won't change with the loop variable" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a788890895ac5bd1.png"/></p>
<p><img alt="Technique 2: maximize independence" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-2823431e7b6af6b3.png"/></p>
<ul>
<li>e.g., if A is unknown, J and M are dependent.</li>
</ul>
<h2>Bayes net in Causal direction</h2>
<p><img alt="Causal direction" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-de69707271c8191e.png"/></p>
<h2>Value elimination</h2>
<p>Value elimination is a two-step operation: eliminate some fac
Joining factors and elimination</p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-d8a72b410de459a0.png"/></p>
<p><img alt="example of joining factors" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-815cb2b480932c38.png"/></p>
<p><img alt="Example of elimination" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-3e3bd7d6e091af34.png"/></p>
<p><img alt="Value elimination 2" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-11f67dd8d9c7a92d.png"/></p>
<p><img alt="Value elimination 3" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-c231266edf7af2d6.png"/></p>
<h2>Approximate Inference</h2>
<p><img alt="approximate by sampling" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-2a8a82d90724b5d3.png"/></p>
<p><img alt="Quiz: sampling, give R and S, what's the rows to consider to sample from, and where W should be positive?" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-bebc4e2032821a8d.png"/></p>
<ul>
<li>the sampling procedure will get the probability distribution to be close to the truth if repeated infinitely. This is called <strong>consistent</strong>.</li>
<li>the sampling method can be used to generate the complete joint probability or individual probability</li>
<li>to generate conditional probability, we can still use the sampling method, but need to cross-off the irrelevant joint probabilities. E.g, if we need to know P(R|C), then all the cases when c is not true will need to be thrown away. this is called <strong>rejection sampling</strong>.</li>
</ul>
<h2>Rejection Sampling</h2>
<ul>
<li>the problem of rejection sampling is that when the condition we care about is at a lower odds, the rejection sampling will reject a lot of samples which is a waste of resources. We can solve the problem by fixing the conditional variable, this is called <strong>likelihood weighting</strong>.</li>
</ul>
<p><img alt="Likelyhood weighting" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-9d2188c9cc94b3ff.png"/></p>
<ul>
<li>the problem of likelihood weighting, is that it is <strong>inconsistent</strong>. In another word, sampling with likelihood weighting cannot get true probability distribution unless it is properly weighted.</li>
</ul>
<h2>Likelihood Weighting</h2>
<p><img alt="quiz: Likelihood Weighting" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-edeb647075dbd78f.png"/></p>
<ul>
<li><p>Specify the weight for w+ and +s when constraining them when sampling. In the quiz, we need to constrain the +2 with the probability of 0.99 since we already know its parents are +S(due to constraining) and +R(due to sampling).</p>
</li>
<li><p>With likelihood weighting, we can make rejection sampling consistent</p>
</li>
<li><p>but it does not solve all the problems, for example</p>
</li>
</ul>
<p><img alt="Likelihood weighting fails helping P(C|+s, +r" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-201d5748381e4d9c.png"/></p>
<ul>
<li>Likelihood weighting with sampling will get good values for WetGrass because S and R determine W.</li>
<li>However, the sampling of C will not look at S or R, so the process will still generate C with values that do not go well with R and S, in this case, a lot of rejections will happen.</li>
</ul>
<h2>Gibbs Sampling</h2>
<p><img alt="image.png" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-cd5ac4b99e2da7a1.png"/></p>
<ul>
<li>Markov Chain Monte Carlo (MCMC): 1) sample by randomly sampling all nodes at the first time, then, 2) randomly identify one node from the first sample, and resample the node while keeping all others consistent with the first one to generate the second sample, and 3) repeat 2) with the second sample to generate the third sample, and so on...</li>
<li>MCMC is consistent.</li>
</ul>
<h2>Monty Hall Problem</h2>
<p><img alt="remember, the host will never reveal the door that the player selected in the second step, so be revealing one door, we did not know more about the door selected, but the door not selected got updated by the information provided by the revealing door" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-78b2c4dba0470e0d.png"/></p>
<ul>
<li>Even Monty Hall himself does not understand the Monty Hall problem.</li>
</ul>
