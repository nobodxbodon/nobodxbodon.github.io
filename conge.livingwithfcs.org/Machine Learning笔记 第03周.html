<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2016/2016-01-28-Machine-Learning-bi-ji---di-03-zhou-.md">ä»“åº“æºæ–‡</a>ï¼Œ<a href="https://conge.livingwithfcs.org/2016/01/28/Machine-Learning-bi-ji---di-03-zhou-">ç«™ç‚¹åŸæ–‡</a></h2>
<hr/>
<h2>layout: post
title: "Machine Learningç¬”è®° ç¬¬03å‘¨"
date: "2016-01-28 00:19:15"
categories: è®¡ç®—æœºç§‘å­¦
excerpt: "Week 03 tasksï¼š Lectures:  Kernel Methods &amp; SVMs as well as Computational..."
auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p>Week 03 tasksï¼š</p>
<ul>
<li>Lectures:  Kernel Methods &amp; SVMs as well as Computational Learning Theory.</li>
<li>Reading:  the tutorials on SVMs, as well as the relevant portions of Mitchell Chapter 7 for Comp. Learning Theory.</li>
</ul>
<h2>SL6: Kernel Methods and SVMs</h2>
<blockquote>
<ul>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/386608826/Lesson%206%20Notes.pdf">Lesson 6 Notes</a></li>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/386608826/Kernel%20Methods%20and%20SVMs.pdf">Kernel Methods and SVMs</a></li>
</ul>
</blockquote>
<p><img alt="Quiz 1: select the best line" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-fedd25c9a2883c94.png"/></p>
<ul>
<li>the middle line is consistent with the data but least commitmented to the data ( to avoid overfitting)</li>
</ul>
<p><img alt="How to find the best line" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a232451582274df1.png"/></p>
<ul>
<li>y is classification label, and the boundary line (orange line) and negative and positive plane is defined.</li>
<li>We should maximize the vector between the two grey lines.</li>
</ul>
<p><img alt="Quiz 2: how to measure the distance between two grey plane" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-1d3f6ebb23fc305e.png"/></p>
<ul>
<li>This finding the best line (least commitment line that separates the data) problem it actually maximizing the margin (2/||w||)</li>
</ul>
<p><img alt="SVM" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-7741318b78b80400.png"/></p>
<ul>
<li>maximizing the margin problem can be converted to minimizing  1/2 * ||w||&lt;sup&gt;2&lt;/sup&gt;, which is a quadratic programming problem.</li>
<li>it can be further expressed as another quadratic programming problem W(Î±)</li>
</ul>
<p><img alt="Quiz 3: Optimal Separator " src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-2ae10f257cfd35c6.png"/></p>
<ul>
<li>w is the sum of the product of Î±&lt;sub&gt;i&lt;/sub&gt;, y&lt;sub&gt;i&lt;/sub&gt; and x&lt;sub&gt;i&lt;/sub&gt;</li>
<li>for the most of time, Î±&lt;sub&gt;i&lt;/sub&gt; is 0.</li>
<li>quiz: in the graph in the right-bottom corner, which point is most likely to have an Î±&lt;sub&gt;i&lt;/sub&gt; == 0?<ul>
<li>answer: the points that are faraway from the optimal separator (or decision boundary) doesn't matter.</li>
<li>It's like KNN but you already figured out which points actually matter by quadratic program</li>
</ul>
</li>
<li>x&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt; x&lt;sub&gt;j&lt;/sub&gt; is a measure of how similar x&lt;sub&gt;i&lt;/sub&gt; and x&lt;sub&gt;j&lt;/sub&gt; are.</li>
</ul>
<h3></h3>
<p><img alt="Quiz 4: Linearly Married" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a9f1be1ed8bb907a.png"/></p>
<ul>
<li>for the pluses surrounded by minuses, they are not linear separable.</li>
<li>by introducing an function with a three dimensional tuple, we can separate them in a higher dimension.</li>
<li>the new function is equivalent to take the squared value of x&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt; x&lt;sub&gt;j&lt;/sub&gt;</li>
<li>with more complicated data, higher dimension can be used. this is called the <strong>Kernel trick</strong>. the kernel function can be expressed by k(x&lt;sub&gt;i&lt;/sub&gt;, x&lt;sub&gt;j&lt;/sub&gt;)</li>
<li>Kernel captures the domain knowledge</li>
</ul>
<p><img alt="Kernel" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-9ef8e23f4ac7adf3.png"/></p>
<ul>
<li>radial basis kernel</li>
<li>sigmoid-like kernel</li>
<li>Mercer Condition:  it acts like a distance, or a similarity</li>
</ul>
<p><img alt="recap" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-7c7944b503e09e27.png"/></p>
<hr/>
<p><img alt="Why Boosting tends not to overfitting" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-c550b993e4b49a53.png"/></p>
<ul>
<li>First, we normalize the final hypothesis with Î£Î±&lt;sub&gt;t&lt;/sub&gt;.</li>
<li>With each loop of boosting, the items that were hard to separate got separated further, e.g. pushing negative to the left and positive signs to the right.</li>
<li>Without changing the error, booting makes the margin larger and larger, thus it more likely to avoid overfitting.</li>
<li>But Boosting could be overfitting in practice</li>
</ul>
<p><img alt="Quiz 5: Boosting tends to overfit when ___." src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-ea0256b63a7b93a2.png"/></p>
<ul>
<li>explanation: if using a complex ANN, applying the training might end up no error, and the distribution will not change at all for the next loop. So no matter how many loops boosting goes through, it will end up of only one hypothesis which is the ANN. ANN overfits so that Boosting overfits.</li>
<li>pink noise, a.k.a uniform noise, also tends to cause boosting to overfit. (by the way, white noise is gaussian noise).</li>
</ul>
<hr/>
<h2>SL7: Computational Learning Theory</h2>
<blockquote>
<ul>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/383498973/Lesson%207%20Notes.pdf">Lesson 7 Notes</a></li>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/383498973/PAC%20Learning.pdf">PAC Learning</a></li>
</ul>
</blockquote>
<p><img alt="Quiz 1: how the region was labelled? " src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-658cf78fe9c0241b.png"/></p>
<ul>
<li>First, decision tree</li>
<li>second, SVM, or Perceptron or neural network</li>
<li>Third: K-NN, k =1.</li>
</ul>
<p><img alt="Learning Theory" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-515f43902041ddf5.png"/></p>
<p><img alt="Quiz 2: resource in machine learning" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-2d1d03ade1070d4d.png"/></p>
<ul>
<li>my answer was: space, time, training set, testing set</li>
</ul>
<p><img alt="Defining inductive learning" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-0a4d3263723a0a2c.png"/></p>
<p><img alt="Three ways of selecting training samples" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-ed93aca2c4426811.png"/></p>
<p><img alt="Quiz 3: Teaching Via 20 questions" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-af1ffa2a6155283a.png"/></p>
<ul>
<li>If the teacher (who knows the answer) tells learning to ask X, then learner ask the question. the learner can get the answer in <strong>1</strong> step.</li>
</ul>
<p><img alt="Quiz 4: Teaching Via 20 questions" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-00174f1ee34783ae.png"/></p>
<ul>
<li>if learner ask questions, how many steps s/he needs to get the correct answer?  log&lt;sub&gt;2&lt;/sub&gt; |H|</li>
<li>Answer: we want to choose a question that can eliminate half of the information at each step. And the number of times you can divide a number in half before you get down to one is exactly the log&lt;sub&gt;2&lt;/sub&gt;.</li>
</ul>
<p><img alt="Teacher With Constrained Queries" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-ff69ee994ca4ee94.png"/></p>
<ul>
<li>teacher is constrained because only information of X1, X3 and X5 will be considered</li>
</ul>
<p><img alt="Quiz 5: Reconstructing Hypothesis" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-208b0597311a458d.png"/></p>
<p><img alt="Quiz 5: answer" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-5290e9aca93d9483.png"/></p>
<p>If the teacher with constrains asked questions, he can ask k+2 questions to figure out the hypothesis, which is linear</p>
<ul>
<li>Number of questions needs to ask: k+2<ul>
<li>shows what's irrelevant :2</li>
<li>show what's relevant: K</li>
</ul>
</li>
<li>questions that could be asked = 3 &lt;sup&gt;k&lt;/sup&gt;</li>
</ul>
<p><img alt="Learner With Constrained Queries" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-d85fa0e042f2323e.png"/></p>
<ul>
<li>If the learner with the same constrains asked the questions, s/he will need to ask at least 2&lt;sup&gt;k&lt;/sup&gt; questions.</li>
<li>most questions asked will not provide any information.</li>
</ul>
<p><img alt="Leaner with Mistake bands 1" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-92388bcbcaa67eea.png"/></p>
<p><img alt="Leaner with Mistake bands 2" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-bde36c491abf155e.png"/></p>
<ul>
<li>The algorithm makes sure that the learning get a lot of information each time it makes a mistake.</li>
<li>the learner will never make more than K+1 mistakes</li>
</ul>
<p><img alt="Some definitions" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-246f4a83691e6691.png"/></p>
<p><img alt="Version Spaces difinition" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-0ec4924924de0d57.png"/></p>
<p><img alt="Quiz 6: Terminology" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-fad0cb57a11c2f30.png"/></p>
<ul>
<li>choose the hypothesis that are still in the version space after seeing the two training samples.</li>
</ul>
<p><img alt="Error of h" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a65d03e4dec034d1.png"/></p>
<p><img alt="PAC Learning" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-ed956054d1dddd85.png"/></p>
<ul>
<li>PAC stands for probably approximately correct. 1-ğ›¿ confident that we will get error Îµ to find the true concept class h(x)=c(x).</li>
<li>please note that the leaning time and samples were bounded by 1/Îµ,1/ğ›¿ and n.</li>
</ul>
<p><img alt="Quiz 7: is the h PAC learnable?" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-2f80add270507e68.png"/></p>
<ul>
<li>my answer was correct but I don't know why, keep going with the slides below.</li>
</ul>
<p><img alt="quiz 8: Epsilon Exhausted" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-f346634ea6e40138.png"/></p>
<ul>
<li>Epsilon Exhausted: if every hypothesis (h) in the version space have the error that's smaller than Îµ, then the version space is Îµ exhausted.</li>
<li>quiz, given the distribution of the samples data, and we only see the samples in green box, what's the smallest Îµ such that VS is  Îµ exhausted?<ul>
<li>From the two instance we see, we will now only three hypothesis are possible candidates.</li>
<li>the distribution for the last instance is 0, so it doesn't matter.</li>
<li>for the instance with 0.5 distribution, hypothesis X has error of 0.5, and OR and XOR have error of 0.</li>
<li>so Îµ should be 0.5 for the VS to be Îµ exhausted.</li>
</ul>
</li>
</ul>
<p><img alt="Haussler Theorem" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-0e3e543973aa10ec.png"/>
<img alt="Haussler Theorem continue" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-d766599e81f68588.png"/></p>
<ol>
<li>High true error: error&lt;sub&gt;D&lt;/sub&gt; is higher than Îµ</li>
<li>then the probability of candidate hypothesis is consistent with true hypothesis is less than or equal to 1- Îµ</li>
<li>then is we draw m samples, the probability of h consistent with C on all m samples is (1-Îµ)&lt;sup&gt;m&lt;/sup&gt;</li>
<li>then the probability of at least one h consistent with c is &lt;= K(1-Îµ)&lt;sup&gt;m&lt;/sup&gt; &lt;= |H|(1-Îµ)&lt;sup&gt;m&lt;/sup&gt;</li>
<li>because -Îµ &gt;= ln(1-Îµ), so (1-Îµ)&lt;sup&gt;m&lt;/sup&gt; &lt;=e&lt;sup&gt;-Îµm&lt;/sup&gt;, so that in 4, the probability of at least one h consistent with c is &lt;= |H|e&lt;sup&gt;-Îµm&lt;/sup&gt; &lt;=ğ›¿ (<em>ğ›¿</em> is failure possibility)</li>
<li>given all the above info, ln|H|-em&lt;= lnğ›¿, so m&gt;=1/Îµ(ln|h| + ln(1/ğ›¿).</li>
</ol>
<p><img alt="Quiz 9: PAC Learnable Example" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-d51b0d7ad53cd737.png"/></p>
<ul>
<li>substitute numbers in the formula above, and will get 40 as answer</li>
<li>Distribution is not used</li>
</ul>
<p><img alt="Recap" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-aafd496927725a2e.png"/></p>
<ul>
<li>two questions<ul>
<li>what if m does not satisfy the equation? agnostic</li>
<li>what if we have infinite hypothesis? hosedness (see next lesson)</li>
</ul>
</li>
</ul>
<p><img alt="" src="https://pbs.twimg.com/media/BYfci7qCEAEgH0O.jpg"/></p>
<pre><code>2016-01-26 å®ŒæˆSL6
2016-01-27 åˆç¨¿å®Œæˆ
</code></pre>
