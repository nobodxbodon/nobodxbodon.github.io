<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2017/2017-12-09-AI--bi-ji--Week-10-Machine-Learning.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2017/12/09/AI--bi-ji--Week-10-Machine-Learning">站点原文</a></h2>
<hr/>
<h2>layout: post
title: "AI 笔记 Week 10 Machine Learning"
date: "2017-12-09 03:36:33"
categories: 计算机科学
excerpt: "This week you should finish Lesson 7, Machine Learning, and read Chapter..."
auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<blockquote>
<p>This week you should finish Lesson 7, <a href="https://classroom.udacity.com/courses/ud954/lessons/6808838653/concepts/68238486020923">Machine Learning</a>, and read Chapter 18.6-11 &amp; 20.3 in Russell &amp; Norvig.  </p>
<p>Assignment 4: Decision Trees
Due: October 29 at 11:59PM UTC-12 (<a href="https://www.timeanddate.com/time/zones/aoe">Anywhere on Earth</a> time)</p>
</blockquote>
<hr/>
<h1>Boosting</h1>
<p><img alt="Boosting" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-1cf950d49d860a58.png"/></p>
<p><img alt="Boosting example" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-76402a83fa9ff53b.png"/></p>
<ul>
<li>??? I don't know how to calculate e2 and e3???
<img alt="Boosting example" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-15c72c2c79dc720f.png"/></li>
</ul>
<p><img alt="Boosting quiz" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-751fb27084ddc88e.png"/></p>
<h1>Neural nets</h1>
<p><img alt="Neura nets" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-1f77dea19b967b08.png"/></p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-33032819783ace0f.png"/></p>
<p><img alt="simple equation can do generalized computation" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-cd5d7e080306645f.png"/></p>
<p>Quiz: Neural Nets Quiz</p>
<p><img alt="quiz" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-b4cd983b27acdf2b.png"/></p>
<blockquote>
<p>Fill in the truth table for NOR and find weights such that:</p>
<p>a = { true if w0 + i1 w1 + i2 w2 &gt; 0, else false }</p>
<p>Truth table
Enter 1 for True, and 0 (or leave blank) for False in each cell.
All combinations of i1 and i2 must be specified.
Weights
Each weight must be a number between 0.0 and 1.0, accurate to one or two decimal places.
w1 and w2 are the input weights corresponding to i1 and i2 respectively.
w0 is the bias weight.
Activation function
Choose the simplest activation function that can be used to capture this relationship.</p>
</blockquote>
<h2>Multilayer Nets</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-4978caa59322eacd.png"/></p>
<ul>
<li>neural nets only makes sense when activation functions are nonlinear. If they are linear, the who network can be reduced to a linear function thus lose the power of the network.</li>
</ul>
<h2>Perceptron Learning</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-3c7e5bb310a9bd90.png"/></p>
<ul>
<li>single layer perceptron can only generate linear boundaries.</li>
</ul>
<p><img alt="Comparison between decision tree and perceptron" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-30cafc63f48ee0dc.png"/></p>
<ul>
<li>the performance of perceptrons is not always better than other methods (e.g. decision tree). it can be improved, however, by adding more layers</li>
</ul>
<h2>Multilayer Perceptrons</h2>
<p><img alt="Multilayer Perceptrons" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-93fd7e4d1a15ccb9.png"/></p>
<h2>Back-Propagation</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-9a657e93c29e5809.png"/></p>
<ul>
<li>Back-Propagation is the way to calculate neural nets.</li>
<li>the harder a problem is, the longer time for the algorithm to converge. Below are examples how fast an algorithm converges.</li>
</ul>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-04c79c803d59ba5a.png"/></p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-9a4a424b66e9f396.png"/></p>
<h2>Deep Learning</h2>
<ul>
<li>neural nets have limitations: need computation power, need more training set but still can be limited on the types of problem it suits.</li>
</ul>
<h1>Unsupervised Learning</h1>
<ul>
<li>or classification. The unsupervised learning algorithm classify data into sub-classes and figure out each which class each case fits in.</li>
</ul>
<h2>k-Means and EM</h2>
<p><img alt="k-Means" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-18d196851e68b7c4.png"/></p>
<ul>
<li>K-means start with randomly initiating the means and generate decision boundaries to separate the data set. the means of the separated data are then recalculated. Then new decision try will be generated to classify the data again. repeat the process until there is no change in the classification anymore.</li>
<li>this is the expectation-maximization procedure</li>
<li>for data that is hard to converge or avoid local maxima, random restart technique can be used.</li>
</ul>
<h2>EM and Mixture of Gaussians</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-6432ed046bc7e4e7.png"/></p>
<ul>
<li>instead of means, we can use k-Gaussians with the EM procedure to do classification.</li>
</ul>
<blockquote>
<p><strong>Readings on EM and Mixture Models</strong></p>
<ul>
<li>AIMA: Chapter 20.3</li>
<li>PRML: <a href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud954/notes/Machine-Learning/Mixture-Models-and-EM_Bishop.pdf">Chapter 9.0-9.2 Mixture Models and EM</a></li>
<li>*PRML = <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/">Pattern Recognition and Machine Learning</a>, Christopher Bishop
<strong>Research articles</strong></li>
<li><a href="http://www-static.cc.gatech.edu/~thad/p/journal/using-gps-to-learn-significant-locations.pdf">Using GPS to Learn Significant Locations and Predict Movement Across Multiple Users</a>, Daniel Ashbrook and Thad Starner</li>
</ul>
</blockquote>
<pre><code>20171101 初稿
</code></pre>
