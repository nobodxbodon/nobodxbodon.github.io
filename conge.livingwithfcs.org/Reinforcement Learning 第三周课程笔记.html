<h2>原文：<a href="https://conge.livingwithfcs.org/2015/09/01/Reinforcement-Learning--di-san-zhou-ke-cheng-bi-ji-">Reinforcement Learning 第三周课程笔记</a></h2>
<hr/>
<h2>layout: post
title: "Reinforcement Learning 第三周课程笔记"
date: "2015-09-01 10:18:34"
categories: 计算机科学
excerpt: "本周三件事：看课程视频，阅读 Littman (1996) Chapters 1-2，作业2（HW2）。 以下为视频截图和笔记： Reinfor..."
auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p>本周三件事：看课程视频，<a href="http://www.jianshu.com/p/d2796b5f87a4">阅读 Littman (1996) Chapters 1-2</a>，作业2（HW2）。</p>
<p>以下为视频截图和笔记：</p>
<h2>Reinforcement Learning Basics</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-eee1e2fb22dc7a3e.png"/></p>
<p>In RL, environment is only available to agent as percepted states (<em>s</em>), the agent can interact with the environment by taking action (<em>a</em>) and the environment gives a reward (<em>r</em>) as feedback to tell the agent is the &lt;s a&gt; pair are good or not. The computation is calculated in the agent's head.</p>
<p>The difference of RL and MDP is that in MDP, environment is totally available to the agent, while in RL, Environment is only available through the agent's perception.</p>
<h2>Demo of RL</h2>
<p><img alt="A MDP Game" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-daa1b12e2feda350.png"/></p>
<p>The small orange square represents the agent, and it can perform 6 actions. The world is the grid with some colored squares and a green dot. The goal what's the game and what are the actions.</p>
<h2>Behavioral structure</h2>
<p>The goal is the generate learning algorithm</p>
<p><img alt="Behavioral Structors" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-0abd9f62d00ed6fc.png"/></p>
<ul>
<li><strong>Plan</strong> is a set of fixed actions. Plan won't work during learning or when the environment is only partially known or stochastic.</li>
<li><strong>Conditional Plan</strong> includes "if" statements</li>
<li><strong>Stationary policy (or Universal Plan)</strong> are mapping from state to action. it can handle stochastic very well but it is very large. <strong>There always is an optimal stationary policy.</strong></li>
</ul>
<h2>Evaluating a policy</h2>
<p><img alt="Quiz: evaluating a policy" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-1945b7bf7a3b456e.png"/></p>
<ul>
<li>The numbers in the parentheses are probabilities of choosing the sequence. R(s,a) is reward function. <strong>Return</strong> is discounted rewards.</li>
<li>0.8^1 * 0.6 + 0.8^3 * 0.1 +(0.8^1 +0.8^4<em>(-0.2))</em>0.3 = .746624</li>
<li>This is the expected value of the policy based on the assumption that we index the states in each sequence from left to right starting at <strong>0</strong> on the far left. Interpreting T=5 to mean either truncating at the fifth circle or after the fifth transition (i.e. at the sixth circle) in each row gives the same result.</li>
</ul>
<h2>Evaluating a Learner</h2>
<p><img alt="Better Learner will get good returning policy with less time and simple data" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-f28c10531d1a11b5.png"/></p>
<h2>Recap</h2>
<p><img alt="Summary" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-66610df205c6fdf4.png"/></p>
<pre><code>2015-08-31 初稿
2015-12-02 reviewed and revised
</code></pre>
