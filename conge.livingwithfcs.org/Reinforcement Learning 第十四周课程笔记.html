<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2015/2015-11-19-Reinforcement-Learning--di-shi-si-zhou-ke-cheng-bi-ji-.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2015/11/19/Reinforcement-Learning--di-shi-si-zhou-ke-cheng-bi-ji-">站点原文</a></h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p>This week </p>
<ul>
<li>should watch CCC. </li>
<li>The readings are: <em>Zeibart et al. (2008)</em>. <em>Babes et al. (2011)</em>. <em>Griffith et al (2013</em>). <em>Cederborg et al (2015)</em>.<em> Roberts (2006</em>).<em> Bhat (2007).</em></li>
</ul>
<p><img alt="CCC" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-d9d3c7b8b970a327.png"/></p>
<p><img alt="Coordinating and communicating" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-fa5073a5b18b16b3.png"/></p>
<p><img alt="The decentralized partially observable Markov decision process (*Dec*-*POMDP*)" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-f5be2d492b469d83.png"/></p>
<ul>
<li>Dec-POMDP has some perspectives of game theory and MDP</li>
<li>Multiple agent working on getting a common reward. (if the rewards are separated for all the agents, then it's a POSG <em>partially observable stochastic game</em>)</li>
</ul>
<p><img alt="DEC-POMDPs properties " src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-f8488655296809fa.png"/></p>
<p><img alt="DEC-POMDPs example" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-170934e4e006cc4b.png"/></p>
<ul>
<li>two agents, they know where they are but don't know the other's position. when the two are in the same room, they win.</li>
<li>Strategy: go to a shared room. But my knowledge of my current position could be wrong ( partially observable world).</li>
</ul>
<p><img alt="Communicating and Coaching" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-ec3c9aeda7b3a039.png"/></p>
<ul>
<li>agent 1 wants to set up some kind of reward function to move agent to do something (e.g. get the apple for me).</li>
</ul>
<h2>Inverse Reinforcement Learning</h2>
<p><img alt="Inverse Reinforcement Learning" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-97621fa6b9936be6.png"/></p>
<ul>
<li>Inverse Reinforcement Learning: the agent experience the environment and a set a behavior and then generate a reward function based on the inputs.</li>
</ul>
<p><img alt="MLIRL: Maximium Likelyhod inverse reinforcement learning." src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-36fcd3662d7f418a.png"/></p>
<p><img alt="MLIRL result" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-db84d25a91887fe5.png"/></p>
<p><img alt="CCC" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-ec5b508388ef8972.png"/></p>
<h2>Policy Shaping</h2>
<p><img alt="Policy Shaping" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-6dd7ab85ee188417.png"/></p>
<ul>
<li>if a human is giving feedback (commentary) about weather the agent's action is good or bad, s/he is doing policy shaping.</li>
<li>policy shaping could be realized by reward shaping which is replace reward of an action with a new reward?</li>
<li>Agent need a mechanism to learn from the environment and the commentary to decide what policy to take (not just listening to the commentary, cause the commentary might not be always right).</li>
</ul>
<p><img alt="quiz 1:  Policy Shaping " src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-aca5004a17fca297.png"/></p>
<ul>
<li>If human is alway correct, given the feedback, what's the probability that the action (x, y, or z) is optimal?</li>
<li>answers in the slides above.</li>
</ul>
<p><img alt="Quiz 2: Policy Shaping" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-51707cf58b48d58e.png"/></p>
<ul>
<li>what if human is 0.8 probability of right?</li>
<li>counting method:<ul>
<li>saying x is optimal is liking saying y and z is not optimal.</li>
<li>since human is 0.8 correct, then x, y, z being optimal is 0.8, 0.2, 0.2. </li>
<li>normalize the numbers above, will get 2/3, 1/6, 1/6.</li>
</ul>
</li>
</ul>
<p><img alt="Policy Shaping probabiligy calculation" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-2a5a956943f0a4c3.png"/></p>
<ul>
<li>Δ&lt;sub&gt;a&lt;/sub&gt; is coming from data of action a (d&lt;sub&gt;a&lt;/sub&gt;). C is the probability of correct of the people giving commentary.</li>
<li>The formula above give the method of calculating probability of action a is optimal. </li>
<li>Note: the final probability will need to be normalized against the probabilities of other actions.</li>
</ul>
<p><img alt="quiz 3: How to combine info from multiple sources in Policy shaping?" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-ee6691a542969101.png"/></p>
<ul>
<li>in the policy shaping case, information are coming from multiple sources.</li>
<li>E.g. π&lt;sub&gt;a&lt;/sub&gt; and π&lt;sub&gt;H&lt;/sub&gt; are policy info from agent exploring the world and human giving feedback.</li>
<li>Some algorithm decrease the importance of π&lt;sub&gt;H&lt;/sub&gt; as time goes. One need to know that π&lt;sub&gt;a&lt;/sub&gt; already incorporated the information of human uncertainty (C).</li>
<li>the way to combine the two sources is to calculate the probability that the two policy will agree: a&lt;sub&gt;opt&lt;/sub&gt;=argmax&lt;sub&gt;a&lt;/sub&gt; p(a|π&lt;sub&gt;1&lt;/sub&gt;) * p(a|π&lt;sub&gt;2&lt;/sub&gt;).<ul>
<li>in the quiz x&lt;sub&gt;opt&lt;/sub&gt; = 1/15, y&lt;sub&gt;opt&lt;/sub&gt;=1/60,a&lt;sub&gt;opt&lt;/sub&gt;=2/15. So we should choose z as optimal.</li>
</ul>
</li>
</ul>
<h2>Drama Management</h2>
<p><img alt="Drama management world" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-8545ffce1cb91298.png"/></p>
<ul>
<li>the way a human can communicate to an agent<ul>
<li>demonstration: show the agent what's the correct action (inverse RL)</li>
<li>reward shaping: giving reward for agent's actions</li>
<li>policy shaping: commentary on the agent's actions</li>
</ul>
</li>
<li>author convey his intent to the agent so the agent can </li>
</ul>
<p><img alt="Drama Management: what's a stroy" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-cc8697a542352e5d.png"/></p>
<ul>
<li>story can be defined as a trajectory through plot points</li>
</ul>
<p><img alt="Trajectories as MDP" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-0b35723e5c3ad7dd.png"/></p>
<ul>
<li>above a some mapping of MDP elements to trajectory MDP elements</li>
<li>Problems<ul>
<li>large number of sequence of states (hyper exponential)</li>
<li>Since MDP will maximize rewards, treating story as an MDP will only make the author happy and force the player to experience the story.</li>
</ul>
</li>
</ul>
<p><img alt="TTD-MDP: Targeted trajectory distributions MDPs" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-748b50e7957bcf24.png"/></p>
<ul>
<li>p(t'|a,t) is the probability that the player at trajectory t and take action a then ended up in trajectory t'. P(T) is a target distribution.</li>
<li>the action is not player's action but the story action</li>
<li>the optimal policy is the policy that will lead to the targeted trajectory distribution P(T)</li>
<li>the calculation time is linear and dependent on the length of the story.</li>
</ul>
<h2>what have we learned</h2>
<p><img alt="recap" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-00d77a30851db60e.png"/></p>
<pre><code>2015-11-18 初稿 完成
</code></pre>
