<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2015/2015-09-26-Reinforcement-Learning--di-liu-zhou-ke-cheng-bi-ji-.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2015/09/26/Reinforcement-Learning--di-liu-zhou-ke-cheng-bi-ji-">站点原文</a></h2>
<hr/>
<h2>layout: post
title: "Reinforcement Learning 第六周课程笔记"
date: "2015-09-26 04:17:30"
categories: 计算机科学
excerpt: "Advanced Algorithmic Analysis Value iteration 1 tells us that VI converg..."
auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<h2>Advanced Algorithmic Analysis</h2>
<p><img alt="Advanced Algorithmic Analysis" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-dcc80797d291bfbd.png"/></p>
<h2>Value iteration</h2>
<p><img alt="Value iteration" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-dc5111ffe8d0c7c8.png"/></p>
<ul>
<li><p>1 tells us that VI converges in a reasonable amount  of time t* (finite). We know t* exists, but we don't know when that will happen.</p>
</li>
<li><p>2 Gives us an bound of difference between the policy now and optimal policy. We can take this bound and test when it is a decent time to stop VI and take the current policy.</p>
</li>
<li><p>Both 1 and 2 encourage us choosing a small γ. (γ tells us how further in the future we should look). The effective horizon is H ~ <strong>1/(1-γ)</strong>.</p>
</li>
<li><p>3 applying Bellman Operator K times to Q functions will shrink the distance between Q functions.</p>
</li>
</ul>
<h2>Linear Programming</h2>
<ul>
<li><p>Only one way to solve MDP in polynomile time：solving Bellman equation through Linear Programming. A <em>linear programming</em> problem may be defined as the problem of maximizing or minimizing a linear function subject to linear constraints. ( Defination extracted form this <a href="www.math.ucla.edu/~tom/LP.pdf">PDF</a>)</p>
</li>
<li><p>Bellman equation has one part that is not linear, the <em>max</em> function. But it can be expressed by a series of linear function and a objective function <code>min</code>.</p>
</li>
</ul>
<p><img alt="Primal" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-585891a739bc1274.png"/></p>
<ul>
<li>in the <em>primal</em>, the objective function is the minimum of sum of all the V&lt;sub&gt;s&lt;/sub&gt;;</li>
<li>in linear programming, we can change constraints to variables and variables to constraints, and the resulting linear program is equivalent to the old one.</li>
</ul>
<p><img alt="The Dual" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-64cccbc4a27ad3fb.png"/></p>
<ul>
<li>Dual is a new linear program comes from the old primal version of linear program (没有推导过程）</li>
<li>q&lt;sub&gt;sa&lt;/sub&gt; is "Policy flow", maximize the expected rewards of all states.</li>
<li>For each possible next state, we wanted it to be true that the amount of policy flows going through the next state should be equal to the number of the state it has been visited.</li>
</ul>
<h2>Policy Iteration</h2>
<p><img alt="Policy Iteration" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-88626729aa22a69c.png"/></p>
<ul>
<li>Initialize the first step Q value of all the states to be 0; improve the policy at time t; Apply the policy to calculate the Q value of t+1 step.</li>
<li>Convergence time is an open question (but it is finite): &gt;= linear, &lt;= exponential</li>
</ul>
<p><img alt="the concept of Domination" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-94c89a0e2d178055.png"/></p>
<ul>
<li>For every state, if the value of it follows π&lt;sub&gt;1&lt;/sub&gt; always equals or is larger than the value when it follows π&lt;sub&gt;2&lt;/sub&gt;, we say π&lt;sub&gt;1&lt;/sub&gt; <strong>dominates</strong> π&lt;sub&gt;2&lt;/sub&gt;.</li>
<li>if π&lt;sub&gt;1&lt;/sub&gt; <strong>dominates</strong> π&lt;sub&gt;2&lt;/sub&gt; and there exits states that V&lt;sup&gt;π&lt;sub&gt;1&lt;/sub&gt;&lt;/sup&gt;(s) &gt; V&lt;sup&gt;π&lt;sub&gt;2&lt;/sub&gt;&lt;/sup&gt;(s), we say π&lt;sub&gt;1&lt;/sub&gt; <strong>strict dominates</strong> π&lt;sub&gt;2&lt;/sub&gt;.</li>
<li>if for every state, the distance between the value following policy π and following the optimal policy π* is no larger than ε, then π is <strong>ε-optimal</strong>.</li>
</ul>
<p><img alt="Why Does Policy Iteration Work" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-5c4b61f8e9a1e74c.png"/></p>
<ul>
<li>B&lt;sub&gt;1&lt;/sub&gt; makes the update follows π&lt;sub&gt;1&lt;/sub&gt; and B&lt;sub&gt;2&lt;/sub&gt; makes the update follows π&lt;sub&gt;2&lt;/sub&gt;</li>
<li>Applying B operator to  two Values will not make them further apart, if the two values are the same, applying B will not make them closer.</li>
</ul>
<p><img alt="B&amp;lt;sub&amp;gt;2&amp;lt;/sub&amp;gt; is Monotonic" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-e326b361a1163cde.png"/></p>
<ul>
<li>Theorem: if V&lt;sub&gt;1&lt;/sub&gt; dominates V&lt;sub&gt;2&lt;/sub&gt;, applying B will keep the ordering: BV&lt;sub&gt;1&lt;/sub&gt; &gt;= BV&lt;sub&gt;2&lt;/sub&gt;.</li>
</ul>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-086e6949e8da3d94.png"/></p>
<ul>
<li>Q&lt;sub&gt;1&lt;/sub&gt; is the fix point of B&lt;sub&gt;1&lt;/sub&gt;;</li>
<li>π&lt;sub&gt;2&lt;/sub&gt; is greedy policy with respect to Q&lt;sub&gt;1&lt;/sub&gt;</li>
<li>B&lt;sub&gt;2&lt;/sub&gt; is the Bellman operator of π&lt;sub&gt;2&lt;/sub&gt;</li>
<li>Applying B&lt;sub&gt;2&lt;/sub&gt; (the greedy function wrt Q&lt;sub&gt;1&lt;/sub&gt;) on &lt;/sub&gt;Q&lt;sub&gt;1&lt;/sub&gt; will add a bound to Q&lt;sub&gt;1&lt;/sub&gt;, thus getting a better Q&lt;sub&gt;1&lt;/sub&gt;. This is called <strong>Value Improvement</strong>.</li>
</ul>
<p><img alt="Quiz 1" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-927cd9e981675b43.png"/></p>
<p><img alt="Quiz 1 answers" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-6732e17c7cc47f0d.png"/></p>
<ul>
<li>Value improvement (or value non deprovement): for each state, value will improved until it could not get better anymore.</li>
<li>Monotonicity: Value can only get better after each iteration.</li>
<li>Transitivity:  a &gt;= b, b &gt;=c, so a &gt;=c.</li>
<li>Fixed point. If we apply B&lt;sub&gt;2&lt;/sub&gt; over and over again on Q1, we will reach the fixed point of B&lt;sub&gt;2&lt;/sub&gt;,which is Q&lt;sub&gt;2&lt;/sub&gt;.</li>
</ul>
<p><img alt="wrap-up" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-ffddbf568bcc50bb.png"/></p>
<ul>
<li>In Value iteration, greedy policy converges in finite step. This does not necessarily mean value function will converge.</li>
</ul>
<pre><code>2015-09-23 初稿
2015-09-26 完成
2015-12-04 reviewed and revised.
</code></pre>
