<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2015/2015-10-18-Reinforcement-Learning--di-jiu-zhou-ke-cheng-bi-ji-.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2015/10/18/Reinforcement-Learning--di-jiu-zhou-ke-cheng-bi-ji-">站点原文</a></h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p>This week</p>
<ul>
<li>Watch <em>Generalization</em>. </li>
<li>The readings are <em>Gordon (1995) </em>and<em> Baird (1995).</em></li>
</ul>
<p><img alt="Generalization" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-9310c266d53d3bb7.png"/></p>
<p><img alt="Example" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-15a6061d8c8cd348.png"/></p>
<ul>
<li>When there are a lot of states, we can use features to represent states (e.g. grouping states using some common features or using some base states to represent other stats)</li>
<li>Inductive bias: refer to the way algorithm prefer one solution against another.</li>
<li>representation can stay unchanged even when states changed(?)</li>
</ul>
<p><img alt="Generalization idea" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-f4afb986d8264ff0.png"/></p>
<ul>
<li>In RL, the goal is to learn some input-output mapping. <ul>
<li>e.g. Policy maps state -&gt; actions </li>
<li>value function maps state/actions to expected return</li>
<li>Model maps T/R (transition and reward)</li>
<li>Generalization can happen on all these levels </li>
</ul>
</li>
<li>Most researchers are focused on <strong>value function</strong> approximation.</li>
</ul>
<p><img alt="Basic Update Rule" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-89f07fed7553e314.png"/></p>
<ul>
<li>Q is represented by function approximator F(w&lt;sup&gt;a&lt;/sup&gt;, f(s)). w&lt;sup&gt;a&lt;/sup&gt; is some kind of weights, f(s) takes a state <em>s</em> and generate a series of features.</li>
<li>experience tuple &lt;s,a,r,s'&gt; comes in, and do an update.</li>
<li>TD error= the difference between predicted value (reward + discounted value of next state) and value of current state. TD error tells us the direction to move the parameters <em>w</em>. (the prediction is high, low or just right).</li>
<li>α is learning rate.</li>
</ul>
<h2>Linear Value Function Approximation</h2>
<p><img alt="Linear value function" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-d9994c638a29f0ff.png"/></p>
<ul>
<li>Q value for an action is represented by features and a set of weights</li>
<li><em>w</em>&lt;sup&gt;a&lt;/sup&gt;&lt;sub&gt;i&lt;/sub&gt; are weights of features f&lt;sub&gt;i&lt;/sub&gt;(s), which represent importance of features</li>
</ul>
<p><img alt="Quiz 1" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-626ee8bf2b94526e.png"/></p>
<h2>Success and fail stories</h2>
<p><img alt="Does it work?" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-cd110eb4ef5a32b6.png"/></p>
<ul>
<li>3-layer back prop: decision making in Jeopardy</li>
<li>CMAC(<a href="https://en.wikipedia.org/wiki/Cerebellar_model_articulation_controller">wikipedia</a>) : </li>
</ul>
<p><img alt="Baird's Counter example" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-510b8512f8a6cf7c.png"/></p>
<p><img alt="Quiz 2: Bad Update Sequence" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-54563d182803acda.png"/></p>
<ul>
<li>V(s') = V(7) =8; V(s) = V(1) =3, derivative is w =1; so  Δw&lt;sub&gt;0&lt;/sub&gt; = 0.1( 0 + 0.9 <em> 8 -3)</em>1 = 0.42;</li>
<li>Δw&lt;sub&gt;0&lt;/sub&gt; is 0.42, and Δw&lt;sub&gt;i&lt;/sub&gt;, i = [0, 6] are all positive, so weight will increase with each update</li>
<li>Δw&lt;sub&gt;7&lt;/sub&gt; is negative when we update &lt;7,0,7&gt;, but w&lt;sub&gt;7&lt;/sub&gt; will still increase because it's original value is &gt;&gt; |Δw&lt;sub&gt;7&lt;/sub&gt;|</li>
<li>So, even the algorithm is doing the right thing, the updates will never converge</li>
</ul>
<p><img alt="Quiz 2: What if we initialize all the weights as 0" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-aaacf7d55655c05f.png"/></p>
<ul>
<li>in this case, the temporal differences are all zeroes because w&lt;sub&gt;i&lt;/sub&gt; is 0, which means no updates (changes) are going to happen. <strong>Sticky</strong></li>
<li>If at any circumstance, when all values are zero, it could not escape the circumstance unless reward (<em>r</em>) is non-zero. <strong>If reward is also zero, we can call the circumstance the solution of the MDP</strong>.</li>
<li>this holds when MDP is <strong>deterministic</strong>.</li>
<li>the lesson: Shared weights can "not converge"!</li>
</ul>
<p><img alt="Averagers: looks like  Linear Value Function Approximation but is non-linear" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-4879e276577c7384.png"/></p>
<ul>
<li>Problem: given a certain points of value, estimate the function of line. How to estimates the points in-between?</li>
<li>Naturally, weighted average of neighbours and get convex combinations of anchor points</li>
<li>for points that are far from anchor points, are good estimate will be the average of the anchor points.</li>
<li>more anchor points lead to less error in MDP estimation.</li>
</ul>
<p><img alt="quiz 3" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-1740031ded7636c1.png"/></p>
<p><img alt="Connection to MDPs" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-c36f7d653a5bbd00.png"/></p>
<ul>
<li>Using base states, we can respresent the original MDP with MDP over S&lt;sub&gt;b&lt;/sub&gt;s</li>
</ul>
<h2>What have we learned</h2>
<p><img alt="recap" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-85ea45dc5fa114f6.png"/></p>
<ul>
<li><strong>Need</strong> for generalization: some problem have Zillions of states; Method: applying supervised learning method .</li>
<li>Methods: Linear function approximation.<ul>
<li>1) value functions 2) policies; 3) models;</li>
</ul>
</li>
<li>Success: TD gamma, Atari, Fitted Q-iteration (but still have need not work cases)</li>
<li>Problem cases: even linear need not work (baird counter problem)</li>
<li>Averagers: can be viewed as MDP itself. Use anchor points to represent the whole MDP.</li>
<li><strong>LSPI</strong>: Least Squared policy iteration</li>
</ul>
<pre><code>2015-10-13 初稿 upto quiz 1
2015-10-17 完成
2015-12-04 reviewed and revisted
</code></pre>
