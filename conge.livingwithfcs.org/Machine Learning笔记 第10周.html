<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2016/2016-03-27-Machine-Learning-bi-ji---di-10-zhou-.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2016/03/27/Machine-Learning-bi-ji---di-10-zhou-">站点原文</a></h2>
<hr/>
<h2>layout: post
title: "Machine Learning笔记 第10周"
date: "2016-03-27 23:27:45"
categories: 计算机科学
excerpt: "第十周根本没时间上课，只能利用第11周的春假补全。 This week: going over Feature Transformation t..."
auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p>第十周根本没时间上课，只能利用第11周的春假补全。</p>
<p>This week: going over Feature Transformation this week, and starting on Information Theory.</p>
<p><img alt="Defination of Feature Ransformation" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-5203ad0fff64c169.png"/></p>
<ul>
<li>Feature selection is a subset of feature transformation</li>
<li>Transformation operator is linear combinations of original features</li>
</ul>
<h2>Why do Feature Transformation</h2>
<p><img alt="Example of words" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-302ead862e35ce2b.png"/></p>
<ul>
<li>XOR, Kernel methods, Neural networks already do FT.</li>
<li><strong>ad hoc Information Retrieval Problem</strong>: finding documents within
a corpus that are relevant to an information need specified using a query. (Query is unknown)</li>
<li>Problems of Information Retrieval:<ul>
<li>Polysemy: e.g. a word have multiple meanings; cause false positive problem</li>
<li>Synonymy: e.g. a meaning can be expressed by multiple words. can cause false negatives problems.</li>
</ul>
</li>
</ul>
<h2>PCA</h2>
<p><a href="http://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf">This paper </a>does a fantastic job building the intuition and implementation behind PCA</p>
<p>An <em>eigenproblem</em> is a computational problem that can be solved by finding the eigenvalues and/or eigenvectors of a matrix. In PCA, we are analyzing the covariance matrix (see the paper for details)</p>
<p><img alt="PCA" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-81eb2ea72b9d0253.png"/>
PCA Features</p>
<ul>
<li><p>maximize variance</p>
</li>
<li><p>mutually orthogonal (every components are perpendicular to each other)</p>
</li>
<li><p>Global algorithm: the resulted components have a global constraint which is that they must be orthogonal</p>
</li>
<li><p>it gives best reconstruction</p>
</li>
<li><p>EigenValue monotonically not increasing and 0 eigenvalue = ignorable (irrelevant, maybe not useful).</p>
</li>
<li><p>It's well studied and fast to run.</p>
</li>
<li><p>it's like a classification. and using a filtering method to select dimensions to use.</p>
</li>
<li><p>PCA is about finding</p>
</li>
</ul>
<h2>ICA</h2>
<p>ICA has also been applied to the information retrieval problem, in <a href="http://www.cc.gatech.edu/~isbell/papers/isbell-ica-nips-1999.pdf">a paper</a> written by Charles himself</p>
<p><img alt="ICA" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-7c249ef746608986.png"/></p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-7d4d4f528cba175f.png"/></p>
<ul>
<li>find components that are statistically independent from each other using mutual information.</li>
<li>Designed to solve the blind source separation problem.</li>
<li>Model: given observables, find hidden variables.</li>
</ul>
<p><img alt="quize 1: defining features for PAC and ICA" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-1cdec1677aa3a6cc.png"/></p>
<p><img alt="More PCA vs ICA" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-dfd7190bb9198258.png"/></p>
<ul>
<li>ICA is more suitable for BSS problems and is directional.</li>
<li>Eg,<ul>
<li>PCA on faces will separate image based on brightness and average faces. ICA will get features such as nose, mouth etc, which are basic components of a face.</li>
</ul>
</li>
</ul>
<h2>Alternatives:</h2>
<p><img alt="RCA" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-5b604c9fdfad5424.png"/></p>
<p>Random components Analysis: generates random directions</p>
<ul>
<li>Can project to smaller dimensions (m &lt;&lt; n)but in practice often have more dimensions than PCA.</li>
<li>Can project to higher dimensions (m &gt; n)</li>
<li>It works and works very fast.</li>
</ul>
<p><img alt="LDA" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-4cb09d5cf333a79e.png"/></p>
<ul>
<li>Linear Discriminant analysis: find a projection that discriminates based on the label</li>
</ul>
<h2>wrap up</h2>
<p><img alt="Wrap up" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-fc64778d86882031.png"/></p>
<p><a href="http://computation.llnl.gov/casc/sapphire/pubs/148494.pdf">This excellent paper </a>is a great resource for the Feature Transformation methods from this course, and beyond</p>
<pre><code>2016-03-17 初稿
2016-03-26 补完
</code></pre>
