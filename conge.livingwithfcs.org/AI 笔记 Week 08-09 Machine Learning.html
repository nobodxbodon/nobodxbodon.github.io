<h2>原文：<a href="https://conge.livingwithfcs.org/2018/09/24/AI--bi-ji--Week-08-09-Machine-Learning">AI 笔记 Week 08-09 Machine Learning</a></h2>
<hr/>
<h2>layout: post
title: "AI 笔记 Week 08-09 Machine Learning"
date: "2018-09-24 14:23:00"
categories: 计算机科学
excerpt: "This week:  watch the first section of Lesson 7, Machine Learning (throu..."
auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<blockquote>
<p>This week:  watch the first section of Lesson 7, <a href="https://www.udacity.com/course/viewer#!/c-ud954/l-6808838653">Machine Learning</a> (through Random Forests), and read Chapters 18.1-5, 18.8, 20.1-20.2 in AIMA (Russell &amp; Norvig).</p>
</blockquote>
<h1>Challenge Question</h1>
<p>Find the most efficient decision tree given the fact table</p>
<p><img alt="quiz 1: challenge question" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-f66bcfa8945d9652.png"/></p>
<h1>k-Nearest Neighbors</h1>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-545bc6eec9dec0c2.png"/></p>
<h1>Cross Validation</h1>
<p><img alt="Cross validation" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-22f440bea9570ab6.png"/></p>
<ul>
<li>Training set and test set.
Cross-validation often uses a randomly chosen independent sample as test sets.</li>
<li>Model selection: tweaking parameters of the model and report the one which gives the best prediction ( be this might lead to the over-fitting problem.</li>
<li>Leave-one-out cross-validation (LOOCV) strategy: when a sample is small, a test set might not be feasible, in this case, use the leave-one-out strategy in which we train the model with all cases in a data set except one random case. repeat the process multiple times with one case leave out.</li>
</ul>
<p><img alt="quiz: CV" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-4ac3ff3348fc22c0.png"/></p>
<blockquote>
<p>AIMA: Chapter 18.8
Further study: <a href="https://classroom.udacity.com/courses/cs271/lessons/48703335/concepts/487480760923">Sebastian Thrun’s and Peter Norvig’s lecture on kNN</a></p>
<p><img alt="Quiz: 1NN" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-cd89a6674a21fca9.png"/></p>
<ul>
<li>which data point is positive given the known data points?</li>
</ul>
<p><img alt="quiz: kNN" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a22e09074b8ceff9.png"/></p>
<ul>
<li>what is the value of the data points using kNN when k = 1, 3, 5, 7, 9?</li>
</ul>
<h1>K As Smoothing Parameter</h1>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-9af621a5260f12dd.png"/></p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-25cf00a684fbb485.png"/></p>
<ul>
<li>as K increases, the separation boundary becomes smoother, but there are more outliers which will be misclassified. The model will also  be come more complex</li>
</ul>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-385ad692aa287f96.png"/></p>
</blockquote>
<h1>The Gaussian Distribution</h1>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-9a6776f1816a1397.png"/></p>
<ul>
<li>standard deviation determines the width of the distribution</li>
<li>68% with 1 SD, 95% within 2 SD, 99.7% with 3 SD.</li>
</ul>
<h2>Central Limit Theorem</h2>
<ul>
<li>Samples of random variables, the mean of the samples will form a Gaussian Distribution. And the mean of the distribution will be expected to be the population mean</li>
</ul>
<h2>Grasshoppers Vs Katydids</h2>
<p>A pattern recognition example of Gaussian distribution</p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-77a5875af6a04c9e.png"/></p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-953ad3c1850f983c.png"/></p>
<ul>
<li>the distribution can be formed by projecting data on to the Y-axis.</li>
<li>once a distribution is formed, it is very easy to get the probability of what a case if from by fit it in the distribution.</li>
</ul>
<h2>Quiz: Gaussian Distribution</h2>
<p>Take the <strong>Insect data</strong>: Antennae length [<a href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud954/data/insects/antennae-length.xlsx">xlsx</a> | <a href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud954/data/insects/antennae-length.csv">csv</a>] and calculate the probability of the length "7" indicate a Katydid or Grasshopper
<img alt="Quiz: Gaussian Distribution" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-e991c4ebac5c0e62.png"/></p>
<h2>Decision Boundaries</h2>
<p><img alt="Decision Boundaries with Gaussian is easy" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-2b5072180cfe8c81.png"/></p>
<ul>
<li>the boundary is where the distributions cross.</li>
</ul>
<p><img alt="Quiz on recoginition" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-2b3b424287e67418.png"/></p>
<ul>
<li>"No" because the recognizer might classify everything as negative and get 90% cases correctly.</li>
</ul>
<h2>Decision Boundaries in Higher Dimensions</h2>
<ul>
<li>The Decision boundaries are also in higher dimensions for higher dimensional distribution</li>
</ul>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-f46091f45179126c.png"/></p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-ae9ba8c522350ada.png"/></p>
<h2>Error</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a6a3dfb48239b29d.png"/></p>
<ul>
<li>we can make the decision boundary which changes the error rate of the classification. In real life, the decision should be made to increase acceptable errors (classify mosquitoes don't care certain disease as the ones who carry the disease) to avoid unacceptable errors (misclassify bad as good).</li>
</ul>
<h1>Bayes Classifier</h1>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-45861ef3136218e0.png"/></p>
<p><img alt="quiz: Bayes rule by counting" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-e90b31540f367612.png"/></p>
<ul>
<li>based on the table P(G|N ) = P(N|G) * P(G)/P(N)</li>
<li>using counting to get the probabilities. E.g. P(male), there are 3 males in 8 data points, then p(male) = 3/8, etc...</li>
<li>N = Name = Drew here.</li>
<li>P (male|Drew)= P(Drew|male) P(male) /P(Drew) = 1/3 * 3/8 / (3/8) = 0.33</li>
<li>P (female|Drew)= P(Drew|female) P(female) /P(Drew) = 2/5 * 5/8 /(3/8) = 10/15 = 0.67</li>
</ul>
<h2>Naive Bayes</h2>
<ul>
<li>Independent assumption gives Naive Bayes.</li>
<li>Naive Bayes can be represented as a tree structure of class pointing to features</li>
</ul>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-f64cc8ce0f17e4d8.png"/></p>
<p>Naive Bayes net assumes independence between features: so P( height, hair length | sex) = P(height | sex) * P(hair length | sex), thus</p>
<ul>
<li>p(d|C&lt;sub&gt;j&lt;/sub&gt;) = p(d&lt;sub&gt;1&lt;/sub&gt;C&lt;sub&gt;j&lt;/sub&gt;) * p(d&lt;sub&gt;2&lt;/sub&gt;C&lt;sub&gt;j&lt;/sub&gt;)  * ... * p(d&lt;sub&gt;m&lt;/sub&gt;C&lt;sub&gt;j&lt;/sub&gt;)</li>
<li>here we assume Maximu Likelihood where everyone prior has the same probability of occurence.</li>
</ul>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-b5b19886f1fc389e.png"/></p>
<p><img alt="answer" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-76e24dd2451626e8.png"/></p>
<blockquote>
<p><strong>Readings for Bayesian Classifiers</strong></p>
<ul>
<li>AIMA: Chapter 20.1-20.2</li>
</ul>
<p><strong>Further resources</strong></p>
<ul>
<li><a href="http://www.cs.ucr.edu/~eamonn/CE/Bayesian%20Classification%20withInsect_examples.pdf">Keogh’s original slides</a></li>
<li><a href="https://www.udacity.com/course/viewer#!/c-ud262/l-478818537/m-482228628">Isbell’s and Littmann’s lecture</a></li>
<li><a href="https://www.udacity.com/course/viewer#!/c-cs271/l-48703335/e-48704326/m-48687677">Thrun’s lecture</a></li>
</ul>
</blockquote>
<h2>No Free Lunch</h2>
<p>No one ML algorithm is good for all problems.</p>
<h2>Naive Bayes vs kNN</h2>
<h2>using a mixture of Gaussians</h2>
<p><img alt="two Gaussian" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-371d861a7029e93a.png"/></p>
<ul>
<li><p>Two Gaussian can generate a more complex decision boundary, and more Gaussian can do more until it creates its own decision boundaries for each of the data points which is nearly the same as kNN.</p>
</li>
<li><p>Kernel density estimation: use fewer Gaussians which would cause the decision boundary to be smooth, But still give a good continuous approximation of the shape</p>
</li>
<li><p>cross-validation can be used to find the good N of Gaussians.</p>
</li>
<li><p>use a final test set to test for overfitting.</p>
</li>
</ul>
<h2>Generalization</h2>
<h2>Visualization</h2>
<p>Visualize data to determine which algorithm might work.</p>
<ul>
<li>If most of the classes from balls of data without many concavities, then modeling it with Gaussians will probably work well.</li>
<li>If there are situations where classes interpenetrate, but still have distinct boundaries, then k-nearest neighbors or one of the kernel methods will probably work.</li>
<li>for high dimensional data we can also use methods like <strong>decision trees</strong> and <strong>boosting</strong> to help you understand which features are most important.</li>
</ul>
<hr/>
<h1>Decision Tree</h1>
<h2>Decision tree with discrete information</h2>
<h2>Decision tree with continuous information</h2>
<h2>Minimum Description Length</h2>
<p><img alt="Minimum Description Length" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-b6a54752f160322c.png"/></p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-c5b389dd8e710e6e.png"/></p>
<ul>
<li>which attributes should we use first?</li>
<li>Which question provides the most solution of the problem? then it should be on the top of the decision trees according to information theory.</li>
</ul>
<h2>Entropy</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-0b4f843382166265.png"/></p>
<ul>
<li>p is the number of positive cases</li>
<li>n is the number of negative cases</li>
<li>B(<em>q</em>) = -(<em>qlog&lt;sub&gt;2&lt;/sub&gt;q</em> + (1 - <em>q</em>)<em>log&lt;sub&gt;2&lt;/sub&gt;</em>(1 - <em>q</em>))</li>
</ul>
<h2>Information Gain</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-f5664cca4d475ec9.png"/></p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-878a3365c095caef.png"/></p>
<ul>
<li>Information Gain is a way to determine how to construct three</li>
</ul>
<p><img alt="image.png" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-acf916c486bb897e.png"/></p>
<ul>
<li>Please note that the attribute which is not important at the first iteration might become important in the new level.</li>
</ul>
<p><img alt="quiz" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-487b9e72f9d7bac4.png"/></p>
<ul>
<li>B(9/14) = 0.94
Gain(outlook) = 0.94 - [5/14 * B(3/5)  + 4/14 * B(4/4) + 5/14 * B(2/5)]
=0.94 - [5/14 * 0.97  + 5/14 * 0.97]</li>
</ul>
<p><img alt="quiz answer" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-71333493eea70af0.png"/></p>
<blockquote>
<p>Readings on Decision Trees<br/>
AIMA: Chapter 18.1-18.5</p>
</blockquote>
<hr/>
<h1>Random Forest</h1>
<p><img alt="Random forest" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-34098eb2341c1430.png"/></p>
<ul>
<li>train several decision trees and let them vote for the answer</li>
<li>Random sampling seems to be able to lower the chance of overfitting</li>
</ul>
<blockquote>
<p>week 09 is the midterm week. No lectures</p>
</blockquote>
