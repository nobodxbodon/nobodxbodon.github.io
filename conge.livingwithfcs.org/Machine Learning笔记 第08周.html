<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2016/2016-03-15-Machine-Learning-bi-ji---di-08-zhou-.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2016/03/15/Machine-Learning-bi-ji---di-08-zhou-">站点原文</a></h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p>注意，第七周是考试周，我其实把考试的的内容压缩进了第06周的笔记中去，所以第七周没有单独笔记。但是所有课堂内容没有缺失，特此声明。</p>
<p>Week 08 tasks:</p>
<ul>
<li>Lectures: unsupervised learning:  clustering and expectation maximization.</li>
<li>Reading: Chapter 6 of Mitchell, and the supplementary notes on Expectation-Maximization and clustering.</li>
</ul>
<hr/>
<p><img alt="" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-32a20ea70376fa4d.png"/></p>
<p><img alt="" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-6f07846c46a41101.png"/></p>
<ul>
<li>Supervised learning: use labeled training data to generate labels to new instances. Can be seen as function approximation.</li>
<li>Unsupervised learning: make sense of unlabeled data. Data description, when new instance presents, describe it with the generated data description.</li>
</ul>
<h2>Basic Clustering Problem</h2>
<p><img alt="Basic Clustering Problem" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-5954c5d888ab6ae1.png"/></p>
<ul>
<li>definition: given set of object X and inter object distance D(x,y) = d(y,x), we can partition the input and put x and y into the same cluster when P&lt;sub&gt;D&lt;/sub&gt;(x) = P&lt;sub&gt;D&lt;/sub&gt;(y)</li>
<li>extreme cases: when P&lt;sub&gt;D&lt;/sub&gt;(x) = 1, then will be only one cluster; when P&lt;sub&gt;D&lt;/sub&gt;(x) = x, then each x is a cluster (each cluster has only one item).</li>
</ul>
<p><img alt="Quiz 1: Single Linkage Clustering" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-7ee69e2751971baf.png"/></p>
<ul>
<li>Single Linkage Clustering: first define each object as a cluster, and then find out the closest two points and merge them ( like kNN). The distance function could be customized. Repeat N-k times can get k clusters</li>
<li>k is prior knowledge</li>
<li>quiz, what two points should be linked together next?</li>
</ul>
<p><img alt="SLC" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-d7fdf7f062d757df.png"/></p>
<ul>
<li>the clustering process could be represented as a hierarchical tree structure</li>
<li>the distance function could be mean, median...</li>
</ul>
<p><img alt="Quiz 2: Running time of SLC" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-a88de5e9412e74c6.png"/></p>
<ul>
<li>For SLC, we need to look at each pair can find the one has the smallest distance and this is a O(n&lt;sup&gt;2&lt;/sup&gt;) process. And we need to do this about n times. So the answer is O(n&lt;sup&gt;3&lt;/sup&gt;)</li>
</ul>
<p><img alt="Quiz 3: Problem of SLC" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-f266128d3802f782.png"/></p>
<ul>
<li>SLC could not clustering this problem correctly</li>
</ul>
<h2>K-means</h2>
<p><img alt="K-means clustering" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-3cdb8cdf9d5447ed.png"/></p>
<ul>
<li>randomly initialize k centers</li>
<li>each center claims points to cluster the data</li>
<li>recompute the center by average the clustered points</li>
<li>repeat above until converge.</li>
<li>Question: does it always converge?</li>
</ul>
<h3>K Means in Euclidean Space proof</h3>
<p><img alt="K Means in Euclidean Space 1" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-dfd1196a4d8bf7b1.png"/></p>
<ul>
<li>Terms: partition, cluster and center</li>
</ul>
<p><img alt="quiz 4: K Means as Optimization" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-10422ff912964320.png"/>
Not sure how to understand the definitions here.</p>
<p><img alt="K-means" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-4cfd861a55a55f08.png"/></p>
<ul>
<li>the error can never go up for partition part and re-centering part of the clustering process</li>
<li>given the that the partition/clustering space is finite, K-means will converge in finite time.</li>
</ul>
<p><img alt="quiz 4: K-means properties" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-2d113f34511683cb.png"/></p>
<ul>
<li>it's possible for K-means to stuck at a local optima, but this can be solved by random restart ( like random hill climbing in the optimization)</li>
</ul>
<h2>Soft Clustering</h2>
<p><img alt="Quiz 6:" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-5d04b7b99513e03d.png"/></p>
<ul>
<li>K-means and SLC could not give the point d a definitive cluster</li>
<li>Solution: lean on possibility :)</li>
</ul>
<p><img alt=" Soft Clustering" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-4c4d2730e5c8e392.png"/></p>
<ul>
<li>here ML is maximum likelihood.</li>
</ul>
<p><img alt="maximum likelihood Gaussian" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-50789937b1629e2b.png"/></p>
<ul>
<li>Hidden variables are used to keep track which Gaussian distribution that x is generated from</li>
</ul>
<p><img alt="Expectation Maximization" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-343798d1c3047a39.png"/></p>
<ul>
<li>there are expectation and maximization for the soft clustering. The probability of X&lt;sub&gt;i&lt;/sub&gt; in the &lt;sub&gt;i&lt;/sub&gt; cluster is between 0 and 1.</li>
<li>soft clustering can be converted to K-means if push the probability to be 0s and 1s.</li>
</ul>
<p><img alt="EM example" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-4fb3636bdb1e7b54.png"/></p>
<ul>
<li>given K, randomly generate k centers</li>
<li>with each iteration, the centers were recalculated and the probability of the each point is in a cluster is also updated until converge.</li>
</ul>
<p><img alt="Properties of EM" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-63c98fb6b8872d72.png"/></p>
<ul>
<li>the reason that EM does not converge is because there are infinite number of configurations.</li>
<li>Local maxima problem can be solved by random restart</li>
<li>E, M might be different for different problems</li>
</ul>
<h2>Clustering Properties</h2>
<p><img alt="Clustering Properties" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-86aae74f2faf42ee.png"/></p>
<p><img alt="Quiz 7" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-63f54274188aadff.png"/></p>
<p><img alt="Impossibility Theorm" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-95f68b2324e24ae7.png"/></p>
<ul>
<li>It's not possible to have all three properties.</li>
</ul>
<p><img alt="Recap" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-478d2f4670efbf73.png"/></p>
<pre><code>2016-03-02 初稿, SLC
2016-03-14 K-Means
2016-03-15 EM
</code></pre>
