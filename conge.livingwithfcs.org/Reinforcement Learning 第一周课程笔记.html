<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2015/2015-08-22-Reinforcement-Learning--di-yi-zhou-ke-cheng-bi-ji-.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2015/08/22/Reinforcement-Learning--di-yi-zhou-ke-cheng-bi-ji-">站点原文</a></h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p>哎呀呀，上这个课被虐了呀。看别的课程都1.5倍速，看这个视频还不到0.5倍速。</p>
<hr/>
<h1>Decision Making and Reinforcement Learning</h1>
<p><img alt="" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-7486b8a884c8a02e.png"/></p>
<p><strong>Supervised Learning</strong>: y = f(x), given many <em>x</em>, <em>y</em> pairs, determine function <em>f</em> to determine the relationship between <em>x</em> and <em>y</em>.</p>
<p><strong>Unsupervised Learning</strong>: f(x), given a lot of <em>x</em>, determine function <em>f</em> to cluster or describe <em>x</em>.</p>
<p><strong>Reinforcement Learning</strong>: y = f(x) with <em>z</em>, given <em>x</em> and <em>z</em>, determine function <em>f</em> which can generate <em>y</em>.</p>
<p>RL is one of the ways to perform decision making.</p>
<hr/>
<p><img alt="When trainsition is stochastic, what's the probability of the sequece?" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-5879e142c65adc80.png"/></p>
<h2>Markov Decision Processes</h2>
<p><img alt="MDP" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-62523e167fc61b2a.png"/></p>
<p><strong>States</strong>: set of tokens which describe every state that one could be in,  </p>
<p><strong>Model/Transition Function</strong>:  probability that you'll end up transition to s' given you are in state s and taking action a. This is 'physics' or rules of the world(not changeable) . </p>
<p><strong>Action</strong>: A(s), things you can do in a given state, a function of state, rule you can play; or A, a set actions not depends regardless of state.</p>
<p><strong>Reward</strong>: scalar value that you get for being in a state. Usefulness of entering a state/and taking an action/and ending up into s' </p>
<p><strong>S, T, A and R define the problem</strong>, policy is the solution,</p>
<p><strong>Policy</strong>: the solution, a function that takes up a state and tells an action that you'll take &lt;s,a&gt; while <em>a</em> is the correct action you want to take to maximize the reward </p>
<p><strong>*Optimal policy</strong>:  π*, optimized, maximize long term expected reward. (note, from &lt;s,a,r&gt;, you know <em>s</em>, a and then know the <em>r</em>, based on rewards, find π*)</p>
<p>Recall that in <strong>Reinforcement Learning</strong>: y = f(x) with <em>z</em>, given <em>x</em> and <em>z</em>, determine function <em>f</em> which can generate <em>y</em>. In Markov process, we know s and r, and we need to learn π to determine a. So,  s--&gt;x, r--&gt;z,  a--&gt;y, π--&gt;f </p>
<p><strong>Markovian property </strong>:</p>
<ul>
<li><p>--&gt; only the present matters; the transition state only depends on the current state </p>
</li>
<li><p>--&gt; Transition model is stationary, rules doesn't change </p>
</li>
</ul>
<h2>MDPs: More About Rewards </h2>
<p><img alt="More About Rewards" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-6fb64ba6fbfbca9c.png"/></p>
<p><strong>Delayed reward</strong>: in each state, we need figure out what action should we take to get the ultimate best rewards at the end.</p>
<p><img alt="Minor changes matter" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-d21e38a10beb8c2d.png"/></p>
<p>** Minor changes matter: By assigning small negative rewards in each state (except the absorbing state), the learning agent is encouraged to take one of the available actions to leave the current state and pursuit a better outcome.</p>
<p>If R(<em>s</em>) is large positive, it encourages you to stay in the game. If R(<em>s</em>) is large negative (&lt; <strong>-1.6294</strong>), it encourages you to leave the world ASAP.</p>
<p>Reward is important to define learning behaviour. <strong>Rewards depends on domain knowledge</strong>. Changing the reward will change the goal of agent.</p>
<p><img alt="Quiz 3: Actions to take given different rewards for the blue states." src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-2897a85299972c9c.png"/></p>
<ul>
<li>rationale: when the rewards are positive, the goal becomes never leaving the game. When the rewards are negative, the goal becomes leaving the game ASAP.</li>
</ul>
<hr/>
<h2>sequences of Rewards: Assumptions</h2>
<p><img alt="Assumptions" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-29e2301012380eb7.png"/></p>
<ul>
<li>infinite horizons: if the time step is finite, the policy might change: π(s,t) --&gt; a, and the Markovian property "π is stationary" will not stand anymore. So, we need to assume we have infinite horizons.</li>
</ul>
<p><img alt="untility of sequeces" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-00a991b8d17242ed.png"/></p>
<ul>
<li>Utility of Sequences: <strong>stationary preferences</strong>, Utility of all the states is the sum of the rewards of all states.</li>
</ul>
<p>Since there are infinite number of states, the top sequence and the bottom sequence are essentially the same utility ∞ since the reward are all positive if we use the equation above.</p>
<p>So a new equation is introduced below by introducing a discount factor γ:</p>
<p><img alt="Bounded Equation" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-56a31b0cb328855c.png"/></p>
<p>The equation introduced Discounted factor of Rewards and made the right side of the equation a geometric series so that the utility of sequences is bounded at Rmax/(1-γ), where 0 ≤ γ &lt;1. This way, the infinite horizons is bounded by a finite number.</p>
<p><img alt="公式推导" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-7090735d8ca55ba6.png"/></p>
<hr/>
<h2>Policies</h2>
<p><img alt="Policies" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-3c444b1449213e09.png"/></p>
<p><strong>Policy</strong> is A mapping from state to action</p>
<p><strong>Optimal policy</strong> π*: the policy allow us getting the largest expected discounted rewards when we follow the policy π* .</p>
<p><strong>Utility</strong> of a state <strong>U(s)</strong> is long term expected discounted rewards from <strong>this point on</strong>; It's delayed reward.</p>
<p>While Reward is immediate satisfaction.</p>
<p><img alt="Bellman Equation" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-ba130e1746d7d0a0.png"/></p>
<p>π*(s) is the optimal policy given current state. <strong> U(s)</strong> is the utility of the state if follow π*. Thus we get the utility of the current state is reward of current state [R(s)] plus the discounted utility from this point on. This is the Bellman Equation.</p>
<hr/>
<p><img alt="Finding policy: value iteration" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-a968f8f7f221ac60.png"/></p>
<p>The Bellman equation has <em>n</em> equations and <em>n</em> unknowns. Because the equations are non-linear (because the max operation), the equations are not easy to solve.</p>
<p>Here list the way to solve the problem: by setting an <strong>arbitrary utility</strong> and update utility based on neighbours, and repeat until converge. This method is called <strong>value iteration</strong>. Neighbours are any state that we can reach.</p>
<p>The reason that VI can work is the reward of each state is true and the utility is discounted.</p>
<p><img alt="quiz 4: value iteration" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-2de4ee00e8f43e0d.png"/></p>
<ul>
<li>in this quiz, assuming the initial U of all states but the terminal states is 0. Compute the U at first and second value iteration.</li>
</ul>
<p><img alt="Finding Policy: policy iteration" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-3b36dd8f312c5117.png"/></p>
<p>By starting with a guessed policy, the Bellman equation don't have to do Max when calculating the expected utility. So there are n linear equation with n unknowns, and it's fairly easy to get the estimation of utility.</p>
<hr/>
<h2>The Bellman Equation</h2>
<p><img alt="The Bellman Equation" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-f0627a3bb06ce714.png"/></p>
<p>V(s) is infinite sequence with sub V. By regrouping the Value function, we can get Q function as:</p>
<p><img alt="Value form and quality form of Bellan Equations" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-605b4ee03c969c52.png"/></p>
<p>and C function as:</p>
<p><img alt="C funciton" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-860883d0bd9cdeba.png"/></p>
<p><img alt="quiz 5: the correct form of C function" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-5170f694e487331b.png"/></p>
<ol>
<li>In the first equation, we do not know which action <em>a</em> to use to transition into state <em>s</em> ', so we cannot compute the transition function.</li>
<li>Correct answer. In this formula, all the variables are properly defined, and there is a meaningful relationship between the continuation function and the reward function.</li>
<li>Notice that there is no meaningful dependency of the max operator on the parameter <em>r</em> (which is already passed in to the continuation function). Also, the variable <em>r</em> ' is never defined.</li>
<li>In this equation, the state <em>s</em> '' is not defined, so it does not make sense to pass it into the recursive call of the continuation function.</li>
</ol>
<p><strong>What's the difference between V form and Q form and C form of Bellman equation?</strong></p>
<ol>
<li>in value function, V is connected by transition T and reward r. we must know transition and the reward of current state s. V is updated with respect to a state.</li>
<li>in quality function, we can rely on experience to update Q, this is useful in reinforcement learning when we do not know T and R ahead of time. Q-value is updated with respect to an action.</li>
</ol>
<p><img alt="" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-00a4fee30c40091e.png"/></p>
<blockquote><p><img alt="Value form and quality form of Bellan Equations" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-605b4ee03c969c52.png"/></p>
<p><img alt="C funciton" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-860883d0bd9cdeba.png"/></p>
</blockquote>
<p>We don't need reward function when we convert Q to C or V. We can convert C to V and q without knowing the transition function.</p>
<h2>Recap</h2>
<p><img alt=" Recap" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-63a585c67ee0149b.png"/></p>
<pre><code>2015-08-25 初稿
2015-12-02 revised 好多知识点第n次看才看懂啊……
</code></pre>
