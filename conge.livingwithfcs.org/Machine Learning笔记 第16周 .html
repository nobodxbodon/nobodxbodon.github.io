<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2016/2016-05-01-Machine-Learning-bi-ji---di-16-zhou--.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2016/05/01/Machine-Learning-bi-ji---di-16-zhou--">站点原文</a></h2>
<hr/>
<h2>layout: post
title: "Machine Learning笔记 第16周 "
date: "2016-05-01 09:24:18"
categories: 计算机科学
excerpt: "Machine learning 从第10周之后我就没再更新。一个原因是自己根本没时间学习，另一个原因是剩下的部分中所有内容，都是在我去年上过的..."
auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p>Machine learning 从第10周之后我就没再更新。一个原因是自己根本没时间学习，另一个原因是剩下的部分中所有内容，都是在我去年上过的另外一门课 Reinforcement Learning 中讲到了。需要那些笔记的话，直接去往下面的链接。 <a href="http://www.jianshu.com/p/881ab7e41adb">Reinforcement Learning 第一周课程笔记</a> : MDP；<a href="http://www.jianshu.com/p/e294d3f5237c">Reinforcement Learning 第十二周课程笔记</a>： Game Theory I；<a href="http://www.jianshu.com/p/ee3f9a553cd2">Reinforcement Learning 第十三周课程笔记</a>： Game Theory II &amp; III。</p>
<p>下面的这部分内容是RL课中没有，而本课独有的内容。</p>
<p>Reinforcement Learning</p>
<p><img alt="RL" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a9cb8480f4a9d306.png"/></p>
<p><img alt="API" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-b9929cc2fc5d8c40.png"/></p>
<ul>
<li>MDP: Model -&gt; planner -&gt; policy</li>
<li>transitions(sars) -&gt; leaner -&gt; policy</li>
</ul>
<p><img alt="Reinforcement learning history" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-2c2d0012ea52b74b.png"/></p>
<ul>
<li>RL = reward maximization.</li>
</ul>
<p><img alt="More RL &amp;quot;APIs&amp;quot;" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-4560d8ce19d2d326.png"/></p>
<p><img alt="What do you call these?" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-802590c140a63e59.png"/></p>
<p><img alt="Three ways of solving RL problems" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-272eed3d844b7348.png"/></p>
<ul>
<li>From Direct use/ indirect learning -&gt; indirect learning/direct use of the policy.</li>
</ul>
<ul>
<li>Policy search: find the optimal policy which can get the right action in given state (s).</li>
<li>Value-function based: find U which can return the best value (v) for state (s). apply argmax on this will generate policy for policy search.</li>
<li>Model-based approach: based on transition function and reward function, we can get the next state (s') and reward for the current state (s) and action (a) pair. These can be used to solve the Bellman equations and eventually solve value function and given policy.</li>
</ul>
<p><img alt="Q-learning" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-c2ff454b627ea8d4.png"/></p>
<p>With Q, we can find out U or PI without knowing transition or action. This is why Q learning works.</p>
<p><img alt="what Q-learning can do" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-acf61ed25ef17627.png"/></p>
<h2>Estimating Q From Transitions</h2>
<p><img alt="Paste_Image.png" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-45a55f200b62db7b.png"/></p>
<ul>
<li>Estimated Q,(Q hat) is the utility of next state plus the current learning rate discounted by the learning rate alpha.</li>
<li>V and be estimated by changing alpha</li>
</ul>
<p><img alt="what V converges to?" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-0041329c99cec99b.png"/></p>
<p>V will converge to the estimated value of X when alpha satisfies: all alphas sum to infinity, but all alpha square sum to a certain number. (e.t alpha = 1/t).</p>
<p><img alt="Q learning proof" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-c03e2713086e4e43.png"/></p>
<p>The first step is a bit ambiguous because Q-hat changes over time. But it works in practice.</p>
<p><img alt="Q learning steps" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-01d05ccb1223136a.png"/></p>
<p>Q-learning only works if s,a visited infinitely often. and alpha&lt;sub&gt;t&lt;/sub&gt; satisfy the conditions that it sums to infinity but the square of it sums to something less than infinity.</p>
<p><img alt="Paste_Image.png" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-eca071ce3195a29d.png"/></p>
<ul>
<li>always choose a&lt;sub&gt;0&lt;/sub&gt;</li>
<li>choose randomly</li>
<li>use Q-hat</li>
<li>"greedy" (local min) choose a&lt;sub&gt;0&lt;/sub&gt; if a&lt;sub&gt;0&lt;/sub&gt; is awesome</li>
<li>annealing</li>
</ul>
<p><img alt="Greedy exploration" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-11b5e537e59913c4.png"/></p>
<p>exploration &amp; exploitation.</p>
<p>Wrap up
<img alt="wrap up" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-daa863bd8825d112.png"/></p>
<p>今天就要考试了，我根本就没复习好。祝我好运吧。</p>
<pre><code>2016-04-30 初稿
</code></pre>
