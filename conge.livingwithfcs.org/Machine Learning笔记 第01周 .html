<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2016/2016-01-23-Machine-Learning-bi-ji---di-01-zhou--.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2016/01/23/Machine-Learning-bi-ji---di-01-zhou--">站点原文</a></h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p>本学期学习Machine Learning。本课程在Udacity上有<a href="https://www.udacity.com/course/machine-learning--ud262">免费版本</a>，并提供详细的笔记。本文的笔记是超级脱水版，目的是自用。</p>
<p>Week 01 tasks</p>
<ul>
<li>Lectures: Decision Trees, Regression and Classification, and Neural Networks.</li>
<li>Reading:  Chapters 1, 3, and 4 of Mitchell</li>
</ul>
<h2>SL1 Decision Trees</h2>
<blockquote><ul>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/313488098/Lesson%201%20Notes.pdf">detailed notes</a></li>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/313488098/ID3%20Algorithm%20for%20Decision%20Trees.pdf">ID3 Algorithm for Decision Trees</a></li>
</ul>
</blockquote>
<p><img alt="Classification and regression" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-444865757053bae6.png"/></p>
<ul>
<li>Classification is simply the process of taking some kind of input,
x, and mapping it to some discrete label.</li>
<li>And regression is mapping from some input space to some real
number</li>
</ul>
<p><img alt="Quiz1: Supervised Learning" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-5e30c92bc2a946de.png"/></p>
<p><img alt="Classification learning terms" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-d596628e5147d365.png"/></p>
<ul>
<li>Instances is input;</li>
<li>Concept is the function to generate labels</li>
<li>Target concept: answer the question</li>
<li>Hypothesis: all possible concepts?</li>
<li>Sample: the training set</li>
<li>candidate: all concepts of the target concept</li>
<li>testing set<ul>
<li>TEsting set should never be the same as the training set to show generalization.</li>
</ul>
</li>
</ul>
<p><img alt="Decision trees" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-437816e07f715374.png"/></p>
<ul>
<li>start with the root node</li>
<li>edges represent different choice</li>
<li>leaf is final output</li>
</ul>
<p><img alt="Quiz 2: Representation" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-c5248d6ef1e8f722.png"/></p>
<p><img alt="20 Question" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-2abe6d687ab77c74.png"/></p>
<p><img alt="20 Question algorithm" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-bad65eb9e947a2d3.png"/></p>
<p><img alt="Quiz 3: Best Attribute" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-a5c4e9ee7270c0ee.png"/></p>
<p><img alt=" Decision Trees Expressiveness AND" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-0c61e4695bb6d130.png"/></p>
<p><img alt=" Decision Trees Expressiveness OR" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-131fac439c3ac236.png"/></p>
<p> <img alt="Decision Trees Expressiveness XOR
" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-569e7378a4e183a7.png"/></p>
<ul>
<li>A and B are commutative so that switching the position of A and B on the trees above is OK</li>
</ul>
<p><img alt="Decision Trees Expressiveness Or and XOR generalization" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-61092d0ae3a3e27b.png"/></p>
<ul>
<li>Linear tree: number of nodes = number of attributes</li>
<li>exponential tree: number of nodes grows exponentially to the number of attributes </li>
</ul>
<p><img alt="Decision Tree Expressiveness Quiz 4 and 5" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-d95bbdf5185bcd82.png"/></p>
<ul>
<li>the number possible trees can be huge</li>
</ul>
<p><img alt=" ID3 algorithm" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-4ac71de57eaa2c81.png"/></p>
<ul>
<li>information gain:</li>
<li>What is Entropy? - a measure of randomness。 ∑P(v) log(P(v))</li>
</ul>
<p><img alt="ID3 Bias" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-beeb83f79c27a4d1.png"/></p>
<ul>
<li>The inductive Bias of ID3 is (Preference bias)<ul>
<li>good splits at top</li>
<li>correct over incorrect</li>
<li>shorter trees</li>
</ul>
</li>
</ul>
<p><img alt="Quiz 6: Decision Trees Other Considerations" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-0f864f06cb177066.png"/></p>
<ul>
<li>For continuous-valued Attributes, we can group them by a range</li>
<li>It does not make sense to repeat a discrete-valued attribute, but continuous attribute could be repeated if a different question is asked.</li>
</ul>
<p><img alt="Decision Trees Other Considerations" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-e9ad7f01e193efaf.png"/></p>
<ul>
<li>all attributes are correctly classified: this can't happen if data is noisy.</li>
<li>Run out of attributes: if attributes are continuous, will never run out.</li>
<li>No overfitting: pruning.</li>
</ul>
<p><img alt="Wrap up" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-5b1c7747e2fcc0a8.png"/></p>
<hr/>
<h2>SL2: Regression and Classification</h2>
<blockquote><ul>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/312357973/Lesson%202%20Notes.pdf">Lesson 2 Notes</a></li>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/312357973/Linear%20Regression%20Review.pdf">Linear regression overview</a></li>
</ul>
</blockquote>
<p>Recap: Supervised learning: learn from pairs of input and output, then given a new set of input, predict the output. This is mapping input to output. If the output is discrete, it's classification. If the output is continuous, it is Regression.</p>
<p><img alt="Quiz 1: " src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-fbf35109ad1dcd45.png"/></p>
<ul>
<li>Originally, regression means regress to the mean</li>
</ul>
<p><img alt="Regression and function approximation" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-4172bc5b7cd30236.png"/></p>
<ul>
<li>regression now means the find the function to represent to the relationship of 2 variables.</li>
</ul>
<p><img alt="Regression, the best line" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-1002183f61a74a4a.png"/></p>
<ul>
<li>the green line is the best fit linear line, but is it the best line?</li>
</ul>
<p><img alt="Quiz 2: how to find the best line" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-52312b0ea0e20a99.png"/></p>
<p><img alt="Quiz 2:  answer" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-b9c57a3936a4c903.png"/></p>
<ul>
<li>the best constant function is the mean of Y.</li>
</ul>
<p><img alt="Order of Polynomial" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-b06e73f35b4c44ff.png"/></p>
<ul>
<li>in the case, the error for order = 8 is zero</li>
</ul>
<p><img alt="Order of PolyNomial: Error function" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-0e127045de63bbe4.png"/></p>
<ul>
<li>Problem: overfitting</li>
</ul>
<p><img alt="Quiz 3: find best function" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-d935c4d108638a60.png"/></p>
<p>Polynomial Regression</p>
<p><img alt="Polynomial Regression" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-b242c01792bf492c.png"/></p>
<p><img alt="Polynomial Regression" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-0691537d6f193850.png"/></p>
<ul>
<li>here shows the polynomial regression represented by matrix and vectors.</li>
<li>the coefficient is computable</li>
</ul>
<p>Errors</p>
<p><img alt="Sources of error " src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-4f198d423fc1cbb9.png"/></p>
<ul>
<li>Sensor Error: The actual reading was 10, but a moth landed on the sensor so it read 0 instead. </li>
<li>Malicious Error: An intelligent malicious agent got in between the measurement and the receiver of the data, they edited the data to say what they wanted it to say, rather than what it actually was. </li>
<li>Transcription Error: A machine copied a number from one place to another and it flattened all of the E notation floats to a bare integer. Or a program cast a UTF16 hieroglyphic to a Unicode pile of poo. </li>
<li>Unmodeled influences: Suppose we are predicting house prices based on square footage and the number of bathrooms. The house price sold for very low value and the reason was that of an unmodeled influence, that there was mold in the attic and walls. The unmodeled influence caused the Machine Learning to fail at predicting a low house price.</li>
</ul>
<h3> Cross Validation</h3>
<p><img alt="Cross Validation" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-e3c215afe3ecb132.png"/></p>
<ul>
<li>the goal is to generalize to the world, not to fit certain training or testing data set perfectly.</li>
<li>we need the training and testing data to be IID: independently identically distributed (Fundamental assumption)</li>
<li>We can split the data into folds and training while leaving one out and use the one for testing. then we average the error of all combinations. Pick the model with the lowest cross-validation error.</li>
</ul>
<p><img alt="Fitting curve" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-8adb383c57d74347.png"/></p>
<p><img alt="Other input spaces" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-4085bb769ef5a26b.png"/></p>
<p><img alt="Recap Regression" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-c72fe7ae616dc6f6.png"/></p>
<hr/>
<h2>SL3 Neural Networks</h2>
<blockquote><ul>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/315142919/Lesson%203%20Notes.pdf">Lesson 3 Notes</a></li>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/315142919/Neural%20Networks.pdf">Neural Networks</a></li>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/315142919/Gradient%20Descent.pdf">Gradient Descent</a></li>
</ul>
</blockquote>
<p><img alt="Neuro Networks" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-aa80489937eea476.png"/></p>
<ul>
<li>A neuron will get input if all the input reach the firing threshold, it will fire</li>
</ul>
<p><img alt="Perceptron" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-b42ac629efaacd56.png"/></p>
<ul>
<li>X&lt;sub&gt;1&lt;/sub&gt;,X&lt;sub&gt;2&lt;/sub&gt;,... are inputs</li>
<li>w&lt;sub&gt;1&lt;/sub&gt;,w&lt;sub&gt;2&lt;/sub&gt;,... are weights</li>
<li>if the sum of all the weighted inputs is <em>activation</em>, if it passes a threshold θ, the output y=1; if not, y=0.</li>
</ul>
<p><img alt="Quiz 1: output is given inputs and weights" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-493678d0a0eaa733.png"/></p>
<p><img alt="How Powerful is a Perceptron Unit" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-0dd4048b26303d13.png"/></p>
<ul>
<li>weight matters a lot when deciding the line to split the plane.</li>
</ul>
<p><img alt="Quiz 2: Neural network can represent AND " src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-e77f97cd2d687a9d.png"/></p>
<ul>
<li>When X&lt;sub&gt;1&lt;/sub&gt;=0 and X&lt;sub&gt;2&lt;/sub&gt;=0, y=0</li>
<li>When X&lt;sub&gt;1&lt;/sub&gt;=0 and X&lt;sub&gt;2&lt;/sub&gt;=1, y=0</li>
<li>When X&lt;sub&gt;1&lt;/sub&gt;=1 and X&lt;sub&gt;2&lt;/sub&gt;=0, y=0</li>
<li>When X&lt;sub&gt;1&lt;/sub&gt;=1 and X&lt;sub&gt;2&lt;/sub&gt;=1, y=1; so y represents AND</li>
</ul>
<p><img alt="Quiz 3: Neural network can represent OR" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-75ebb55e2d31382d.png"/></p>
<p><img alt="Quiz 4: Neural network can represent NOT" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-48878c8b06e86f86.png"/></p>
<p><img alt="Quiz 5: Neural network can represent XOR" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-0cb9b34763141bfd.png"/></p>
<ul>
<li>XOR = OR - 2 * AND</li>
</ul>
<h3>Perceptron Training</h3>
<p><img alt="Perceptron Training" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-e07a714664c561d7.png"/></p>
<p><img alt="Perceptron Training" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-495afe39b1a55e7b.png"/></p>
<ul>
<li>the Perceptron Rule updates weight with weight_change ( Δw&lt;sub&gt;i&lt;/sub&gt;). ( Δw&lt;sub&gt;i&lt;/sub&gt;) is defined by the learning rate, the difference between target and output and input.</li>
<li>if the data is linearly separable, Perceptron rule will find it, in finite iterations ( but it's hard to know how many iterations are needed)</li>
</ul>
<p><img alt=" Gradient Descent" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-7fbfba00424d3c2e.png"/></p>
<p><img alt="Quiz 6: Comparison of Learning Rules Quiz" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-9107e823511125eb.png"/></p>
<p><img alt="Sigmoid" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-0555904869603df1.png"/></p>
<p><img alt="Neural Network" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-d1dfa79405009549.png"/></p>
<p><img alt=" Optimizing Weights" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-7c8ac8f186a73fbc.png"/></p>
<p><img alt=" Restriction Bias" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-5fef7108a371a82f.png"/></p>
<p> Preference Bias</p>
<p><img alt="Preference Bias" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-289df1980dbd6ff2.png"/></p>
<ul>
<li>Preference bias tells you something about the algorithm that you are using to learn. </li>
<li>Prefer simpler explanations</li>
<li>do not multiply unnecessarily ( not fitting the data).</li>
</ul>
<h3>Summary</h3>
<p><img alt="Summary" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-d95ed6e37ad5faff.png"/></p>
<p>这些内容本该是Jan 11 – 17, 2016之间完成的，但是因为准备面试，拖到了一周之后。于是不得不一周补两周的内容。现在去看本周内容了……下次不要拖了，一拖就压力陡增啊。</p>
<pre><code>2016-01-21 看到 Cross Validation in SL2
2016-01-22 继续SL3，初稿完成。
</code></pre>
