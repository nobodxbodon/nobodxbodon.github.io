<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2016/2016-02-19-Machine-Learning-bi-ji---di-06-zhou-.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2016/02/19/Machine-Learning-bi-ji---di-06-zhou-">站点原文</a></h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p>Week 06 tasks:</p>
<ul>
<li>Lectures: Randomized Optimization.</li>
<li>Reading: Chapter 9 of Mitchell, as well as the overview on the No Free Lunch Theorem.</li>
</ul>
<p><img alt="Randomized Optimization" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-caf3c9112399c651.png"/></p>
<p><img alt="Optimization" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-793ee932acc99772.png"/></p>
<ul>
<li>optimization is to find a x&lt;sup&gt;*&lt;/sup&gt; in input space X to make the objective function has the maximum (or close to maximum).</li>
</ul>
<h2>Optimize Me</h2>
<p><img alt="Quiz: best x" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-c84e02010ff65c18.png"/></p>
<ul>
<li>The first question, since the input space is small, we can plot the function and find the largest f(x).</li>
<li>the second the functions, we can try to use calculus: Set the derivative f(x) to 0 and solve the derivative function ( which will be hard to solve since it's a cubic function). Alternatively, we can try to plot the data using subgroup of x.</li>
</ul>
<h2>Optimization Approaches</h2>
<p><img alt="Optimization Approaches" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-915c869fca69aa8b.png"/></p>
<ul>
<li>Three approaches:<ul>
<li>generate &amp; test: when we have a small input space but a complex function</li>
<li>using calculus: when the function has derivative and the derivative function is solvable when its value is set to 0</li>
</ul>
</li>
<li>Newton's method: when the function has derivative and it can be improved iteratively.</li>
<li>When these approaches does not work, then <strong>Randomized Optimization</strong>.</li>
</ul>
<p><img alt="Hill Climbing" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-ba928c8c03ef0b4f.png"/></p>
<ul>
<li>the hill climbing algorithm will stuck at a <strong>local maxima</strong>.</li>
</ul>
<p><img alt="Quiz 2: find my word" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-c39d7c1515aca84c.png"/></p>
<h2>Random Restart Hill Climbing</h2>
<p><img alt="Random Restart Hill Climbing" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-273a0bb7f5b14d05.png"/></p>
<ul>
<li>how to avoid the unlucky starting point?<ul>
<li>checking and make sure the starting point is not visited before</li>
<li>make sure the starting point is faraway from known local maxima</li>
</ul>
</li>
</ul>
<p><img alt="Quiz3: randomized Hill climbing" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-90b89394de06def1.png"/></p>
<ul>
<li>The six points under the maxima hill are special<ul>
<li>4 points under the global maxima will take average 4 steps to get to the global maxima. and that's the 4/28*(4)</li>
<li>the 2 basin points, half of the time will lead up in global maxima, and half of the time will lead to local maxima (and restart).</li>
</ul>
</li>
<li>all the other 22 points will lead to local maxima and then start a new round with a randomized starting point.</li>
<li>if keep track of the points visited, the algorithm might do better than 28 on average.</li>
<li>if the attraction basin under the local maxima is large, hill climbing will do better.</li>
</ul>
<h2>Annealing algorithm</h2>
<p><img alt="Simulated Annealing" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-6c9f354d70ee651e.png"/></p>
<ul>
<li>One step further than hill climbing: allow downwards to "explore"</li>
</ul>
<p><img alt="Annealing algorithm" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-85b6a8cdb56911b6.png"/></p>
<ul>
<li>if f(x&lt;sub&gt;t&lt;/sub&gt;) and f(x) is very close, then f(x&lt;sub&gt;t&lt;/sub&gt;) -f(x) is close to 0 and p will be 1.</li>
<li>If T is really big, say infinity, then f(x&lt;sub&gt;t&lt;/sub&gt;) -f(x) doesn't matter then. P will be 1, and x will jump to x&lt;sub&gt;t&lt;/sub&gt;</li>
<li>if T is really small, close to 0, p will climb, not jump.</li>
</ul>
<p><img alt=" Properties of Simulated Annealing" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-65210d36eff5b72a.png"/></p>
<ul>
<li>T -&gt; 0: like hill climbing</li>
<li>T-&gt; ∞: random walk</li>
<li>Desearse T slowly:  As T decreases, it's harder and harder to jump over local maxima, and algorithm will reach global maxima</li>
</ul>
<h2>Genetic Algorithms</h2>
<p><img alt="GA" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-2e25936150f0be16.png"/></p>
<ul>
<li>GA is like random restart but doing the restart in parallel.</li>
<li>Crossover - population holds information.</li>
</ul>
<p><img alt=" GA Skeleton" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-6c6754dbf31031e0.png"/></p>
<ul>
<li>Define "most fit"<ul>
<li>top half or truncate selection:the half individuals with the largest score</li>
<li>weighted prob. or roulette wheel: weighted fitness function with probability.</li>
</ul>
</li>
<li>what's crossover?</li>
</ul>
<p><img alt="Crossover Example" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-9d43da8681c2a253.png"/></p>
<ul>
<li>Uniform crossover: randomization at each position</li>
</ul>
<p><img alt="Recap" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-b778c74f2f9ab60f.png"/></p>
<h2>MIMIC</h2>
<ul>
<li><a href="http://www.cc.gatech.edu/~isbell/papers/isbell-mimic-nips-1997.pdf">Charles' paper on MIMIC</a></li>
</ul>
<p><img alt="MiMIC" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-6bc2056cc3077470.png"/></p>
<p><img alt="Quiz 4:" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-c361ac5d21a7e5a3.png"/></p>
<ul>
<li>when θ is min, then P is the uniform distribution of X</li>
<li>when  θ is max, we get the maxima or optima of the distribution</li>
</ul>
<p><img alt="Pseudo code" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-c6fbe26d31b05f63.png"/></p>
<ul>
<li>Given a threshold θ&lt;sub&gt;t&lt;/sub&gt;, generate samples from P&lt;sup&gt;θ&lt;sub&gt;t&lt;/sub&gt;&lt;/sup&gt;(x), then set θ&lt;sub&gt;t+1&lt;/sub&gt; to be n&lt;sup&gt;th&lt;/sup&gt; percentage and retain only those samples, s.t. f(x)&gt;=θ&lt;sub&gt;t_1&lt;/sub&gt;</li>
<li>instead of using these samples, we estimate P&lt;sup&gt;θ&lt;sub&gt;t+1&lt;/sub&gt;&lt;/sup&gt;(x), and then generate new samples from it. then repeat the process again and again until converge.</li>
<li>Two assumptions:<ul>
<li>P&lt;sup&gt;θ&lt;sub&gt;t&lt;/sub&gt;&lt;/sup&gt;(x) is estimable</li>
<li>P&lt;sup&gt;θ&lt;sub&gt;t+1&lt;/sub&gt;&lt;/sup&gt; is close enough to P&lt;sup&gt;θ&lt;sub&gt;t&lt;/sub&gt;&lt;/sup&gt;</li>
</ul>
</li>
<li>this is a lot like Genetic algorithm but P&lt;sup&gt;θ&lt;/sup&gt; and P&lt;sup&gt;θ＋ ϵ&lt;/sup&gt; can preserve the structure.</li>
</ul>
<p><img alt="Estimating Distributions" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-c3e4aa7a8aafd90a.png"/></p>
<ul>
<li>P(x) is the combination of the p of each x in X=[x1 x2 .... x&lt;sub&gt;n&lt;/sub&gt;] given others in X. To estimate P(x) this way, we will need huge amount of data ( exponential to n).</li>
<li>Dependent tree: each node only have one parent.<ul>
<li>it can represent relationships</li>
<li>and sampling dependent tree is easy</li>
<li>it's a general form of crossover and it does not require locality.</li>
</ul>
</li>
</ul>
<h2>Finding Dependency Trees</h2>
<p><img alt="Finding Dependency Trees" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-92f96ba174026094.png"/></p>
<ul>
<li>Minimize Divergence of P and P_hat&lt;sub&gt;π&lt;/sub&gt;, Divergency can be written as entropy.</li>
<li>build a cost function by remove -h(p) and add entropy -h(x&lt;sub&gt;i&lt;/sub&gt;), then we get the mutual information between x and <strong>π</strong></li>
<li>Finding Dependency Trees is now converted to a problem to maximize mutual information between x and it's parent. or in another word, to find the parent who knows the most about the node.</li>
</ul>
<p><img alt="Finding Dependency Trees" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-e4f4cd310aea0f3b.png"/></p>
<ul>
<li>This can be solved by solving the maximum spanning tree. Using Prim.</li>
<li>The prim algorithms is fairly fast algorithm ( polynomial to the number of edges of a graph) and it's good for dense connected graph.</li>
</ul>
<p><img alt="back to pseudo code" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-786b60e3a31ce2fc.png"/></p>
<ul>
<li>dependent tree is used to estimate P given the samples from the previous step.</li>
</ul>
<p><img alt="Quiz 5: Probability Distribution" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-5ae60dc9cb79b77f.png"/></p>
<ul>
<li>what distribution can represent the maxima of the problems on left column.</li>
</ul>
<p><img alt="Practical Matters" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-e7908601e6234331.png"/></p>
<ul>
<li>MIMIC evaluate cost function less when comparing to Simulated Annealing, but it usually takes longer to run.</li>
<li>MIMIC will get more information in and out</li>
</ul>
<pre><code>2016-02-17 完成 Genetic Algorithms
2016-02-19 初稿
</code></pre>
