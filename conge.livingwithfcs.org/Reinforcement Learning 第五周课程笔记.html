<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2015/2015-09-18-Reinforcement-Learning--di-wu-zhou-ke-cheng-bi-ji-.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2015/09/18/Reinforcement-Learning--di-wu-zhou-ke-cheng-bi-ji-">站点原文</a></h2>
<hr/>
<h2>layout: post
title: "Reinforcement Learning 第五周课程笔记"
date: "2015-09-18 11:05:48"
categories: 计算机科学
excerpt: "本周三件事：看课程视频 Convergence. 读 Littman and Szepesvari (1996), 作业四。 no action..."
auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p>本周三件事：看课程视频 <em>Convergence</em>. 读 <em>Littman and Szepesvari (1996)</em>, 作业四。</p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-cf2b6693fcc36b03.png"/></p>
<p><img alt="Learning without control:" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-8db8c4fe2150fa34.png"/></p>
<ul>
<li>no actions by learner. The value function is that value equals reward R(s) plus discounted expected value of the next state. (T(s, s') is transision function.</li>
<li>The new estimated value function for the state the learner just left is TD(0).</li>
</ul>
<p><img alt="Learning with control/actions." src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-68ab733666d61e66.png"/></p>
<p>Q function is used to estimate the value at current state, take a action then get a reward and ended up in a new state &lt; s&lt;sub&gt;t-1&lt;/sub&gt;, a&lt;sub&gt;t-1&lt;/sub&gt;,r&lt;sub&gt;t&lt;/sub&gt;,s&lt;sub&gt;t&lt;/sub&gt; &gt;.  The Q updating rule takes care of two approximations: 1) if model is known, it can be used to update Q; 2) if Q* is known, it can also be used to update Q.</p>
<p>And both will converge.</p>
<p><img alt="Quiz 1: Bellman Operator" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-d53c693f855032f6.png"/></p>
<ul>
<li>See value iteration <a href="http://www.jianshu.com/p/881ab7e41adb">here</a></li>
</ul>
<p><img alt="Contraction mapping def: If applying the B operator makes the distance between two functions smaller than the the distance between the original functions." src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-7c3977dbd01f8714.png"/></p>
<p><img alt="Quize 2: " src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-fa0691820356d664.png"/></p>
<ul>
<li>Practically, if B is multiplying a number in [0,1), B is contraction mapping.</li>
<li>take the first option for example: ||B(x) - B(y)||=||x/2 - y/2|| =1/2 ||x - y||, so there exists γ &gt;=1/2 makes ||B(x) - B(y)|| &lt;=γ||x - y||. So B(x) = x/2 is a contraction mapping.</li>
</ul>
<p><img alt="Contraction Properties" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-dd729fbea44b9ed8.png"/></p>
<ul>
<li><p>① and ② are true, so that BF&lt;sub&gt;t-1&lt;/sub&gt; = F&lt;sub&gt;t&lt;/sub&gt;, F&lt;sub&gt;t&lt;/sub&gt; will converge at F* through value iteration.</p>
</li>
<li><p>If there is two fix point, G* and F*, Putting them into the B operator will not change the distance of them because both of them are fixed, and this violates the definition of B.</p>
</li>
</ul>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-9bf2051b7eafe222.png"/></p>
<ul>
<li>Applying B operator to Bellman equation and unpacking ||BQ&lt;sub&gt;1&lt;/sub&gt; - BQ&lt;sub&gt;2&lt;/sub&gt;||&lt;sub&gt;∞&lt;/sub&gt;.</li>
<li>Not combining the <em>max</em> a' because the a' are not the same in Q&lt;sub&gt;1&lt;/sub&gt; and Q&lt;sub&gt;2&lt;/sub&gt;</li>
</ul>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-f8ea3d21e4bd4b5e.png"/></p>
<ul>
<li>stop here, first, will go back. that's call this the unfinished proof.</li>
</ul>
<p><img alt="quiz 3" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-340442f9bfd6c502.png"/></p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a49f5787c0ee1ee1.png"/></p>
<ul>
<li>Max is non-expansion: ||maxf(a) - maxg(a)|| &lt;= max|(f(a) -g(a)|. Using this</li>
</ul>
<p><img alt="Statement of therom" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-0f4d33c91c04799c.png"/></p>
<p>The three properties need to be true for Q&lt;sub&gt;t&lt;/sub&gt; to converge to Q*. and they are true</p>
<p>So, Q-learning converges.</p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-5d6cfb5b05d86892.png"/></p>
<p>B&lt;sub&gt;t&lt;/sub&gt; is the operator we are going to update Q at <em>t</em> time step. <em>Q(s,a)</em> is the Q function value of the state we just left and Q function <em>w</em> is the Q value of the state we just arrive. In regular Q Learning update, Q and <em>w</em> are the same, here we separated them in the theorem.</p>
<p>So, rule number one is saying if we know the Q* and use the updating rule (in pink) to update the Q function,  the expected value of the one-step look ahead (<em>w</em>) can be calculated and the stochasticity will be averaged out.</p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-eda5a85c25ff1978.png"/></p>
<p>If we hold the Q(s,a) fixed and only varies the way we calculate the one-step look ahead, the distance between the Q* and Q can only get closer with each update.</p>
<p>The third condition is the learning rate condition.  which is needed for Bellman equation.</p>
<p><img alt="Quiz 4: 1. decision making on estimated value based on best next action (regular MDP). 2. The environment puts you in the worst possible state and you choose the next best action given the state (risk averse); 3. The state is the expected value but the action to take is based on ranking of the action(exploration-sensitive, min, max, mediocre ); 4. (zero-sum game)" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-c3439dd875346192.png"/></p>
<ul>
<li>the take home messages are, these generalized MDPs all converges.</li>
<li>so the correct answers are: (from top to bottom) worst possible decision, not-optimal decision, two agent competing decision making (the zero-sum game).</li>
</ul>
<p><img alt="Recap" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-5587b8046cfcadd7.png"/></p>
<p>Generalized MDP can be seen as redefine fix point. Contraction might be something like "收敛" in Chinese. Q-learning converges to Q*. Generalized Convergence theorem uses two Q-functions to prove the convergence of Bellman Equation.</p>
