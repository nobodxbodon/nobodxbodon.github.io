<h2>åŸæ–‡ï¼š<a href="https://conge.livingwithfcs.org/2016/02/10/Machine-Learning-bi-ji---di-04-zhou-">Machine Learningç¬”è®° ç¬¬04å‘¨</a></h2>
<hr/>
<h2>layout: post
title: "Machine Learningç¬”è®° ç¬¬04å‘¨"
date: "2016-02-10 13:59:40"
categories: è®¡ç®—æœºç§‘å­¦
excerpt: "Week 04 tasksï¼š Lectures:  VC Dimensions and Bayesian Learning. Reading: ..."
auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p>Week 04 tasksï¼š</p>
<ul>
<li>Lectures:  VC Dimensions and Bayesian Learning.</li>
<li>Reading:  Mitchell Chapter 7 and Chapter 6.</li>
</ul>
<hr/>
<h2>SL8: VC Dimensions</h2>
<ul>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/417758568/Lesson%208%20Notes.pdf">Lesson 08 Notes</a></li>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/417758568/VC%20Dimensions%20Review.pdf">VC Dimensions Review</a></li>
</ul>
<p><img alt="Quiz 1: Which Hypothesis Spaces Are Infinite" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-1dce9bfce0118179.png"/></p>
<ul>
<li>m&gt;= 1/Îµ( ln|H|+ln(1/ğ›¿) ). Here the sample size <em>m</em> is dependent on the size of hypothesis |H|, the error <em>Îµ</em> and the failure parameter_Â ğ›¿_. What happens if |H| is infinite?</li>
<li>quiz 1: Which Hypothesis Spaces Are Infinite</li>
</ul>
<p><img alt="Â Maybe It Is Not So Bad" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-16af59ab5565e63a.png"/></p>
<ul>
<li>In the example above, although the hypothesis space is infinite (syntactic), we can still explore the space efficiently because a lot of hypothesis are not that meaningfully different (semantic).</li>
</ul>
<p><img alt="What Does VC Stand For" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-b41b58c51fe96c88.png"/></p>
<ul>
<li>VC dimension: what is the largest set of inputs that the hypothesis class can shatter.</li>
<li>Vapnic-Chervonenkis</li>
</ul>
<p><img alt="Quiz 2: internal training" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-7ebcf827b204ebd9.png"/></p>
<ul>
<li>not sure how to answer this question. need to rewatch.</li>
</ul>
<p><img alt="Quiz 3: Linear Separators" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-97e2e9eca094b42e.png"/></p>
<ul>
<li>Here VC = 3.</li>
</ul>
<p><img alt="The ring" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-6630ff78a04a84b5.png"/></p>
<ul>
<li>the vc dimension is going to end up being d plus 1 because the number of parameters needed to represent a d dimensional hyperplane is __ d plus 1__.</li>
</ul>
<p><img alt="quiz 4: polygons" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-ac5b3e3da2ca55a0.png"/></p>
<ul>
<li>if the hypothesis is that points inside some convex polygon, then the VC = infinite.</li>
</ul>
<p><img alt="Sample size with infinate hypothesis space" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-0cd6eccc48d49904.png"/></p>
<p><img alt="VC of finite H" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-1c31b43c8c9e407d.png"/></p>
<p><img alt="recap lesson 8" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-45a29c1496d24f21.png"/></p>
<hr/>
<h2>Bayesian Learning</h2>
<ul>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/454308909/Lesson%209%20Notes.pdf">Lesson 09 Notes</a></li>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/454308909/Bayesian%20Learning%20Extension.pdf">Bayesian Learning Extension</a></li>
</ul>
<p><img alt="Bayesian Learning" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-4f8709b87ce50261.png"/></p>
<ul>
<li>the best hypothesis is the most probable hypothesis given data and domain knowledge.</li>
<li>argmax&lt;sub&gt;hâˆˆÂ H&lt;/sub&gt; Pr(h|D)</li>
</ul>
<p>Bayes Rule</p>
<p><img alt="Bayes Rule" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-1868eda627d94cb9.png"/></p>
<ul>
<li>Bayes Rule: Pr(h|D) = Pr(D|h)Pr(h)/Pr(D)<ul>
<li>Pr(D) is the prior about data</li>
<li>Pr(h) is the prior of hypothesis, and it's the domain knowledge.</li>
<li>Pr(D|h) is the possibility of data given h, it is much easier than Pr(h|D) to compute.</li>
</ul>
</li>
</ul>
<p><img alt="Quiz 1" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-33a65d121026117a.png"/></p>
<ul>
<li>comparing the probability of one having /not having spleentitis.</li>
</ul>
<h2>Bayesian Learning</h2>
<p><img alt="Bayesian Learning" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-7c5a04709e12c948.png"/></p>
<ul>
<li>to find the largest Pr(D|h), we could drop P(D) for the bayes rule because it doesn't matter since our task is to find the best <em>h</em>. <strong>MAP: maximum a posterior</strong>.</li>
<li>If we don't have a strong prior or we assume the prior is uniform for every <em>h</em>, we can drop Pr(h). ML: __maximulikelihoodod___</li>
<li>the hard part is to look into every h</li>
<li>Since H is often very large, this learning algorithm is not practical</li>
</ul>
<h2>Bayesian Learning in Action</h2>
<p><img alt="Bayesian Learning when the data has no noise" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-c33d108f530ec7cf.png"/></p>
<ul>
<li>given a bunch of data, your probability of a particular hypothesis being correct, or being the best one or the right one, is simply uniform over all of the hypotheses that are in the version space. That is, are consistent with the data that we see.</li>
</ul>
<p><img alt="Quiz 2:" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-54cb514b8ab7b632.png"/></p>
<ul>
<li>given &lt;x,d&gt; pairs, and d&lt;sub&gt;i&lt;/sub&gt; =k * x&lt;sub&gt;i&lt;/sub&gt; which has a probability of Pr(1/2&lt;sup&gt;k&lt;/sup&gt;), what is the probability of <em>D</em> given <em>d</em>.</li>
</ul>
<p><img alt="Bayes learning given gausion error" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-f28b37c3c6278e9f.png"/></p>
<ul>
<li>given training data, figure out f(x) and with its error term. If the error can be modeled by Gaussian function, then</li>
<li>h&lt;sub&gt;ML&lt;/sub&gt; can be simplified to minimizing a sum of squared error.</li>
</ul>
<p><img alt="Quiz 3" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-3bf017b4832ed7c9.png"/></p>
<ul>
<li>find best hypothesis from the three.<ul>
<li>calculate and compare squared error.</li>
</ul>
</li>
</ul>
<p><img alt="Quiz 4: small trees" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a31610986ba0308b.png"/></p>
<ul>
<li>h&lt;sub&gt;MAP&lt;/sub&gt; can be transformed to minimize the length of hypothesis (size of h) and the length of the D|h (which is misclassification error)</li>
<li>there is a tradeoff between size of h and error. this is called minimum description length</li>
<li>there is a unit problem: unit of error and size need to be figured out</li>
</ul>
<h2>Bayesian Classification</h2>
<p><img alt="Bayesian Classification" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-798bc2122020fbc1.png"/></p>
<ul>
<li>when we do the Classification, we will have each hypothesis to vote</li>
</ul>
<p><img alt="Recap" src="/media/wwww/share/study/èšèš/æºæ•°æ®/åšå®¢èšåˆ/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-9142b746f7847502.png"/></p>
<ul>
<li>Bayes optimal classifier = weighted voting by h.</li>
</ul>
<pre><code>2016-02-08 SL8 å®Œæˆ
2016-02-08 å‡Œæ™¨ï¼ŒSL9 å®Œæˆ.ç¬¬ä¸€ç¨¿å‘å¸ƒ

</code></pre>
