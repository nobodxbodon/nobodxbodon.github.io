<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2015/2015-09-01-RL--yue-du-cai-liao-bi-ji--Littman-(1996">仓库源文</a>-Chapters-1-2.md)，<a href="https://conge.livingwithfcs.org/2015/09/01/RL--yue-du-cai-liao-bi-ji--Littman-(1996">站点原文</a>-Chapters-1-2)</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<blockquote><p>Littman, 1996, chapter 1,2</p>
</blockquote>
<h1>Chapter 1: Introduction</h1>
<h2>1.1 Core concepts of Sequential decision making</h2>
<p>Purpose: producing policies that maximize a measure of long-term reward to an agent following it in a specific environment.</p>
<h3>Elements of Sequential decision making</h3>
<ul>
<li>Agent: the system responsible for interacting with the world and making decisions.</li>
<li>Environment: change from state to state (<em>s</em>) in response to the actions (<em>a</em>) of the agent according to a fixed set of rules (T). Environment can be stationary(&lt;-main focus of the paper) or nonstationary (&lt;-hard to model). Environment can have multiple agents.</li>
<li>Reward: the purpose of agents' action is to maximize rewards. Reward is a defining property of models.</li>
<li>Policies: Agent's prescription for actions. plan-&gt; conditional plan -&gt; universal plan or "stationary policy" -&gt; stochastic policy. <strong>Stationary policy</strong> is the most important one in this paper.</li>
</ul>
<p><img alt="Elements of Sequential decision making" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-88859cc624f06a91.png"/></p>
<p>In the figure above, <em>s</em> is state, <em>a</em> is action, T is transition function which changes <em>s</em> given <em>a</em>. R is the reward function and <em>r</em> is the reward of the state. O is perception function (usually it is an identity function) and <em>z</em> is the agent's perception of the environment.</p>
<h3>Problem scenarios：</h3>
<ul>
<li><strong>Planning</strong>: the complete model of the environment is known and a planner takes the model and generate policy.</li>
<li><strong>Reinforcement Learning (RL)</strong>: the learner only knows the agent's perception of the world and action it can take. Environment is unknown. It's up to the agent to generate a policy and follow it.</li>
<li><strong>Model based RL</strong>: building a model with the limited information known to the agent, and then use a planning algorithm to generate a policy.</li>
<li><strong>Simulated RL</strong>: in this case, the model is known but the agent acts like it is not known and use RL algorithm to generate policy. It is used when the environment is too complicated. </li>
</ul>
<p><img alt="Problem scenarios" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-6ad9674f224346bd.png"/></p>
<h2>1.2 Formal Models</h2>
<p>To simplify the problem, the properties of the environment model must follow the following conditions:</p>
<blockquote><p>finite state space; finite actions; sequential environment; accessible and inaccessible environment; Markovian environment (prediction can be made by environment's state); fixed environment; stochastic or deterministic transitions; synchronous environment ( environment won't change until the agent takes an action); with one or two agent.</p>
</blockquote>
<p><img alt="Markovian Models" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-dc7bbf7e2929cc62.png"/></p>
<h2>1.3 evaluation criteria</h2>
<p><strong>Evaluating Policies</strong>: an objective function takes all the &lt;s, a&gt; sequences and their probabilities to generate value. The optimal sequence maximize the objective function. 1) for each &lt;s, a&gt; sequences, objective function replace each transition with a "goodness" value; 2) the values are truncated according to the finite horizon; 3) transition values were summarized for each sequence; 4) this sequence value are summarized to  get a policy value; 5) in multiagent environment, other agent can change transition value.</p>
<p><strong>Evaluating Planning algorithms</strong>: 1) All criteria above; 2) whether an optimal policy can be reached; 3) algorithm efficiency.</p>
<p><strong> RL algorithms</strong>: RL algorithms tries to achieve what planning can do without complete knowledge of the whole environment. If the algorithm can reach optimal policy (converge), then it's good.</p>
<p><img alt="Paste_Image.png" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-5cd6a2ed1c142795.png"/></p>
<hr/>
<h1>Chapter 2: Markov Decision Process</h1>
<p><img alt="The problem to address is, given complete and corrent dynamics of environment find the best way to behave" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-aa84fe211dde76e5.png"/></p>
<p>Core concept of MDP below is extracted from <a href="http://www.jianshu.com/p/881ab7e41adb">my notes</a> of the course lecture.</p>
<blockquote><p><img alt="MDP" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-62523e167fc61b2a.png"/></p>
<p><strong>States</strong>: set of tokens which describe every state that one could be in,  </p>
<p><strong>Model/Transition Function</strong>:  probability that you'll end up transition to s' given you are in state s and taking action a. This is 'physics' or rules of the world(not changeable) . </p>
<p><strong>Action</strong>: A(s), things you can do in a given state, a function of state, rule you can play; or A, a set actions not depends regardless of state.</p>
<p><strong>Reward</strong>: scalar value that you get for being in a state. Usefulness of entering a state/and taking an action/and ending up into s' </p>
<p><strong>S, T, A and R define the problem</strong>, policy is the solution,</p>
<p><strong>Policy</strong>: the solution, a function that takes up a state and tells an action that you'll take &lt;s,a&gt; while <em>a</em> is the correct action you want to take to maximize the reward </p>
<p><strong>*Special policy</strong>:  π*, optimized, maximize long term expected reward. (note, from &lt;s,a,r&gt;, you know <em>s</em>, a and then know the <em>r</em>, based on rewards, find π*) r--&gt;z s--&gt;x a--&gt;y, π--&gt;f </p>
</blockquote>
<h2>Algorithms</h2>
<p><img alt="Value function algorithm" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-dd3e20cd1a05ad50.png"/></p>
<p><img alt="Value Iteration algorithm" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-cb4377a287cbb947.png"/></p>
<p><img alt="Policy iteration algorithm" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-2610cafe3b60422e.png"/></p>
<p><img alt="Linear Programming" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-baa2c95445e5f84e.png"/></p>
<p><img alt="Linear programming dual" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-aa5df06007d39cfe.png"/></p>
<p><img alt="Paste_Image.png" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-e871fa461fff1828.png"/></p>
<pre><code>2015-08-26 第一章完成

第二章待续
</code></pre>
