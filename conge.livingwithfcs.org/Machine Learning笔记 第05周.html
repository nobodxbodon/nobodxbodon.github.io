<h2>原文：<a href="https://conge.livingwithfcs.org/2016/02/12/Machine-Learning-bi-ji---di-05-zhou-">Machine Learning笔记 第05周</a></h2>
<hr/>
<h2>layout: post
title: "Machine Learning笔记 第05周"
date: "2016-02-12 04:57:04"
categories: 计算机科学
excerpt: "Week 05 tasks Lectures:  the remainder of Bayesian Learning and Inferenc..."
auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p>Week 05 tasks</p>
<ul>
<li>Lectures:  the remainder of Bayesian Learning and Inference.</li>
<li>Reading: Chapter 6 of Mitchell.</li>
</ul>
<hr/>
<h2>SL10: Bayesian Inference</h2>
<ul>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/478818537/Lesson%2010%20Notes.pdf">Lesson 10 Notes</a></li>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/478818537/Bayesian%20Inference%20Review.pdf">Bayesian Inference Review</a></li>
</ul>
<h3>Intro</h3>
<p><img alt="Intro" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-eedbc5db0ba74c96.png"/></p>
<ul>
<li>About these probabilistic quantities that we're working with, Is there anything that we need to know about how to represent and reason with them.</li>
<li>Bayesian Networks, which is this wonderful representation for representing and manipulating probabilistic quantities over complex spaces.</li>
</ul>
<p>Joint Distribution
<img alt="Quiz 1: Joint Distribution" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-0bfd7ff222d1a7b2.png"/></p>
<ul>
<li>build on this idea of a joint distribution.</li>
<li>The probability of not have a storm and the the probability of lightning given there is a strom.</li>
</ul>
<h3>Adding Attributes</h3>
<p><img alt="Adding attribute" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-5296e02421e6c29c.png"/></p>
<ul>
<li>each time we add one variable, the number of probabilities that we have to write down will go up as factor of two</li>
<li>factor it: instead of representing all as eight numbers, we can represent it by 2 times 2 time 2.</li>
</ul>
<h3>Definition for conditional independence</h3>
<p><img alt="Definition for conditional independence" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-17b8a00370fbcefe.png"/></p>
<ul>
<li>conditional independence:  the probabilities associated with the values in this variable X Is independent of the value of y given the value of
z.</li>
<li>if we know z, then the probability of x can be figured out without knowing y.</li>
<li><strong>normal independence</strong>: Pr(x,y) =Pr(x)Pr(y)
*<strong>the chain rule</strong>: Pr(x,y) =Pr(x|y)Pr(y)</li>
<li>So that means: Pr(x|y) = Pr(x).</li>
<li>conditional independence gives us: As long as there is some z
that we stick in here, that gives us that property that we can essentially ignore y, when we are talking about the probability of x.</li>
<li>We are factoring that probability distribution</li>
</ul>
<p><img alt="Quiz 2: conditional independence" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-29a7ff17cb7b1f00.png"/></p>
<ul>
<li>find a truth setting for thunder and lightning, such that the probability that thunder takes on that value, given that lightning takes on the value that you give, and the storm is true, ends up equaling the probability that thunder takes on that value given lightning takes on the value that you gave and storm is false.  so a setting here so that basically the value of storm doesn't matter.</li>
<li>No matter what you put in here, the answer will be correct. Why? because Storm doesn't matter.</li>
</ul>
<p><img alt="Quiz 3: belief Networks" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-3d57aab8f9764346.png"/></p>
<ul>
<li>figuring out those probabilities is really easy</li>
</ul>
<p><img alt="quiz 3: solution" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-498354061776822c.png"/></p>
<ul>
<li>the probability depends on the connections between nodes and it can grow exponentially with more variables.</li>
</ul>
<h3>Sampling From The Joint Distribution</h3>
<p><img alt="quiz 4: Sampling From The Joint Distribution" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-6ae81b01f0974d76.png"/></p>
<ul>
<li>topological sort: a standard thing that you can do with a graph, and it's very quick.</li>
<li>The graph must be a directed  <strong>acyclic</strong> one: you can't have arrows that take you back.
.</li>
</ul>
<h3>Recovering the Joint Distribution</h3>
<p><img alt="using bayse net to recover the joint distribution" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-f5bf0ba336915bf2.png"/></p>
<ul>
<li>recover the joint distribution</li>
<li>and that's much more compact a representation ( sing 14 numbers instead of 31 numbers)</li>
</ul>
<p><img alt="Why Sampling" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-88d01663561e369a.png"/></p>
<ul>
<li>With a distribution you can<ul>
<li>tell the probability of a certain value</li>
<li>generate values according to that distribution.</li>
</ul>
</li>
<li>Simulate the distribution. A distribution represents kind of a process, we could duplicate that process by sampling</li>
<li>approximate inference: get a sense how the data is by sampling- machine</li>
<li>Visualization - to get feel of the data - human sense.</li>
</ul>
<h3>Inferencing Rules</h3>
<p><img alt="Quiz 5: Inferencing Rules" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-ba80216e292afa00.png"/></p>
<ul>
<li>marginalization: representing the probability of x by summing over some other variable y and looking at the joint probabilities of those. e.g. <code>P(x) = P(x|y=true) +P(x|y=false)</code></li>
<li>The chain rule: P(x,y) = P(x)p(y|x)=p(y)p(x|y)</li>
<li>Bayes rule: p(y|x)=P(x|y)P(y)/P(x)</li>
<li>question: which tree represents P(x,y) = p(y)p(x|y).</li>
<li>These three together could work out the probability of various kinds of events.</li>
</ul>
<h3>Inference By Hand</h3>
<p><img alt="quiz 6: Inference By Hand" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-2879aa97235ff309.png"/></p>
<ul>
<li>example: 2 boxes with balls in it. Question: what's the probability of drawing a blue ball given that we get a green ball in box 1.</li>
<li>need to apply the marginalization, the chain rule and the bayes rule to break the first probability down.</li>
<li>then use the bayes rule to calculate and normalize P(box=1|1=green) and P(box=2|1=green).</li>
</ul>
<p><img alt="Naive bayes" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-7d5decd84963c9e2.png"/></p>
<ul>
<li>in the spam classification problem. bayes rule can be used to map class from attributes.</li>
<li>it assumes that all the attributions are conditional independent</li>
</ul>
<p><img alt="Why Naive bayes is Cool" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-8236de1445196e7b.png"/></p>
<p><img alt="Wrap up" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-5b8867ec0ea68730.png"/></p>
<pre><code>2016-02-10 stopped at "Recovering the Joint Distribution"
2016-02-11 初稿完成
</code></pre>
