<h2>原文：<a href="https://conge.livingwithfcs.org/2015/12/09/8803-RL-Fall-2015--qi-mo-fu-xi-bi-ji-jing-jian-ban-">8803 RL Fall 2015 期末复习笔记精简版</a></h2>
<hr/>
<h2>layout: post
title: "8803 RL Fall 2015 期末复习笔记精简版"
date: "2015-12-09 00:33:05"
categories: 计算机科学
excerpt: "简介： 这是在复习Reinforcement Learning的期末考的时候做的复习笔记。基本内容就是平时的课堂笔记（参见2015秋季学期笔记 ..."
auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<h2>简介：</h2>
<p>这是在复习Reinforcement Learning的期末考的时候做的复习笔记。基本内容就是平时的课堂笔记（参见<a href="http://www.jianshu.com/p/9985a7b4067c">2015秋季学期笔记 8803 RL and 6440 HIT</a>）的精简版本，涵盖每一张的基本知识点，方便在短时间内快速回顾。</p>
<p>这个笔记的顺序是从后向前，这是我当时的复习顺序，这里就不重新树立了。因为时间关系，我也没有能完成对全部笔记的精简，对于没有的部分，也不打算补充了。</p>
<p>这个学期结束，是时候放松一下，读一些书，投一些简历，然后为下一学期做准备了。</p>
<h2>Coordinating, communicating and coaching</h2>
<h3>Decentralized POMDP</h3>
<ul>
<li>Shared Reward</li>
<li>multiple independent agent</li>
<li>Optimal solution balance the benefit communicating and cost of not communicating</li>
<li>example: finding each other in a museum</li>
</ul>
<h3>Inverse RL</h3>
<ul>
<li>Agents generate reward function after experiencing the world and a set of behaviors</li>
<li>MLIRL: Maximum likelihood IRL (dynamic reward shaping??)</li>
</ul>
<h3>Policy shaping:</h3>
<ul>
<li>Human commentary on Agent's action.</li>
<li>Human might not be always correct.</li>
<li>The knowledge about the world comes from multiple sources (e.g. human and agent)</li>
<li>combining info by calculating the probability that all the sources agree (multiplying the values)</li>
</ul>
<h3>Ways that machine can learn from human</h3>
<ul>
<li>Demonstrations: IRL</li>
<li>Reward shaping:</li>
<li>Policy shaping:</li>
<li>An example Drama Management: treating story as MDP<ul>
<li>Target Trajectory Distribution-MDPs</li>
<li>Trajectories are states, actions are story actions; Model is the probability of given action (a) and trajectory (t) and ending at new trajectory (t'), P(t'|a,t).</li>
<li>optimal policy is the policy that will lead to the TTD</li>
</ul>
</li>
</ul>
<hr/>
<h2>Game Theory I</h2>
<ul>
<li>Game theory is mathematics of conflict of interests, and it generalizes the RL to multiple agents.</li>
<li>Pure strategy V.S. Mixed strategy.</li>
<li>MinMax and Maximin: all agents tries to maximize their own reward. minMax have one same result.<ul>
<li>In 2-player, zero-sum deterministic game of perfect information: minMax ≡ MaxiMin and there is always a optimal pure strategy;</li>
<li>In 2-player, zero-sum <strong>non</strong>-deterministic game of perfect information: minMax ≡ MaxiMin and there is always a optimal pure strategy (the same as above);</li>
<li>In 2-player, zero-sum non-deterministic game of <strong>hidden</strong> information (mini-pocket game): minmax ≠ Maximin and there will be no optimal pure strategy; e.g.  there will be mixed strategy (intersect of strategy lines).</li>
<li>In 2-player, <strong>non</strong>-zero-sum non-deterministic game of hidden information (Prisons' Dilemma): both prisoners will always defect ( in a Nash equilibrium)</li>
</ul>
</li>
<li>Nash Equilibrium<ul>
<li>in the n-player pure strategy game, if elimination of strictly dominated strategies eliminates all but one combination, that combination is the unique NE.</li>
<li>Any N.E. will survive elimination of strictly dominated strategies</li>
<li>if n is finite, for each set of finite strategies, then there will be at least one strategy is N.E.</li>
<li>if agents are in a Nash Equilibrium, any agent will have no good reason to change strategy ( pure or mixed)</li>
</ul>
</li>
</ul>
<h2>Game Theory II</h2>
<ul>
<li>Iterated Prisoner's Dilemma (IPD),<ul>
<li>when the last a few games are known, Prisoners will always defect.</li>
<li>when the last a few games are unknown (with probability of γ that the same will continue), it opens door for cooperation.	In this case, using TfT, IPD will reach new NE, which is always being cooperative.</li>
</ul>
</li>
<li>Tit For Tat: the first round Cooperate, then copy opponent's previous move.</li>
<li>Folk Theorem describes the set of payoffs that can result from Nash strategies in repeated games.<ul>
<li>Feasible payoff are average payoff of some joint strategies.</li>
<li>acceptable payoff are average payoff of joint strategies better than minMax profile. MinMax profile or security level profile are pair of payoffs that represent the payoffs that can be achieved by a player defending itself from a malicious adversary.</li>
<li>Folk Theorem: any feasible payoff profile that strictly dominates the minimax profile can be realized as a Nash equilibrium payoff profile, with sufficiently large discount factor.</li>
<li>Proof: the minimax profile can be used as threat.</li>
<li>but threat is not always plausible when it is not subgame perfect.</li>
<li>Subgame perfect: it is stable if taking test response independently</li>
<li>Pavlov V.S Pavlov is NE and subgame perfect. TfT V.S. TfT is NE but not subgame perfect</li>
</ul>
</li>
<li>Computational Folk Theorem: can build Pavlov -like machines for any game and construct sublime perfect NE for any game in polynomial time.</li>
</ul>
<h3>Is Stochastic games solvable (solved)?</h3>
<ul>
<li>Stochastic Games is generalization of both MDP and single agent RL. MDP:RL :: Stochastic game:multi agent RL</li>
<li>Restrict some parameters of SG, will get MDP (make only one agent matter), Zero sum stochastic game (set rewards to be adding up to a constant) and repeated games (|S|=1).</li>
<li>Zero sum stochastic games can by solved by using Minimax in a Bellman-like equation. VI works for the situation, minimax-Q converges, and Q* is unique, policies can be computed.</li>
<li>General-Sum Games are not solved currently but we have hope<ul>
<li>repeated stochastic games</li>
<li>cheap talk --&gt; correlated equilibria</li>
<li>cognitive hierarchy -&gt; best responses</li>
<li>Side payments (CoCo values)</li>
</ul>
</li>
</ul>
<h2>Game Theory III</h2>
<ul>
<li>Correlated equilibrium and the chicken game:<ul>
<li>Chicken game has 2 NE, CD or DC. But the 2 NE is not stable. The mixed strategy gives an expected payoff of 14/3</li>
<li>introducing a middle man who sees the cards and tell agents what to do. the agents will get a new NE which is aways listening to the middle man. This new NE is called correlated equilibrium。 CE will get payoff of 15/3 which is better than 14/3.</li>
<li>CE can be found in poly time；all mixed Nash are correlated；all convex combination of mixed Nash are correlated</li>
</ul>
</li>
<li>Cooperative-Competitive Values<ul>
<li>coco(U,Ubar) =MaxMax((U +Ubar)/2) + MinMax((U-Ubar)/2); P =coco(u,Ubar)-U(a*,a*bar)</li>
<li>coco decompose game into some of two games. it has unique solution, can be extended to stochastic games, but not necessarily equilibrium and does not generate to n &gt; 2.</li>
</ul>
</li>
<li>Mechanism Design: based on wanted behavior to build/make a game.<ul>
<li>Peer teaching: student +1 when get one question correct. teacher +1 when student makes mistake on easy question and get correct on hard question.</li>
<li>Baby devision: asks A B, if both yes, A pays a small fine but B announces a V, ask again, if A say yes, A keep baby and pay V, and if A says no, B keeps baby and pay V. Assumption, baby worth more to real mom and less to fake mom.</li>
</ul>
</li>
</ul>
<hr/>
<h2>Options</h2>
<h3>Temporal Abstraction and options</h3>
<ul>
<li>Temporal Abstraction is representing many actions with one or a few options<ul>
<li>Option = &lt;I, π, β&gt;. I is __I__nitiation set of states, β is termination set of states and probability of ending at state s. π is policy</li>
<li>Bellman-like equations can be written using o to replace a and F(s,o,s') to replace T(s,a,s') and discount factor is hidden.</li>
<li>using option changes MPD to Semi-MDP</li>
<li>example: the Pac-Man problem</li>
<li>Options have computational benefits. If done right, it can also decrease state space.</li>
</ul>
</li>
</ul>
<h3>Goal Abstraction and Modular RL</h3>
<ul>
<li>Options give ways to think actions as accomplishing parallel goals.</li>
<li>Modular RL sees options as sub-agents with different goals.</li>
<li>But only one goal can be active at a given time. To choose a goal, one can choose<ul>
<li>greatest mass Q-learning(summing up all the Q of each action and excuse the option with largest Q),</li>
<li>Top Q-learning (choose the action with highest Q, doesn't work for multiple action with the highest Q)</li>
<li>negotiated W-learning: Minimizing loss.</li>
</ul>
</li>
</ul>
<h3>Monte Carlo Tree Search</h3>
<ul>
<li>Procedure: Select, expand, Simulate, back up and repeat.<ul>
<li>Select actions based on policy π, for all known states</li>
<li>when reach unknown states, simulate actions and sample. Based on the simulation, calculate value of unknown state and backup and expend π. and then repeat.</li>
</ul>
</li>
<li>Trade-off is between number of state and how deep to search.</li>
<li>the MCTS can be seen as policy search. When reach a state we are not confident, a inner loop is executed to do some RL.</li>
</ul>
<hr/>
<h2>Partially Observable MDPs</h2>
<ul>
<li>in POMDP, MDP is not directly observable, agent can only observe states. Z is observables, O is the probability of agent in S when sees Z.</li>
<li>We can estimate states the agent are in using believe states which turns MDP in to belief MDP &lt;b, a, z, b'&gt;.</li>
</ul>
<h3>VI in POMDP</h3>
<ul>
<li>Vt(b) can be represented by the maximum of a set of linear functions of b and α. α is a finite set of vectors. this is called piecewise linear and convex.</li>
<li>Some linear functions are purgeable if it does not contribute to the max</li>
<li>two types of RL: model based and model free. but both need to interact with the MDP.</li>
<li>Expectation Maximization is a procedure to learn the hidden property in POMDP.</li>
</ul>
<h3>Bayesian RL</h3>
<ul>
<li>take action to figure out which MDP we are in.</li>
<li>RL requires exploring and exploiting, which is like MDP.</li>
<li>Piecewise polynomial and convex</li>
</ul>
<h3>Predictive State representation</h3>
<ul>
<li>We can do test to get predictions of states</li>
<li>But belief states and PSR does not have a one-on-one mapping</li>
<li>Any n-state POMDP can be represented by a PSR with no more than n tests, each  of which is no longer than n steps.</li>
<li>Using PSR to do some test can help us figure out what state we might be in and learning PSR sometimes is easier than learning a POMDP. and we need no more than n tests.</li>
</ul>
<hr/>
<h2>Generalization</h2>
<ul>
<li>some problem have Zillions of states, Learning all of them are not practical.</li>
<li>Solution:  use features to represent states</li>
<li>Method: applying supervised learning method</li>
<li>Generalization can happen on three levels<ul>
<li>value function (which maps state/actions to expected return)</li>
<li>Policy (which maps state-action)</li>
<li>Model (which maps transition and reward)</li>
</ul>
</li>
<li>value function approximation<ul>
<li>Q value for an action is represented by features and a set of weight</li>
</ul>
</li>
<li>success stories: averager, TD gammon, Atari</li>
<li>failed stories: Baird's counter</li>
<li>Linear approximation need not work</li>
</ul>
<pre><code>2015-12-06 初稿
2015-12-08 发布
</code></pre>
