<h2>原文：<a href="https://conge.livingwithfcs.org/2016/01/27/Machine-Learning-bi-ji---di-02-zhou--">Machine Learning笔记 第02周 </a></h2>
<hr/>
<h2>layout: post
title: "Machine Learning笔记 第02周 "
date: "2016-01-27 04:02:41"
categories: 计算机科学
excerpt: "本学期学习Machine Learning。本课程在Udacity上有免费版本，并提供详细的笔记。本文的笔记是超级脱水版，目的是自用。 Week..."
auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<blockquote>
<p>本学期学习Machine Learning。本课程在Udacity上有<a href="https://www.udacity.com/course/machine-learning--ud262">免费版本</a>，并提供详细的笔记。本文的笔记是超级脱水版，目的是自用。</p>
</blockquote>
<p>Week 02 tasks：
* Lectures:  Instance Based Learning, Ensemble B&amp;B, and Kernel Methods &amp; SVMS.</p>
<ul>
<li>Readings: Mitchel Chapter 8, Schapire's overview to Machine learning and Jiri Matas' slides on AdaBoost.</li>
</ul>
<h2>SL4: Instance Based Learning</h2>
<blockquote>
<ul>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/666010252/Lesson%204%20Notes.pdf">Lesson 4 Notes</a></li>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/666010252/Instance%20Based%20Learning.pdf">Instance Based Learning</a></li>
</ul>
</blockquote>
<p><img alt="Instance Based Learning Before" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a731793f5383c519.png"/></p>
<ul>
<li>in supervised learning, we learn a function from data and use the function to predict future data.</li>
</ul>
<p><img alt="Instance Based Learning Before" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-b3d5d0d163b6c997.png"/></p>
<ul>
<li>Now we keep everything in a database and f(x)=lookup(x). this database approach has pros (fast and simple) and cons (no generalizable and overfitting).</li>
</ul>
<p><img alt=" Cost of the House example" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-c8cc81158090dfbe.png"/></p>
<ul>
<li>classify unknow price houses by<ul>
<li>nearest neighbor</li>
<li>k nearest neighbors</li>
</ul>
</li>
</ul>
<p><img alt="K-NN" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-3b1126963f9b3dfa.png"/></p>
<ul>
<li>in K-NN, there are a lot decisions can be made to get the result: e.g.  classification: by voting or weighted voting; regression: average or weighted average.</li>
</ul>
<p><img alt="Quiz 1: compute my neighbors" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-ee3c6f6036852ea5.png"/></p>
<ul>
<li>the running time and space required for each algorithm.</li>
<li>Sorted data points is more efficient in query.</li>
<li>Lazy learning (push work to when it's needed) V.S. eager learning</li>
</ul>
<p><img alt="quiz 2: Domain K-nnowledge" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a1c2bc983929864d.png"/></p>
<ul>
<li>Different distance function and K makes very different prediction of the query point q.</li>
</ul>
<p><img alt="K-NN Bias" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-6cbf6c759643d79e.png"/></p>
<ul>
<li>K-NN works well when data has 1) locality, 2) smoothness and 3) equality of features</li>
</ul>
<p><img alt=" Curse of Dimensionality" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-d7d989991708fcd6.png"/></p>
<ul>
<li>This is bad</li>
<li>the curse is not just for KNN, it's for ML.</li>
</ul>
<p><img alt="Some Other Stuff" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a85eb0986cd51494.png"/></p>
<ul>
<li>Picking d(), picking K, replacing average with local weighted regression.</li>
</ul>
<p><img alt="Wrap up" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-cd89a2931b23cb02.png"/></p>
<hr/>
<h2>SL5: Ensemble Learning Boosting</h2>
<blockquote>
<ul>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/367378584/Lesson%205%20Notes.pdf">Lesson 05 Notes</a></li>
<li><a href="https://storage.googleapis.com/supplemental_media/udacityu/367378584/Intro%20to%20Boosting.pdf">Intro to Boosting</a></li>
</ul>
</blockquote>
<p><img alt=" Ensemble Learning Boosting" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-c21363eddbae9c4e.png"/></p>
<ul>
<li>example of spam email rules: but the rules are not definitive.</li>
</ul>
<p><img alt="Ensemble learning algrithm" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-1ff2c1ba5fe204c4.png"/></p>
<p>algorithm:</p>
<ol>
<li>pick up rules from subset of training data and</li>
<li>combine rules.</li>
</ol>
<p><img alt="Ensemble learning algrimth continue" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-87b6f7da041b8a3c.png"/></p>
<p>For each step of ensemble learning algorithm, there are multiple ways of doing things.</p>
<ul>
<li>picking up dataset uniformly randomly and</li>
<li>combine by average give us bagging. See examples below.</li>
</ul>
<p><img alt="Quize 1. Ensemble Learning Outputs" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-6cbb2481a39cffd0.png"/></p>
<p>Example 1:</p>
<ul>
<li>N points and learner is zeor order polynormial</li>
<li>combining method: mean</li>
<li>Subsets are N disjoint, each w/ one data point.</li>
</ul>
<p>What's the ensemble output? A: average of N data points.</p>
<p>Now, another example:</p>
<p><img alt="Ensemble learning example" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-5037209e4ece089b.png"/></p>
<ul>
<li>3rd order polynomial (redline) on subset is better than 4th order polynomial on whole data on testing. It somehow removes overfitting.</li>
<li>Bagging or bootstrapping:  take a random subset and combine by the mean</li>
</ul>
<p><img alt="Ensemble Boosting" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-d8c65c78a34ca7d7.png"/></p>
<p>New subsetting and combination method:</p>
<ul>
<li>Choose hard examples: hard relative to if we were to stop now, how well we do?<ul>
<li>hard samples are samples which hypotheis do not match with the target function</li>
</ul>
</li>
<li>Use weighted average</li>
</ul>
<p><img alt="Quiz 2: Error" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-b185ac9be2894ca3.png"/></p>
<ul>
<li>subscript D stands for distribution. So h is the hypothesis and C is the true underlying concept.</li>
<li>Olde definition: an error rate or an error percentage as the total number of mismatches over the total number of examples</li>
<li>New Definition: error is the probability, given the underlying distribution, that I will disagree with the true concept on some particular instance X.</li>
<li>Quiz 2: say we have 4 instance, we got 2 correct and 2 wrong, what's the error? A: 0.5, assuming each instance has the same possibility to be seen.</li>
</ul>
<p><img alt="Quiz 3: Error when instances have different possibility" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-e1d65608db26dd46.png"/></p>
<ul>
<li>What's the error rate using new definition? A: 0.1.</li>
</ul>
<p><img alt="Weak Learner" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-1ef38c50638b7a1b.png"/></p>
<ul>
<li>a weak learner is a learner that, no matter what the distribution is over your data, will do <strong>better than chance</strong> when it tries to learn labels on that data.</li>
</ul>
<p><img alt="Quiz4: Weak Learning" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-496d1d83cb982d43.png"/></p>
<ul>
<li><p><strong>Good D</strong>: find distribution over the 4 different examples, such that a learning algorithm that has to choose between one of those hypotheses will in fact be able to find one that does better than chance (by having an expected error less than 1/2)</p>
</li>
<li><p><strong>evil D</strong>: find a distribution such that if you have that distribution over the four examples, a learning algorithm that only looked at H1, H2 and H3 would not be able to return one of them that has an expected error less than 1/2.</p>
</li>
</ul>
<p><img alt="Quiz4: answer, and my answer above also passed the grader" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-ad077ecf7a98e79a.png"/></p>
<p><img alt="Boosting in code" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-d089133eaab68e9c.png"/></p>
<ul>
<li>a training set made up of a bunch of x&lt;sub&gt;i&lt;/sub&gt;, y&lt;sub&gt;i&lt;/sub&gt; pairs.</li>
<li>loop at every time step <em>t</em>, from step 1, to some <em>T</em><ul>
<li>construct a distribution D&lt;sub&gt;t&lt;/sub&gt;</li>
<li>Given that distribution, find a weak classifier to output hypothesis H&lt;sub&gt;t&lt;/sub&gt; with some small error  ϵ&lt;sub&gt;t&lt;/sub&gt;, and ϵ&lt;sub&gt;t&lt;/sub&gt; can't be bigger than 1/2.</li>
</ul>
</li>
<li>eventually,  output some final hypothesis. H&lt;sub&gt;final&lt;/sub&gt;.</li>
<li>the two really important things are left out:<ol>
<li>where do we get this distribution and</li>
<li>where do we get this final hypothesis?</li>
</ol>
</li>
</ul>
<p><img alt="Quiz, what will happen when D agrees" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-67d74bc522b21d79.png"/></p>
<ul>
<li>Important part 1: how to construct distributions<ul>
<li>initial D(i) = 1/n. No special weights</li>
<li>for D&lt;sub&gt;t&lt;/sub&gt;(i), use normalized (by Z&lt;sub&gt;t&lt;/sub&gt;) distribution which is weighted by e to the alpha, Y, and h&lt;sub&gt;t&lt;/sub&gt; .</li>
</ul>
</li>
<li>When y and h&lt;sub&gt;t&lt;/sub&gt; disagree, the distribution will change. but whether the change is increase or decrease will depends on normalization factor and others.</li>
</ul>
<p><img alt="Final Hypothesis" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-35e56c74acf96384.png"/></p>
<ul>
<li>H&lt;sub&gt;final&lt;/sub&gt; is defined  as a SGN function of the sum of all the weighted hypothesis.</li>
</ul>
<p><img alt="Boosting example" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-1f57b629091134e1.png"/></p>
<ul>
<li>in three steps, boosting will solve the problem of separating red positive sign and green negative sign.</li>
<li>Size of sign represents their weights.</li>
</ul>
<p><img alt="Quiz 6 answer" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-fa921961e0f28097.png"/></p>
<ul>
<li>by average the three hypothesis, we can get the correct Hypothesis here.</li>
</ul>
<p><img alt="Intuition of why boosting will converge to a good answer?" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-9446c2cdbd51da5c.png"/></p>
<p>I am lost here, and need to read the proof!</p>
<p><img alt="Recap" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-446cec807a6b6400.png"/></p>
<p>In practice, Boosting might not have a overfitting problem (testing error goes high)</p>
<pre><code>2016-01-24 SL4 完成
2016-21-25 SL5，初稿完成
</code></pre>
