<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2015/2015-08-28-Reinforcement-Learning--di-er-zhou-ke-cheng-bi-ji-.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2015/08/28/Reinforcement-Learning--di-er-zhou-ke-cheng-bi-ji-">站点原文</a></h2>
<hr/>
<h2>layout: post
title: "Reinforcement Learning 第二周课程笔记"
date: "2015-08-28 09:47:43"
categories: 计算机科学
excerpt: "Intro to Burlap In this introduction lecture to Burlap, I learned how to..."
auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<h2>Intro to Burlap</h2>
<p>In this introduction lecture to Burlap, I learned how to use BURLAP to setup a MDP.</p>
<p>Steps includes:</p>
<ol>
<li>setup domain (using GraphDefinedDomain)</li>
<li>setup nodes (states)</li>
<li>setup actions</li>
<li>initialize state</li>
<li>setup reward function, termination function, hashFactory</li>
<li>run Value Iteration</li>
<li>and get the optimal first action.</li>
</ol>
<p>See the sample code below and read<a href="http://burlap.cs.brown.edu/doc/">BurLAP documentation</a> to understand the code.</p>
<p><img alt="First MDP problem" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-0e51ecbf79011127.png"/></p>
<pre><code>import burlap.behavior.singleagent.planning.stochastic.valueiteration.ValueIteration;
import burlap.behavior.statehashing.DiscreteStateHashFactory;
import burlap.domain.singleagent.graphdefined.GraphDefinedDomain;
import burlap.oomdp.auxiliary.DomainGenerator;
import burlap.oomdp.auxiliary.common.NullTermination;
import burlap.oomdp.core.*;
import burlap.oomdp.singleagent.GroundedAction;
import burlap.oomdp.singleagent.RewardFunction;

public class FirstMDP {
		
    DomainGenerator				dg;
    Domain						domain;
    State						initState;
    RewardFunction				rf;
    TerminalFunction			tf;
    DiscreteStateHashFactory	hashFactory;
    int                         numStates;

    public FirstMDP(double p1, double p2, double p3, double p4) {
        int numStates = 6;
        this.dg = new GraphDefinedDomain(numStates);
        
        //actions from initial state 0
        ((GraphDefinedDomain) this.dg).setTransition(0, 0, 1, 1.);
        ((GraphDefinedDomain) this.dg).setTransition(0, 1, 2, 1.);
        ((GraphDefinedDomain) this.dg).setTransition(0, 2, 3, 1.);
        
		//transitions from action "a" outcome state
		((GraphDefinedDomain) this.dg).setTransition(1, 0, 1, 1.);

		//transitions from action "b" outcome state
		((GraphDefinedDomain) this.dg).setTransition(2, 0, 4, 1.);
		((GraphDefinedDomain) this.dg).setTransition(4, 0, 2, 1.);

		//transitions from action "c" outcome state
		((GraphDefinedDomain) this.dg).setTransition(3, 0, 5, 1.);
		((GraphDefinedDomain) this.dg).setTransition(5, 0, 5, 1.);
        
        this.domain = this.dg.generateDomain();
        this.initState = GraphDefinedDomain.getState(this.domain,0);
		this.rf = new FourParamRF(p1,p2,p3,p4);
        this.tf = new NullTermination();
        this.hashFactory = new DiscreteStateHashFactory();
    }
    
    public static class FourParamRF implements RewardFunction {
		double p1;
		double p2;
		double p3;
		double p4;
		
		public FourParamRF(double p1, double p2, double p3, double p4) {
			this.p1 = p1;
			this.p2 = p2;
			this.p3 = p3;
			this.p4 = p4;
		}
		
		@Override
		public double reward(State s, GroundedAction a, State sprime) { 
			int sid = GraphDefinedDomain.getNodeId(s);
			double r;
			
			if( sid == 0 || sid == 3 ) { // initial state or c1
				r = 0;
			}
			else if( sid == 1 ) { // a
				r = this.p1;
			}
			else if( sid == 2 ) { // b1
				r = this.p2;
			}
			else if( sid == 4 ) { // b2
				r = this.p3;
			}
			else if( sid == 5 ) { // c2
				r = this.p4;
			}
			else {
				throw new RuntimeException("Unknown state: " + sid);
			}
			
			return r;
		}
    }
    
    private ValueIteration computeValue(double gamma) {
    	double maxDelta = 0.0001;
    	int maxIterations = 1000;
    	ValueIteration vi = new ValueIteration(this.domain, this.rf, this.tf, gamma, 
    			this.hashFactory, maxDelta, maxIterations);
    	vi.planFromState(this.initState);
    	return vi;
    }
    
	public String bestFirstAction(double gamma) {
        double V1, V2, V3;
        String best;
        best = "";
        ValueIteration vi = computeValue(gamma);
        

        V1 = vi.value( GraphDefinedDomain.getState(this.domain, 1) );
        V2 = vi.value( GraphDefinedDomain.getState(this.domain, 2) );    
        V3 = vi.value( GraphDefinedDomain.getState(this.domain, 3) );
             
        
        if (V1 &gt;= V2 &amp;&amp; V1 &gt;= V3){
            best = "action a";
        } else if (V2 &gt;= V3 &amp;&amp; V2&gt; V1){
            best = "action b";
        } else if (V3&gt; V2 &amp;&amp; V3 &gt; V1){ 
            best = "action c";
        }
        
        return best;
		// Return "action a" if a is the best action based on the discount factor given.
		// Return "action b" if b is the best action based on the discount factor given.
		// Return "action c" if c is the best action based on the discount factor given.
		// If there is a tie between actions, give preference to the earlier action in the alphabet:
		//   e.g., if action a and action c are equally good, return "action a".
	}
	
	public static void main(String[] args) {
		double p1 = 5.;
		double p2 = 6.;
		double p3 = 3.;
		double p4 = 7.;
		FirstMDP mdp = new FirstMDP(p1,p2,p3,p4);
		
		double gamma = 0.6;
		System.out.println("Best initial action: " + mdp.bestFirstAction(gamma));
	}
}

</code></pre>
<p><img alt="Second MDP Problem" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-670b14abd5d3d9cf.png"/></p>
<p>The solution of this second problem will not be posted here since it's the quiz of the lecture.</p>
<pre><code>2015-08-26 初稿
</code></pre>
