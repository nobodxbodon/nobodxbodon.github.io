<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2019/2019-03-31-ML4T-03-06-Q-Learning.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2019/03/31/ML4T-03-06-Q-Learning">站点原文</a></h2>
<hr/>
<h2>layout: post
title: "ML4T笔记 | 03-06 Q-Learning"
date: "2019-03-31 03:31:31"
categories: 计算机科学
auth: conge
tags: Machine_Learning Trading ML4T OMSCS</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<h2>01 - Overview</h2>
<p><img alt="image.png" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a02f0222d9920020.png"/></p>
<ul>
<li>Q-learning is a model-free approach. The transitions T or the rewards R are unknown.</li>
<li>Q-learning builds a table of utility values as the agent interacts with the world.</li>
<li>These Q-values can be used at each step to select the best action based on what it has learned so far.</li>
<li>Q-learning guaranteed to provide an optimal policy. There is a hitch, however, that we'll cover later.</li>
</ul>
<blockquote>
<p><em>Time: 00:00:38</em></p>
</blockquote>
<h2>02 - What is Q</h2>
<p><img alt="image.png" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-415c5ddf65ef7967.png"/></p>
<h3>What is Q?</h3>
<p>Well, let's dig in and find out.</p>
<ul>
<li>in this class we're going to view Q as a table with two dimensions, s and a.</li>
<li><code>Q[s, a]</code>Q represents the value of taking action A in state s.</li>
<li>two components: the <strong>immediate reward</strong> that you get for taking action A in state s, plus the discounted reward or the reward you get for future actions.</li>
<li>Q is <strong>not greedy</strong>: it just represents the reward you get for acting now and the reward you get for acting in the future.</li>
</ul>
<h3>How can we use Q to figure out what to do?</h3>
<p>Policy ($\Pi$) defines what we do in any particular state</p>
<ul>
<li>$\Pi(s)$ is the action we take when we are in state s.</li>
<li>$\Pi(s) = argmax_a(Q[s, a])$, So we step through each value of a, and the one that is the largest is the action we should take.</li>
</ul>
<p>After we run Q learning for long enough, we will eventually converge to the optimal policy ($\Pi^*$).</p>
<blockquote>
<p><em>Time: 00:02:53</em></p>
</blockquote>
<h2>03 - Learning Procedure</h2>
<p><img alt=" " src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-4b9355c6da090baf.png"/></p>
<p><strong>the big picture of how to train a Q-learner</strong>.</p>
<ol>
<li>select data to train on: time series of the stock market.</li>
<li>iterate over this data over time. Evaluate the situation there and for a particular stock that gives us <strong>s</strong> our state. consult our policy and get an action <em>a</em>. take that action, evaluate the next state <strong>s'</strong> and our reward <strong>r</strong>. that is &lt;s, a, r, s'&gt;, the experience tuple.</li>
<li>Once get all the way through the training data, test our policy and see how well it performs in a backtest.
4 If it's converged or it's not getting any better then we say we're done. If not, repeat this whole process all the way through the training data.</li>
</ol>
<h3>what does converge mean?</h3>
<p>As we cycle through the data training our Q table and then testing back across that same data, we get some performance. And we expect performance to get better and better.</p>
<ul>
<li>But after a point, it finally stops getting better and it converges. When more iterations don't make performance better and we call it <strong>converged</strong> at that point.</li>
</ul>
<h3>Detail on what happens here when we're iterating over the data.</h3>
<ol>
<li>start by setting our start time, and initialize our Q table.</li>
</ol>
<ul>
<li>The usual way to initialize a Q table is with small random numbers, but variations of that are fine.</li>
</ul>
<ol start="2">
<li>observe the features of our stock or stocks and from those build up together our state <code>s</code>.</li>
<li>consult our policy (Q table) to find the best action in the current state to get <code>a</code>.</li>
<li>step forward and get reward <code>r</code> and new state <code>s'</code>.</li>
<li>Update the Q table with this complete experience tuple</li>
</ol>
<p>Then repeat.</p>
<blockquote>
<p><em>Time: 00:03:22</em></p>
</blockquote>
<h2>04 - Update Rule</h2>
<p><img alt=" " src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-44f8099d3385c0f3.png"/></p>
<p>Once an experience tuple &lt;s, a, r, s'&gt; is generated by interacting with the environment, how does it take that information to improve this Q table?</p>
<p>There are two main parts to the update rule.</p>
<ol>
<li>the old value that we used to have: Q [s, a].</li>
<li>improved estimate</li>
</ol>
<p><strong>Learning Rate</strong>:
New concept: $\alpha$ is the <strong>learning rate</strong>. $\alpha \in [0,1]$, usually use about 0.2.</p>
<ul>
<li>$Q'[s,a] = ( 1 -\alpha)Q[s,a] + \alpha  *  \text{estimated improvement})$</li>
<li>This learning rate dictates the ratio the Q' takes from the old Q and the estimated improvement.</li>
<li>larger values of $\alpha$  cause us to learn more quickly, lower values of $\alpha$  cause the learning to be slower.</li>
<li>a low value of alpha, for instance, means that in this update rule, the previous value for Q[s,a] is more strongly preserved.</li>
</ul>
<p><strong>Discount rate</strong>
$\gamma$ is the <strong>discount rate</strong>. $\gamma \in [0, 1]$. A low value of $\gamma$  means that we value later rewards less.</p>
<ul>
<li>A low value of $\gamma$  equates to essentially a high discount rate.</li>
<li>The high value of $\gamma$, in other words, a $\gamma$  near 1, means that we value later rewards very significantly.</li>
<li>$Q'[s,a](1 - \alpha)Q[s,a] + \alpha( r + \gamma Q[s',argmax_{a'}(Q[s',a'])$</li>
</ul>
<p><strong>future discounted rewards</strong>.</p>
<p>what is the value of those future rewards if we reach state s' and we act appropriately?</p>
<ul>
<li>Q value, but need to find out what the action to take. which is $argmax_{a'}(Q[s',a']$</li>
</ul>
<p><strong>This is the equation you need to know to implement Q learning.</strong></p>
<blockquote>
<p><em>Time: 00:05:07</em></p>
</blockquote>
<h2>05 Update Rule</h2>
<p>The formula for computing <code>Q</code> for any state-action pair <code>&lt;s, a&gt;</code>, given an experience tuple <code>&lt;s, a, s', r&gt;</code>, is:
<code>Q'[s, a] = (1 - α) · Q[s, a] + α · (r + γ · Q[s', argmaxa'(Q[s', a'])])</code></p>
<p>Here:</p>
<ul>
<li><code>r = R[s, a]</code> is the immediate reward for taking action <code>a</code> in state <code>s</code>,</li>
<li><code>γ ∈ [0, 1]</code> (gamma) is the <em>discount factor</em> used to progressively reduce the value of future rewards,</li>
<li><code>s'</code> is the resulting next state,</li>
<li><code>argmaxa'(Q[s', a'])</code> is the action that maximizes the Q-value among all possible actions <code>a'</code>from <code>s'</code>, and,</li>
<li><code>α ∈ [0, 1]</code> (alpha) is the <em>learning rate</em> used to vary the weight given to new experiences compared with past Q-values.</li>
</ul>
<h2>06 - Two Finer Points</h2>
<p><img alt=" " src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-77e409d95ef985ef.png"/></p>
<ol>
<li>Q-learning depends to a large extent on exploration. So we need to explore as much of the state and action space as possible.</li>
</ol>
<p><strong>Randomness</strong>.</p>
<ul>
<li>where we are selecting an action, we flip a coin and randomly decide if we're going to randomly choose an action.</li>
</ul>
<p>Two flips of the coin.</p>
<ol>
<li>choose a random action or are action with the highest Q value?</li>
<li>if random choice, then flip the coin again to choose which of those actions we're going to select.</li>
</ol>
<ul>
<li>A typical way to implement this is to set this probability at about 0.3 or something at the beginning of learning * And then over each iteration to slowly make it smaller and smaller and smaller until eventually we essentially don't choose random actions at all.</li>
</ul>
<p>By doing this,</p>
<ol>
<li>we're forcing the system to explore and try different actions in different states.</li>
<li>it also causes us to arrive at different states that we might not otherwise arrive at if we didn't try those random actions.</li>
</ol>
<blockquote>
<p><em>Time: 00:01:32</em></p>
</blockquote>
<h2>07 - The Trading Problem - Actions</h2>
<p><img alt="Actions" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-36947003bc22c170.png"/></p>
<p>To turn the stock trading problem into a problem that Q learning can solve, we need to define our actions, we need to define our state, and we also need to define our rewards.</p>
<h3>Actions.</h3>
<p>Three actions, buy, sell or do nothing.</p>
<p>Usually what's going to happen most frequently is that we do nothing.</p>
<ol>
<li>So we evaluate the factors of the stock (e.g. several technical indicators) and get our state.</li>
<li>We consider that state and we do nothing for a while</li>
<li>something triggers an buy action: So we buy the stock and holding 4. then do nothing until our very intelligent Q learner says otherwise.</li>
</ol>
<p>How this sort of stepped behavior affect our portfolio value</p>
<blockquote>
<p><em>Time: 00:02:22</em></p>
</blockquote>
<h2>08 - The Trading Problem: Rewards</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-8f9d1df0617ee7d2.png"/></p>
<p>Now consider rewards for our learner: 1)Short-term rewards in terms of daily returns or 2) long-term rewards that reflect the cumulative return of a trade cycle from a buy to a sell, or for shorting from a sell to a buy.</p>
<p>Which one of these do you think will result in faster conversions?
<strong>Solution</strong>: The correct answer is daily returns.</p>
<p>Daily returns give more frequent feedback while cumulative rewards need to wait for a trading cycle to end.</p>
<blockquote>
<p><em>Time: 00:00:41</em></p>
</blockquote>
<h2>09 - The Trading Problem: State</h2>
<p><img alt=" " src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-df2a40f278719282.png"/></p>
<p>Which of the indicators are good candidates for states?</p>
<p><strong>Solution:</strong>
<strong>Adjusted close or SMA</strong> are not good factors for learning, not able to generalize over different price regimes for when the stock was low to when it was high.
<strong>but the combine adjusted close and simple moving average</strong> into a ratio that makes a good factor to use in state.</p>
<p>Bollinger Band value, P/E ratio is good.</p>
<p><strong>holding the stock or not</strong>: is important to the actions.
<strong>return since we entered the position</strong> might be useful to determine the exit points</p>
<blockquote>
<p><em>Time: 00:01:34</em></p>
</blockquote>
<h2>10 - Creating the State</h2>
<p><img alt=" " src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-23d7136d66fb7e23.png"/></p>
<p>The state is a single integer so that we can address it in our cue table.
We can 1) discretize each factor and 2) combine all of those integers together into a single number.</p>
<ul>
<li><p>For example, we have four factors and each one is a real number.
Then we discretize each one of these factors.</p>
</li>
<li><p>So we run each of these factors through their individual discretizers and we get an integer.</p>
</li>
</ul>
<p>Now we can stack them one after the other into our overall discretized state.</p>
<blockquote>
<p><em>Time: 00:01:52</em></p>
</blockquote>
<h2>12 - Discretizing</h2>
<p><img alt=" " src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-e998d9b3a64cc770.png"/></p>
<p>Discretization or discretizing: convert a real number into an integer across a limited scale.</p>
<ul>
<li>the number of steps we're going to have.</li>
<li>divide how many data elements we have all together by the number of steps.</li>
<li>Then we sort the data, and then the thresholds just end up being the locations for each one of these values.</li>
</ul>
<blockquote>
<p><em>Time: 00:01:53</em></p>
</blockquote>
<h2>13 - Q-Learning Recap</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a8a5561a97058b89.png"/></p>
<blockquote>
<p>Total Time: 00:24:12</p>
</blockquote>
<h2>Summary</h2>
<h3>Advantages</h3>
<ul>
<li>The main advantage of a model-free approach like Q-Learning over model-based techniques is that it can easily be applied to domains where all states and/or transitions are not fully defined.</li>
<li>As a result, we do not need additional data structures to store transitions <code>T(s, a, s')</code> or rewards <code>R(s, a)</code>.</li>
<li>Also, the Q-value for any state-action pair takes into account future rewards. Thus, it encodes both the best possible <em>value</em> of a state (<code>maxa Q(s, a)</code>) as well as the best <em>policy</em> in terms of the action that should be taken (<code>argmaxa Q(s, a)</code>).</li>
</ul>
<h3>Issues</h3>
<ul>
<li>The biggest challenge is that the reward (e.g. for buying a stock) often comes in the future - representing that properly requires look-ahead and careful weighting.</li>
<li>Another problem is that taking random actions (such as trades) just to learn a good strategy is not really feasible (you'll end up losing a lot of money!).</li>
<li>In the next lesson, we will discuss an algorithm that tries to address this second problem by simulating the effect of actions based on historical data.</li>
</ul>
<h2>Resources</h2>
<ul>
<li>CS7641 Machine Learning, taught by Charles Isbell and Michael Littman<ul>
<li>Watch for free on <a href="https://classroom.udacity.com/courses/ud262">Udacity</a> (mini-course 3, lessons RL 1 - 4)</li>
<li>Watch for free on <a href="https://www.youtube.com/watch?v=_ocNerSvh5Y&amp;list=PLAwxTw4SYaPnidDwo9e2c7ixIsu_pdSNp">YouTube</a></li>
<li>Or take the course as part of the <a href="http://www.omscs.gatech.edu/cs-7641-machine-learning/">OMSCS program</a>!</li>
</ul>
</li>
<li><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">RL course by David Silver</a> (videos, slides)</li>
<li><a href="http://mnemstudio.org/path-finding-q-learning-tutorial.htm">A Painless Q-Learning Tutorial</a></li>
</ul>
<pre><code>2019-03-31 初稿
</code></pre>
