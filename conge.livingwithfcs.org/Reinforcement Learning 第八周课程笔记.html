<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2015/2015-10-13-Reinforcement-Learning--di-ba-zhou-ke-cheng-bi-ji-.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2015/10/13/Reinforcement-Learning--di-ba-zhou-ke-cheng-bi-ji-">站点原文</a></h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p>This week</p>
<ul>
<li>Watch <em>Exploration</em>. </li>
<li>The readings were <em>Fong (1995) </em>and <em>Li, Littman, Walsh (2008).</em></li>
</ul>
<p><img alt="Exploration: Specific to RL " src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-cc685f5f28f0c812.png"/></p>
<p><img alt="Sub topics of Exploration" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-2a07bcc776fb8522.png"/></p>
<table>
<thead><tr>
<th>Type</th>
<th>state transition</th>
<th>Stochastic</th>
<th>solution</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bandits</td>
<td>✘ </td>
<td>✔ </td>
<td>hoeffding bound to do stochastic decision making</td>
</tr>
<tr>
<td>Deterministic MDPs:</td>
<td>✔ </td>
<td>✘ </td>
<td>Rmax to do sequential decision making</td>
</tr>
<tr>
<td>Stochastic MDPs</td>
<td>✔ </td>
<td>✔ </td>
<td>both hoeffding bound and Rmax</td>
</tr>
</tbody>
</table>
<h2>K-armed Bandit Problem</h2>
<ul>
<li>Each slot has a probability to get certain payoff or no payoff</li>
<li>Payoff or the associated probabilities are unknown.</li>
<li>what's the best thing to do? <strong>Exploration</strong>.</li>
</ul>
<h2>Confidence based exploration</h2>
<p>!Quiz 1: K-Armed Bandits, given the observed data, which arm gives the ightest expected payoff and which arm are most comfident](/assets/images/计算机科学/118382-6578df35f8c00073.png)</p>
<ul>
<li>arm d gives 1 pay off in the smallest number of try</li>
<li>arm g gives most reliable output since it's been pulled the most and gives a good payoff.</li>
</ul>
<p><img alt="Confidence-based Exploration" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-6d78c0df33788cb4.png"/></p>
<ul>
<li>Maximum Likelihood: ① Given current info, figure out the best expected action based on best likelihood -&gt; ② act, and get new info -&gt; repeat ① ② will always choose b </li>
<li>Maximum Confidence: ① Given current info, figure out the best expected action based on best confidence -&gt; ② act, and get new info -&gt; repeat ① ② will also choose b only</li>
<li>Minimum Confidence: ① Given current info, figure out the best expected action based on smallest confidence -&gt; ② act, and get new info -&gt; repeat ① ② will also choose to alternating between a and b. But it will not favor the better arm.</li>
<li><strong>Exploration-exploitation dilemma</strong>: combine maximum maximum likelihood and minimum confidence. The latter allows to try different actions and the former allows us to choose the best reward.</li>
</ul>
<h2>Metrics for Bandits</h2>
<p><img alt="Metrics for Bandits" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-0803c0a9cf9a2a6c.png"/></p>
<ol>
<li>identify optimal arm in the limit: exploration</li>
<li>maximize (discounted) expected rewards: exploration ＋ exploitation, but computationaly expensive<ul>
<li><a href="https://en.wikipedia.org/wiki/Gittins_index">Gittins index</a>: a real scalar value associated to the state of a stochastic process with a reward function and a probability of termination.</li>
<li>Gittins index works well with Bandits, should not generalize to other RL.</li>
</ul>
</li>
<li>Maximize expected reward in finite horizon - similar to value iteration</li>
<li>Identify near-optimal arms (ε) with high probabilities ( 1- δ) in time τ(k, ε, δ) (polynomial in k, 1/ε, 1/δ) - The <strong>"get there" goal</strong> is to find near optimal arms. similar to <a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning">PAC learning</a>: probably approximately correct.</li>
<li>Nearly Maximize reward (ε) with high probabilities ( 1- δ) in time τ'(k, ε, δ) (polynomial in k, 1/ε, 1/δ): the <strong>"how to get there" goal</strong> is to accumulate as much more rewards as possible in finite time τ'.</li>
<li>Pull a non-near optimal arm (ε) no more than τ''(k, ε, δ) times with high probabilities ( 1- δ): The <strong>"control error" goal</strong> is to minimize mistakes.</li>
</ol>
<h2>Find Best Implies Few Mistakes</h2>
<p><img alt="Find Best Implies Few Mistakes" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-5aa7c24743690dc5.png"/></p>
<ul>
<li>τ' is the total number of mistakes when we find ε' close arm </li>
<li>τ' is bounded to τ.</li>
</ul>
<h2>Few Mistakes Implies Do Well</h2>
<p><img alt="Few Mistakes Implies Do Well" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-4ae23f39551df6b2.png"/></p>
<ul>
<li>is it true that the smaller the <em>m</em> is, the smaller the <em>n</em> is? not always.</li>
<li>but it is true that the smaller the <em>m</em> is, the small the <em>n</em> could be.</li>
<li>"Do well" means algorithm C can lead to ε' close to optimal arm where ε' &lt; ε.</li>
</ul>
<h2>Do Well Implies Find Best</h2>
<p><img alt="Do Well Implies Find Best" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-35d051f87aeecd1e.png"/></p>
<p><img alt="Putting it together" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-bcfd6dfd62f191bf.png"/></p>
<ul>
<li>Given algorithm of one goal, we can derive algorithms to reach the other two.</li>
<li>So there is no need to pick one.</li>
<li>need to know that the three goals are not always solvable (so you might have to pick one?).</li>
</ul>
<h2>Hoeffding</h2>
<p><img alt="Hoeffding bounds" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-2def5eb63bf51061.png"/></p>
<ul>
<li>0 &lt;= δ &lt;=1. The more confident we are the smaller the δ -&gt; the larger the Z&lt;sub&gt;δ&lt;/sub&gt; -&gt; the larger the confidence interval</li>
<li>the larger the <em>n</em> is, the smaller the confidence interval</li>
</ul>
<h2>Combining Arm Info</h2>
<p><img alt="Combining Arm Info" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-dcb0bdc2c48298e0.png"/></p>
<ul>
<li>the length of the bar, ε, is the confidence interval of μ&lt;sub&gt;k&lt;/sub&gt;.</li>
</ul>
<h2>How Many Samples?</h2>
<p><img alt="quiz 2" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-9fc0b5d222933312.png"/></p>
<ul>
<li>C depends more on ε and less on δ (δ is in <em>ln</em>)</li>
<li>PAC-style bounds for bandits.</li>
</ul>
<h2>Exploring Deterministic MDPs</h2>
<p><img alt="MDP optimization Criteria" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-b5d196bdd7e3bad9.png"/></p>
<ul>
<li>in MDP, we can only choose action but not state</li>
<li>we can treat action selection as bandit problem (e.g. <em>k</em> action is a <em>k</em> arm bandit).</li>
</ul>
<p><img alt="Exploring MDP" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-6f43b8cc91674522.png"/></p>
<ul>
<li><strong>Rmax</strong>: assume any state we have not full explored  (unknown state-action pair) has a self-loop of reward Rmax.</li>
<li>This way, the learner is encouraged to explore the whole MDP.</li>
</ul>
<h2>Rmax Analysis</h2>
<p><img alt="Rmax Analyis" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-d08d9ab4e8bc5d29.png"/></p>
<ul>
<li>the example: even with Rmax, there is a possibility that the learning stops visiting unknown states</li>
<li>discount factor is important to determine if the agent is going to explore unknown edge.</li>
</ul>
<p><img alt="Lower bound using a special kind of MDP: sequential combination lock" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-ac865f9b41abceb0.png"/></p>
<ul>
<li>We have a upper bound saying the algorithm can solve the problem with n&lt;sup&gt;2&lt;/sup&gt;k steps</li>
<li>now we are proving that no other algorithms can take less than n&lt;sup&gt;2&lt;/sup&gt;k steps</li>
<li>The sequential combination lock has <em>n</em> states, in each state the learner can take <em>k</em> actions.</li>
<li>at <em>i</em> th state, one action can send the learner to next state (<em>i + 1</em>), all other actions will take the learner to state 0, to get back to state <em>i</em>, the learner has to take at least <em>i</em> steps (the worst case deterministic MDP).</li>
<li>In the worst case, it might take kn&lt;sup&gt;2&lt;/sup&gt; steps to solve a deterministic MDP before stopping make mistakes.</li>
</ul>
<h2>General Rmax</h2>
<p><img alt="" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-9e07aa0a27aa783e.png"/></p>
<p>For Stochastic properties, applying hoeffding bound. For sequential deterministic MDP, Rmax can solve the problem.  Now applying both, we can solve the stochastic MDP.</p>
<p><img alt="General Rmax" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-1c3713b7d5b72723.png"/></p>
<ul>
<li>For Rmax algorithm, define Unknown s, a pair as: if tried fewer than C times. Then use maximum likelihood estimate.</li>
<li>the C part is hoeffding bound.</li>
</ul>
<p>Some key ideas for this to work: Simulation Lemma and Explore-or-Exploit Lemma</p>
<h3>Simulation Lemma</h3>
<p><img alt="Simulation Lemma" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-0b75d0382f17fb58.png"/></p>
<ul>
<li>if we get an estimated MDP that is close to real MDP, then the optimizing the reward based on the estimate will be very close to the optimal policies for the real MDP.</li>
<li>α is the difference between <strong>estimated</strong> and <strong>real</strong> MDP.</li>
<li>α could be different if transition possibility and reward are not in the same scale, but we can alway rescale things to that α is between 0 and 1.</li>
</ul>
<p><img alt="the difference between estimated and real MDP (α) is polynomially related to the error in value of actions (ε)" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-0b12d8740e47f3b7.png"/></p>
<ul>
<li>α is related to ε </li>
<li>α is used to determine C</li>
<li>using <strong>Hoeffding bound</strong> and <strong>union bound</strong> to find a big enough C which can estimate an MDP which is close enough to real MDP.</li>
</ul>
<h3>Explore-or-Exploit Lemma</h3>
<p><img alt="Explore-or-Exploit " src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-2cdf5dabb90f2860.png"/></p>
<h2>What Have We Learned?</h2>
<p><img alt="Recap" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-194b003340c3e668.png"/></p>
<ul>
<li>Hoeffding bound and Union bound let us know how certain we are about the environment</li>
<li>Rmax makes sure we "visited" things enough to get accurate estimates</li>
</ul>
<pre><code>2015-10-06 初稿 up-to quiz 1
2015-10-07 updated to Exploring Deterministic MDPs
2015-12-04 reviewed and revised
</code></pre>
