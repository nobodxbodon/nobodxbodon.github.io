<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2018/2018-12-02-AI--bi-ji--Week-15-Planning-under-uncertainty.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2018/12/02/AI--bi-ji--Week-15-Planning-under-uncertainty">站点原文</a></h2>
<hr/>
<h2>layout: post
title: "AI 笔记 Week 15 Planning under uncertainty"
date: "2018-12-02 17:11:33"
categories: 计算机科学
excerpt: "Link to the videosLink to the video transcripts Introduction This lectur..."
auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<blockquote>
<p><a href="https://classroom.udacity.com/courses/ud954/lessons/6889127738/concepts/69398612260923">Link</a> to the videos
<a href="https://github.com/conge/OMSCS6601_Transcripts/blob/master/lesson10.md">Link</a> to the video transcripts</p>
</blockquote>
<h2>Introduction</h2>
<p>This lecture will focus on marrying planning and uncertainty together to drive robots in actual physical roles and find good plans for these robots to execute.</p>
<h2>Planning Under Uncertainty MDP</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-0327f22f4faeb2ac.png"/></p>
<ul>
<li>Planning + uncertainty: MDPs (Markov decision process) and POMDPs (partially observed MDP)</li>
<li>Planning +  uncertainty + learning: Reinforcement Learning</li>
</ul>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-1ed1347466b8b3e1.png"/></p>
<p>Methods categorized based on the characteristics of the world (observability and certainty).</p>
<ul>
<li>MDP deals with situations where the world is stochastic and fully observable.</li>
</ul>
<p><img alt="MDP" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a56bf08d752c38c8.png"/></p>
<ul>
<li>MDP has states, actions and state's transition matrix and a reward function. (it can attach to state, or action or transition?)</li>
<li>it becomes Markov if the outcomes of actions are somewhat random.</li>
<li>planning problem is depended on actions to each possible state. But the operational goal becomes maximizing total reward.</li>
</ul>
<h2>Robot Tour Guide Examples</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-b2cc9d1b575ea4ec.png"/></p>
<p>All these robots need to deal with uncertainties and observabilities to do their jobs (tour guide or mine explorer).</p>
<h2>MDP Grid World</h2>
<p><img alt="image.png" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-9b2dbfeef01f4fd2.png"/></p>
<p><strong>Absorbing states</strong>: search will end if the agent is at the absorbing states.
<strong>Policy</strong> assign action based on the state the agent is in.</p>
<h2>Problems With Conventional Planning 1</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-5e87dc252488f50a.png"/></p>
<h2>Policy Question</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-f8741ae743a580ef.png"/>
Question: what is the best action to take when an agent is in states a1, c1, c4 and b3?</p>
<h2>MDP And Costs</h2>
<p>The reason that the agent should be avoiding the b4 state is the cost.
<img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-3597f117909d764e.png"/></p>
<ul>
<li>The goal is to find the best path wich cost less with finite steps.</li>
<li>The discount fact can bound the max result.</li>
</ul>
<h2>Value Iteration</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a75fc02831914964.png"/></p>
<ul>
<li>value function can determine the policy based on the total reword.</li>
</ul>
<p><img alt="Intuition of VI functions" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-16b011014945ccdd.png"/></p>
<ul>
<li>The value function is a potential function that leads from the goal location all the way into the space so that hill climbing in this potential function leads you on the shortest path to the goal.</li>
<li>The algorithm is a recursive algorithm and it converges, and you have a grayscale value that really corresponds to the best way of getting to the goal.
<img alt="Bellman equation" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-31673e55db2b25ec.png"/></li>
</ul>
<h2>Quiz</h2>
<h3>Determistic question:</h3>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-add4356c9bc81b7a.png"/>
I Quiz: calculate Value when an agent was at each state.</p>
<h3>Stochastic Question :</h3>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-8701cbb732608bd8.png"/>
Calculation is complicated here because the reward of each action must be evaluated.</p>
<h2>Value Iterations And Policy</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-b51744dcfed36099.png"/></p>
<p>The policy can be defined by the value function after the value of each cell is calculated. The action policy is to choose the action which leads to the highest path reward.</p>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-ef64d7e3cd16bbe0.png"/></p>
<p>If the cost of each state is positive, the policy will encourage action to stay in the current state.
<img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-0c441011b4ec2cde.png"/></p>
<p>If the cost is too low, the value of each state might become so low that the agent will try to end the search as soon as possible without looking for an optimal solution</p>
<h2>MDP Conclusion</h2>
<p><img alt="Conclusion" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-e6e0843725292462.png"/></p>
<hr/>
<h2>POMDP Vs MDP</h2>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-1400007f7aa70143.png"/></p>
<ul>
<li>MDP is for fully observable environments and planning in stochastic environments.</li>
<li>Partially observable environments, or POMDPs address problems of optimal exploration versus exploitation, where some of the actions might be <strong>information-gathering actions</strong>; whereas others might be goal-driven actions.</li>
</ul>
<h2>POMDP</h2>
<p><img alt="conventional planning" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-6f60d72514190217.png"/></p>
<ul>
<li>When the world observable and the action is deterministic, Conventional planning will work and plan the best path to the goal state.</li>
</ul>
<p><img alt="MDP" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-e9e8911e2818cf76.png"/></p>
<ul>
<li>When the world observable and the action is stochastic, MDP will work and find the best path to the goal state.</li>
</ul>
<p><img alt="POMDP" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-2f904876d2da5a09.png"/></p>
<ul>
<li>When the world partially observable (e.g. we don't know where the absorbing states are) and the action is stochastic, POMDP will need to gather information and then work to find the best path to the goal state.</li>
</ul>
<p><img alt="POMDP wouldn't work" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-bcac7d03d18a3c1b.png"/></p>
<p>POMDP would not work when there are two worlds
So here's a solution that doesn't work: Obviously, the agent might be in 2 different worlds and it does not know. Solving the problem for both of these cases and then put these solutions together will not work because the average result will never let it go south to gather information.</p>
<p><img alt="POMDP will work" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-71b9f9f58b6c1b3e.png"/></p>
<p>POMDP on belief states will work. If the agent goes south and reaches the sign. 50% chance it will go the right-side belief state. if MDP was performed, then it will reach the +100 state. the same will happen if it goes to left-side belief state (50% chance).</p>
<blockquote>
<h2>Readings on Planning under Uncertainty</h2>
<p>AIMA: Chapter 17</p>
<h3>Further Study</h3>
<p>Charles Isbell and Michael Littmann’s ML course:</p>
<ul>
<li><a href="https://classroom.udacity.com/courses/ud262/lessons/684808907/concepts/last-viewed">Markov Decision Processes</a></li>
<li><a href="https://classroom.udacity.com/courses/ud262/lessons/643978935/concepts/last-viewed">Reinforcement Learning</a></li>
</ul>
<p>Peter Norvig and Sebastian Thrun’s AI course:</p>
<ul>
<li><a href="https://classroom.udacity.com/courses/cs271/lessons/48724471/concepts/last-viewed">Reinforcement Learning</a></li>
</ul>
</blockquote>
<pre><code>2018-12-01 First draft
</code></pre>
