<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2015/2015-10-31-Reinforcement-Learning--di-shi-zhou-ke-cheng-bi-ji-.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2015/10/31/Reinforcement-Learning--di-shi-zhou-ke-cheng-bi-ji-">站点原文</a></h2>
<hr/>
<p>layout: post
title: "Reinforcement Learning 第十周课程笔记"
date: "2015-10-31 04:57:13"
categories: 计算机科学
excerpt: "This week watch <em>POMDPs. </em> The reading is Littman (2009). POMDP POMDPs g..."</p>
<h2>auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p>This week</p>
<ul>
<li>watch <em>POMDPs. </em></li>
<li>The reading is <em>Littman (2009).</em> </li>
</ul>
<p><img alt="Partially Observable MDPs" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-d0c762ee64a9cb17.png"/></p>
<h2>POMDP</h2>
<p><img alt="POMDP definition" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-02856ea580f2ccbb.png"/></p>
<ul>
<li>POMDPs generalizes MDPs.</li>
<li>In POMDP, MDP (represented by &lt;S,A,T,R&gt;) is not directly observable to the agent. we can only observe the states. Z are observables to the agent, Z is a set of states in S. </li>
<li>O(S,Z) is the mapping function of Z to &lt;S,A&gt; (e.g. Given Z, what's the probability of S).</li>
</ul>
<p><img alt="quiz 1" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-0ea8aefefba47abc.png"/></p>
<ul>
<li>When O(S,Z) = 1, the observable Z is S. So, Z and O together determines the relationship between S and Z in the POMDP.</li>
</ul>
<p><img alt="quiz 2: POMDP Example " src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-671cb3a41b80aacb.png"/></p>
<p>Solution:</p>
<p><img alt="Solution" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-2a31e9646dc5c34a.png"/></p>
<ul>
<li>keep track of probability of ending up in each state after each step.</li>
<li>There is a normalization after each step (not the right thing to do but works.).</li>
</ul>
<h2>State Estimation</h2>
<p><img alt="State Estimation" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-dd48f36e60324c86.png"/></p>
<ul>
<li>Belief state b(s) is a probability distribution vector of whether we are in s.</li>
<li><p>using belief state can turn POMDP into belief MDP &lt;b, a, z, b'&gt;</p>
<blockquote><p>b'(s') = Pr(s'|b, a, z) 
= Pr(z|b,a,s') Pr(s'|b,a) / Pr(z|b,a) <br/>
= Pr(z|b,a,s') sum Pr(s'|s,b,a) Pr(s|b,a) / Pr(z|b,a) <br/>
= O(s',z) sum&lt;sub&gt;s&lt;/sub&gt; T(s,a,s') b(s) / Pr(z|b,a) </p>
</blockquote>
</li>
<li><p>Note: belief MDP has infinite number of belief states which make VI, LP, PI impossible because they can only deal with finite number of states.</p>
</li>
</ul>
<h2> Value Iteration in POMDPs </h2>
<h3>Math</h3>
<p><img alt="Value Iteration in POMDPs" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-cc7502ad8b6219d2.png"/></p>
<ul>
<li>Initialized V&lt;sub&gt;0&lt;/sub&gt;(b) as 0 </li>
<li>V&lt;sub&gt;t&lt;/sub&gt;(b) can be written as something similar to bellman equation</li>
<li>Claim  V&lt;sub&gt;t&lt;/sub&gt;(b)  can be represented by the maximum of a set of linear functions of b and α. α is a finite set of vectors. this is called piecewise linear and convex. </li>
</ul>
<p><img alt="quiz 3: piecewise linear and convex. " src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-ea0ffd6d6d73b636.png"/></p>
<ul>
<li>Γ&lt;sub&gt;0&lt;/sub&gt; should be a vector of zeroes.</li>
</ul>
<p><img alt="piecewise linear and convex" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-7377a46c63733df3.png"/></p>
<ul>
<li>Piecewise linear means V&lt;sub&gt;t&lt;/sub&gt;(b) can be represented by V&lt;sup&gt;a&lt;/sup&gt;&lt;sub&gt;t&lt;/sub&gt;(b),  V&lt;sup&gt;a&lt;/sup&gt;&lt;sub&gt;t&lt;/sub&gt;(b) can be represented by V&lt;sup&gt;a,z&lt;/sup&gt;&lt;sub&gt;t&lt;/sub&gt;(b) and V&lt;sup&gt;a,z&lt;/sup&gt;&lt;sub&gt;t&lt;/sub&gt;(b) can be represented by something like a Bellman equation</li>
</ul>
<p><img alt="piecewise linear and convex" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-4517de226631e99d.png"/></p>
<ul>
<li>In a similar sense, Γ&lt;sub&gt;t&lt;/sub&gt; can be represented by  Γ&lt;sup&gt;a&lt;/sup&gt;&lt;sub&gt;t&lt;/sub&gt; , Γ&lt;sup&gt;a,z&lt;/sup&gt;&lt;sub&gt;t&lt;/sub&gt; and something like a Bellman equation.</li>
<li>Thus the belief MDP becomes computable.</li>
<li>pomdp-solve is an existing solver for pomdp.</li>
<li>Guaranteed to be exponential because size of Γ is exponential in respect to the size of observables: | Γ&lt;sub&gt;t&lt;/sub&gt;| = | Γ&lt;sub&gt;t-1&lt;/sub&gt;|&lt;sup&gt;|z|&lt;/sup&gt; dot |A|</li>
<li>the calculation could be faster if Q function (V&lt;sup&gt;a&lt;/sup&gt;&lt;sub&gt;t&lt;/sub&gt;(b) is the Q function) has a small representation.</li>
</ul>
<h3>algorithm</h3>
<p><img alt="algorithm for vector purge" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-c22f477ddfd1de4d.png"/></p>
<p>In the figure:</p>
<ul>
<li>all the linear functions (vectors) are dominated by the blue lines which involves three linear functions.</li>
<li>function ① and ③  can be purged because they have no contribution to the overall vector</li>
</ul>
<p><img alt="Quiz 4: Domination" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-44a16fda21e55833.png"/></p>
<ul>
<li>Domination: if one vector is always less than (in this case) another vector, we say it's dominated by that another vector.</li>
<li>Vector is purgeable if it does not contribute to the max</li>
</ul>
<h3>Learning POMDP</h3>
<p><img alt="RL for POMDPs" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-0ce90c853759dcee.png"/></p>
<ul>
<li>Planning: if know the model of POMDP, can run value iteration. Since POMDP have infinite number of states, we can only get approximation of optimal policy.<ul>
<li>VI may not be converge </li>
<li>PI is not possible because infinite believe states </li>
</ul>
</li>
<li>Difference between planning and reinforcement learning:<ul>
<li>Planning: know the model - planning in POMDP is undecidable.</li>
<li>RL: we don't know the model have to interact to learn</li>
</ul>
</li>
<li>RL has 2 types:<ul>
<li>Model based RL: learn the model (POMDP) and use it</li>
<li>Model free RL: don't learn the model but mapping observables and action.</li>
</ul>
</li>
</ul>
<p><img alt="Quiz 4: relationship of POMDP and other Markov systems" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-5b98d6c15ffd811b.png"/></p>
<ul>
<li>MC: <em>Markov Chain</em>, observed but uncontrolled; HMM: <em>Hidden Markov Model</em>, partially observed and uncontrolled; MDP: observed and controlled. POMDP: partially observed and controlled. </li>
<li>EM: <em>Expectation Maximization</em>, an procedure to learn the hidden property in HMM or POMDP. </li>
<li>Model based Learning.</li>
</ul>
<h2> Learning Memoryless Policies (Model free RL of POMDP)</h2>
<p><img alt="quiz 5:" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-9173406f2d344664.png"/></p>
<ul>
<li>in the MDP, There 3 blue states, to get to the green state, the agent needs to move to right in 2 of the states and move to left in another state. So the solution for this quiz is 1/3 and 2/3.</li>
</ul>
<p><img alt="验证quiz" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-a63cdcbcb73523dd.png"/></p>
<ul>
<li>solve and compare the value function based on the 50-50 probability (policy) and 1/3 - 2/3 policy.<blockquote><p>X = p * (0.5 * X) + (1 -p)(0.5 * Y)  =&gt; X = 0.5 * (1 - p)Y / (1 - 0.5p)
Y = p * (0.5 * x) + (1 -p) =&gt; X = 2 <em>(Y - 1 + p) / p
Z = p \</em> (1) + (1 -p) (0.5 * Z) =&gt; 
V = 1/3 * (X + Y + Z)</p>
</blockquote>
</li>
</ul>
<h2>RL as POMDP:</h2>
<p><img alt="quiz 7:  Bayesian RL" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-e83623b793fa63e0.png"/></p>
<ul>
<li>we can be in MDP A or B but not C, and we will be in state 2.</li>
<li>Next, we can take a purple action to figure out which MDP we are in.</li>
</ul>
<p><img alt="" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-b2d2289d40dae8a6.png"/></p>
<ul>
<li>So the optimal policy is figuring out which MDP we are in and get hight reward by exploiting in the belief state.</li>
<li>RL itself is like POMDP which requires exploring and exploiting.</li>
</ul>
<p><img alt="Bayesian RL" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-a4551ff84f46ca8f.png"/></p>
<h2>Predictive State Representation</h2>
<p><img alt="Predictive State Representation" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-2e5a6fa72a06bf5b.png"/></p>
<p><img alt="quiz 8: Using belief state to figure out Predictive State" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-94385421de2f7518.png"/></p>
<ul>
<li>The POMDP has 4 states but 3 observables represented by colors where the two red states are not distinguishable.</li>
<li>We can do test to get predictions of states. (e.g, a test can be moving up, and see the probability of ending up at the blue test).  </li>
</ul>
<p><img alt="Predictive State" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-4de05f4afd159cdc.png"/></p>
<ul>
<li>Belief state and predictive state does not have a one-on-one mapping.</li>
<li>Increasing number of tests will constrain the mapping.</li>
</ul>
<p><img alt="PSR Theorem" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-64cba59d3e23c01b.png"/></p>
<p>Why go to PSR?</p>
<ul>
<li>We can not observe all the states in POMDP and they might not even exist.</li>
<li>we can do some test to figure out what state we might be in.</li>
<li>Learning a PSR sometimes is easier than learning a POMDP.</li>
<li>Number of test is always going to be no more than the number of states</li>
</ul>
<p><img alt=" What Have We Learned?" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/conge.livingwithfcs.org/assets/images/计算机科学/118382-3b3c97e223cb009d.png"/></p>
<pre><code>2015-10-23 初稿
2015-11-03 补全
2015-12-04 reviewed
</code></pre>
