<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2015/2015-09-06-Reinforcement-Learning--di-si-zhou-ke-cheng-bi-ji-.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2015/09/06/Reinforcement-Learning--di-si-zhou-ke-cheng-bi-ji-">站点原文</a></h2>
<hr/>
<h2>layout: post
title: "Reinforcement Learning 第四周课程笔记"
date: "2015-09-06 12:04:19"
categories: 计算机科学
excerpt: "本周三件事：看课程视频，阅读 Sutton (1988)，作业3（HW3）。 以下为视频截图和笔记： Temporal Difference  ..."
auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p>本周三件事：看课程视频，<a href="http://www.jianshu.com/p/d2796b5f87a4">阅读 Sutton (1988)</a>，作业3（HW3）。</p>
<p>以下为视频截图和笔记：</p>
<h2>Temporal Difference  Learning</h2>
<p><img alt="Read Sutton 1988 first" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-504d712e4b0ca50f.png"/></p>
<ul>
<li>Read Sutton, Read Sutton, Read Sutton. Because the final project was based on it!</li>
</ul>
<p><img alt="Three families of RL algorithms" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-311884cbd051001b.png"/></p>
<ol>
<li>Model based</li>
<li>Model free</li>
<li>Policy search</li>
</ol>
<ul>
<li>Form 1 --&gt; 3: more direct learning</li>
<li>From 3 --&gt; 1 more supervised</li>
</ul>
<hr/>
<h2>TD-lambda</h2>
<p><img alt="TD-lambda" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-f152593e62818c2f.png"/></p>
<p><img alt="Quiz 1: TD-lambda Example" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-d38378591d3a8af2.png"/></p>
<ul>
<li>in this case the model is known, the calculation is easy.</li>
</ul>
<p><img alt="Quiz 2: Estimating from Data" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-05faa4b08ca9c881.png"/></p>
<ul>
<li>Remember from the previous lecture, we need to get value from each episode and average over them.</li>
</ul>
<p><img alt="Computing Estimates Incrementally" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-c22280d7d09675e9.png"/></p>
<ul>
<li>The rewrite makes the formula looks a lot like neuro-net learning. and alpha is introduced.</li>
</ul>
<p><img alt="Quiz 2: alpha will mke learning converge (tips:if 指数i大于1, 1/(T)&amp;lt;sup&amp;gt;i&amp;lt;/sup&amp;gt; will be bounded" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-48e9f2fda101215d.png"/></p>
<p><img alt="TD (1) rule" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-7b73fc9a7148b71f.png"/></p>
<p><img alt="TD(1) with and without repeated states" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-6af4a8d2994cc513.png"/></p>
<ul>
<li>When no repeated states, the TD(1) is the same as outcome-based updates ( which is see all the rewards in each state and update weights).</li>
<li>when there is repeated states, extra learning happens.</li>
</ul>
<p><img alt="Why TD(1) is &amp;quot;Wrong&amp;quot;" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-d7dcc73a78d77d2c.png"/></p>
<ul>
<li>in case of TD(1) rule, V(s2) can be estimated by average episodes. we only see V(s2) once and the value is 12. Then V(s2) = 12</li>
<li>in case of Maximum likelihood estimates, we have to kind of learn the transition from data. e.g. for the first 5 episodes, we saw s&lt;sub&gt;3&lt;/sub&gt;-&gt;s&lt;sub&gt;4&lt;/sub&gt; 3 times and s&lt;sub&gt;3&lt;/sub&gt; -&gt; s&lt;sub&gt;5&lt;/sub&gt; 2 times. So the transition probability can be extracted from data as 0.6 and 0.4 respectively.</li>
</ul>
<p><img alt="TD(0) Rule" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-9598276643dcdf3e.png"/></p>
<ul>
<li>First of all, if we have infinite data, TD(1) will also do the right thing.</li>
<li>When we have finite data, we can repeatedly infinitely sample the data to figure out all the ML. This is what TD(0) do.</li>
</ul>
<p><img alt="Connecting TD(0) and TD(1)" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-67c08454de051f6b.png"/></p>
<h2>K-Step Estimators</h2>
<p><img alt="K-Step Estimators" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-43686f7925cab82d.png"/></p>
<ul>
<li>E1 is one-step estimator (one-step look up) TD(0)</li>
<li>E2 is two-step estimator, and Ek is k-step lookup.</li>
<li>When K goes to infinity, we got TD(1)</li>
</ul>
<p><img alt="K-step Estimators and TD-lambda" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-90ca5c847c79ce90.png"/></p>
<p>TD-lambda can be seen as weighted combination of K-step estimators. the weight factor are λ&lt;sup&gt;k&lt;/sup&gt;(1-λ).</p>
<p><img alt="Why use TD-lambda?" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-bbd74b441ed9432f.png"/></p>
<p>The best performed lambda is typically not TD(0), but some λ in between 0 and 1.</p>
<p><img alt="Summary" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-a078afff664eb78f.png"/></p>
<pre><code>2015-09-5 初稿
2015-12-03 reviewed and revised until the "Connecting TD(0) and TD(1)" slides
</code></pre>
