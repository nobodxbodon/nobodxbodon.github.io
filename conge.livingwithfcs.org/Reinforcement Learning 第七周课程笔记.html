<h2><a href="https://github.com/conge/conge.github.io/blob/master/_posts/2015/2015-09-30-Reinforcement-Learning--di-qi-zhou-ke-cheng-bi-ji-.md">仓库源文</a>，<a href="https://conge.livingwithfcs.org/2015/09/30/Reinforcement-Learning--di-qi-zhou-ke-cheng-bi-ji-">站点原文</a></h2>
<hr/>
<h2>layout: post
title: "Reinforcement Learning 第七周课程笔记"
date: "2015-09-30 02:54:06"
categories: 计算机科学
excerpt: "This week's tasks watch Reward Shaping. read Ng, Harada, Russell (1999) ..."
auth: conge</h2>
<ul>
<li>content
{:toc}</li>
</ul>
<p><img alt="" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-5130b0de811fc710.png"/></p>
<h2>This week's tasks</h2>
<ul>
<li>watch <em>Reward Shaping</em>.</li>
<li>read <em>Ng, Harada, Russell (1999)</em> and* Asmuth, Littman, Zinkov (2008).*</li>
<li>office hours on Friday, October 2nd, from 4-5 pm(EST).</li>
<li>Homework 6.</li>
</ul>
<h2>Why changing RF?</h2>
<p><img alt="Why changing MDP" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-8266bfdd6ee09ca4.png"/></p>
<p>Given an MDP, RF can affect the behavior of the learner/agent so it ultimately specifies the behavior (or policy) we want for the MDP. So changing rewards can make the MDP easy to solve and represent</p>
<ol>
<li><strong>Semantics</strong>: what the agent are expected to do at each state;</li>
<li><strong>Efficiency</strong>: speed (experience and/or computation needed), space (complexity), and solvability .</li>
</ol>
<h2>How to change RF without changing optimal policy.</h2>
<p><img alt="How to Change RF" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-060f9fe65c9e2e60.png"/></p>
<p>Given an MDP described by &lt;S, A, R, T, γ&gt;, there are three ways to change R without changing optimal solution. (Note, if we know T, then it is not a RL problem any more, so this part of lecture if for MDP not RL specifically).</p>
<ol>
<li>Multiply by a positive constant ( non-zero 'cause multiply by 0 will erase all the reward information)</li>
<li>shift by a constant</li>
<li>non-linear potential-based</li>
</ol>
<h3>1. Multiply by a positive constant</h3>
<p><img alt="Quiz 1" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-ce3893e4ffd59041.png"/></p>
<ul>
<li>Q(s,a) is the solution of Bellman function with the old RF R(s,a).</li>
<li>R'(s,a) is a new RF with is the old RF multiplying by a constant.</li>
<li>What's the new solution Q'(s,a) with respect to the new RF R'(s,a)  and old Q(s,a)?</li>
</ul>
<p>Here is how to solve the problem:</p>
<ol>
<li>Q = R + γR+γ&lt;sup&gt;2&lt;/sup&gt;R + ... + γ&lt;sup&gt;∞ &lt;/sup&gt;R)</li>
<li>Q' = R' + γR'+γ&lt;sup&gt;2&lt;/sup&gt;R' + ... + γ&lt;sup&gt;∞ &lt;/sup&gt;R'</li>
<li>Replace R' with c*R,
Q'=(c*R) +γ(c*R)+γ&lt;sup&gt;2&lt;/sup&gt;(c*R) + ... + γ&lt;sup&gt;∞ &lt;/sup&gt;(c*R)
=c(R + γR+γ&lt;sup&gt;2&lt;/sup&gt;R + ... + γ&lt;sup&gt;∞ &lt;/sup&gt;R)
=c*Q</li>
</ol>
<h3>2. shift by a constant</h3>
<p><img alt="Quiz 2: Add a scalar" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-b11b96e380510abf.png"/></p>
<p><img alt="Solution and proof of Quiz 2" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-9dc4a4660fed5fd6.png"/></p>
<blockquote>
<ol>
<li>Q = R + γR+γ&lt;sup&gt;2&lt;/sup&gt;R + ... + γ&lt;sup&gt;∞ &lt;/sup&gt;R)</li>
<li>Q' = R' + γR'+γ&lt;sup&gt;2&lt;/sup&gt;R' + ... + γ&lt;sup&gt;∞ &lt;/sup&gt;R'</li>
<li>Replace R' with R+c,
Q'=(R+c) +γ(R+c)+γ&lt;sup&gt;2&lt;/sup&gt;(R+c) + ... + γ&lt;sup&gt;∞ &lt;/sup&gt;(R+c)
=(R + γR+γ&lt;sup&gt;2&lt;/sup&gt;R + ... + γ&lt;sup&gt;∞ &lt;/sup&gt;R) + (c+γc+γ&lt;sup&gt;2&lt;/sup&gt;c + ... + γ&lt;sup&gt;∞ &lt;/sup&gt;c)</li>
<li>The first part is Q and the second part is geometric series. So,
Q'  = Q + c/(1-γ)</li>
</ol>
</blockquote>
<h3>3. nonlinear potential-based reward shaping</h3>
<p><img alt="Quiz 3: potential-based reward shaping" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-efa41453eb9c20b6.png"/></p>
<ol start="5">
<li>Given γ is in (0,1), so γ&lt;sup&gt;∞ &lt;/sup&gt;=0. Then we have Q':
Q' = Q - ψ(s)</li>
</ol>
<blockquote>
<ol>
<li>Q = R + γR+γ&lt;sup&gt;2&lt;/sup&gt;R + ... + γ&lt;sup&gt;∞ &lt;/sup&gt;R)</li>
<li>Q' = R' + γR'+γ&lt;sup&gt;2&lt;/sup&gt;R' + ... + γ&lt;sup&gt;∞ &lt;/sup&gt;R'</li>
<li>Replace R' with R-ψ(s) + γψ(s'),
Q'=(R-ψ(s) + γψ(s')) +γ(R-ψ(s') + γψ(s''))+γ&lt;sup&gt;2&lt;/sup&gt;(R-ψ(s'') + γψ(s''')) + ... + γ&lt;sup&gt;∞ &lt;/sup&gt;(R-ψ(s&lt;sup&gt;∞ &lt;/sup&gt;) + γψ(s'&lt;sup&gt;∞ &lt;/sup&gt;))
=(R + γR+γ&lt;sup&gt;2&lt;/sup&gt;R + ... + γ&lt;sup&gt;∞ &lt;/sup&gt;R) + (-ψ(s) + γψ(s') +γ(-ψ(s') + γψ(s''))+γ&lt;sup&gt;2&lt;/sup&gt;(-ψ(''s) + γψ(s''')) + ... + γ&lt;sup&gt;∞ &lt;/sup&gt;(-ψ(s&lt;sup&gt;∞ &lt;/sup&gt;) + γψ(s'&lt;sup&gt;∞ &lt;/sup&gt;))</li>
<li>The first part is Q. In the second part, most of the elements are cancelling each other out and only has the very first and last elements left. So,
Q'  = Q + (-ψ(s) + γ&lt;sup&gt;∞ &lt;/sup&gt;ψ(s'&lt;sup&gt;∞ &lt;/sup&gt;)</li>
</ol>
</blockquote>
<h2>Q-learning with potential</h2>
<p>Updating the Q function with the potential based reward shaping,</p>
<ol>
<li>Q function will converge at Q*(s,a).</li>
<li>we know that Q(s,a) = Q*(s,a) - ψ(s). If we initialize Q(s,a) with zero, then  Q*(s,a) - ψ(s) =  Q*(s,a) - max&lt;sub&gt;a&lt;/sub&gt;Q*(s,a) = 0, that means a is optimal.</li>
<li>so Q-learning with potential is like initializing Q at Q*</li>
</ol>
<p><img alt="Q-learning with potential" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-22ebfa9051884003.png"/></p>
<h2>What have we learned?</h2>
<p><img alt="Summary" src="/media/wwww/share/study/聚聚/源数据/博客聚合/conge.livingwithfcs.org/assets/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/118382-1a8dc85f3abe5f75.png"/></p>
<ul>
<li>Potential functions is a way to speed up the process to solve MDP</li>
<li>Reward shaping might have suboptimal positive loops which will never converge?</li>
</ul>
<pre><code>2015-09-29 初稿
2015-12-04 reviewed and revised
</code></pre>
