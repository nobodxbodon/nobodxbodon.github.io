<h2>原文：<a href="https://kexizeroing.github.io/what-is-nginx">What is Nginx</a></h2>
<hr/>
<p>layout: "../layouts/BlogPost.astro"
title: "What is Nginx"
slug: what-is-nginx
description: ""
added: "Nov 3 2022"
tags: [devops]</p>
<h2>updatedDate: "Jan 15 2024"</h2>
<p>Developers started simply using the app as an HTTP server. You can serve your node.js application without using any other web servers. Other web development frameworks in Go, Java and Swift also do this. When you serve a node.js app, note that you are the author of your own web server. Any potential bug in your app is a directly exploitable bug on the internet. Some people are not comfortable with this. Adding a layer of Apache or Nginx in front of your app (proxies the requests to a node.js server) means that you have a battle-tested, security-hardened piece of software on the live internet as an interface to your app.</p>
<p>Nginx (pronounced "engine-x") is open source software for web serving, reverse proxying, caching, load balancing, media streaming, and more. It started out as a web server designed for maximum performance and stability.</p>
<h3>Nginx as a reverse proxy</h3>
<p>Many modern web applications written in Node.js or Angular can run with their own standalone server but they lack a number of advanced features like load balancing, security, and acceleration that most of these applications demands. Nginx with its advanced features can act as a reverse proxy while serving the request for a Node.js application. The servers that Nginx proxies requests to are known as <strong>upstream servers</strong>.</p>
<blockquote><p><strong>Forward proxies</strong> are crucial for privacy and security when browsing the internet, accessing geo-restricted content, web scraping, and much more. <strong>Reverse proxies</strong> are important for websites with many visitors daily because they help avoid overloading and are a perfect fit for caching content, SSL encryption.</p>
</blockquote>
<p>&lt;img alt="nginx" src="https://raw.gitmirror.com/kexiZeroing/blog-images/main/008vxvgGly1h7rrtzcyydj30ww0magnw.jpg" width="700" &gt;</p>
<p>Create a <code>server</code> block that will act as a reverse proxy. <strong>The <code>proxy_pass</code> directive in the configuration makes the server block a reverse proxy</strong>. All traffic destined to the domain <code>SUBDOMAIN.DOMAIN.TLD</code> and those matches with <code>location</code> block will be forwarded to <code>http://PRIVATE_IP:3000</code> where the node.js or angular application is running. When Nginx proxies a request, it automatically makes some adjustments to the request headers it receives from the client. To adjust or set headers, we can use the <code>proxy_set_header</code> directive. For example, the "Host" header by default will be set to the value of <code>$proxy_host</code>, a variable that will contain the domain name or IP address taken directly from the <code>proxy_pass</code> directive. It can also be set to <code>$host</code> which is equal to the "Host" in the header of the request.</p>
<pre><code class="lang-nginx">server {  
  listen 80;
  server_name SUBDOMAIN.DOMAIN.TLD;
  location / {  
    proxy_pass http://PRIVATE_IP:3000;  
    proxy_http_version 1.1;  
    proxy_set_header Host $host;  
  }  
}
</code></pre>
<p>The <code>Forwarded</code> request header (i.e. <code>X-Forwarded-For</code>, <code>X-Forwarded-Host</code>) contains information that may be added by reverse proxy servers (load balancers, CDNs, and so on) that would otherwise be altered or lost when proxy servers are involved in the path of the request. For example, if a client is connecting to a web server through an HTTP proxy, server logs will only contain the IP address, host address, and protocol of the proxy. The header is optional and may be added to, modified, or removed, by any of the proxy servers on the path to the server.</p>
<h3>Nginx Load Balancing</h3>
<p>Nginx can also be configured to act as a load balancer that can handle a large number of incoming connections and distribute them to separate upstream servers for processing thereby achieving fault tolerance and better performance of deployed applications. To configure Nginx as a load balancer, the first step is to include the <code>upstream</code> in the configuration. Once upstream servers have been defined, you just need to refer them in the <code>location</code> block by using <code>proxy_pass</code> directive. For example, whenever traffic arrives at port 80 for the domain <code>SUBDOMAIN.DOMAIN.TLD</code>, Nginx will forward the request to each upstream servers one by one. The default method of choosing an upstream server will be round robin.</p>
<ul>
<li><strong>round robin</strong>: distributes the traffic to upstream servers equally and is the default scheme if you don’t specify. Each upstream server is selected one by one in turn according to the order you place them in the configuration file. </li>
<li><strong>least connected</strong>: assigns the request to the upstream server with the least number of active connections. To configure the least connected load balancing, add <code>least_conn</code> directive at the first line within the upstream module.</li>
<li><strong>IP hash</strong>: selects an upstream server by generating a hash value based on the client’s IP address as a key. This allows the request from clients to be forwarded to the same upstream server provided it is available and the clients IP address has not changed. Add <code>ip_hash</code> directive at the first line within the upstream module.</li>
<li><strong>weighted method</strong>: the upstream server with the highest weight is selected most often. This scheme is useful in the situation where the upstream server’s resources are not equal and favors the one with better available resources. Add <code>weight</code> directive after the URL parameter in the upstream section.</li>
</ul>
<pre><code class="lang-nginx"># nginx.conf
# backend_servers is the upstream module name
upstream backend_servers {
  # may specify load balancing method here
  server 10.0.2.144;
  server 10.0.2.42;
  server 10.0.2.44;
}

server {
  listen 80; 
  server_name SUBDOMAIN.DOMAIN.TLD;
  location / {
    proxy_pass http://backend_servers;
    proxy_set_header Host $host;   
    proxy_set_header X-Real-IP $remote_addr;
  }
}
</code></pre>
<h3>Nginx and API gateway</h3>
<p>API gateway is an API management tool that sits between a client and a collection of backend services. It acts as a reverse proxy to accept all API calls, takes request and redirects them to the right service.</p>
<p>It is easier to think about them if you realize they aren't mutually exclusive. Think of an API gateway as a specific type reverse proxy implementation. API gateway can be configured dynamically via API and potentially via UI, while traditional reverse proxy (like Nginx or Apache) is configured via config file and has to be restarted when configuration changes.</p>
<blockquote><p>It is not uncommon to see both used in conjunction where the API gateway is treated as an application tier that sits behind a reverse proxy for load balancing and health checking. As you take a basic reverse proxy setup and start adding on more pieces like authentication, rate limiting, dynamic config updates, and service discovery, people are more likely to call that an API gateway.</p>
</blockquote>
<p>Typically the types of functions the gateway may provide may include:</p>
<ul>
<li>access control, filtering traffic so only authenticated/authorized traffic gets through.</li>
<li>rate limiting, restricting how much traffic can be sent by each client of the API.</li>
<li>analytics capture and logging, tracking what's going on on the API.</li>
<li>security filtering, checking the content on incoming messages for attacks.</li>
<li>traffic routing, sending traffic to different endpoints in your own infrastructure depending on the sender or the request.</li>
</ul>
<p>&lt;img alt="gateway" src="https://raw.gitmirror.com/kexiZeroing/blog-images/main/e6c9d24ely1h61oh00uc2j20tq14y42a.jpg" width="500" &gt;</p>
<p>API Gateway and Load Balancer are two different things. Load Balancer works at protocol or socket level (eg. tcp, http, or port 3306 etc). Its job is to balance the incoming traffic by distributing it to the destinations with various logics. It doesn't offer features such as authorization checks, authentication of requests etc.</p>
<h3>Nginx and Ingress</h3>
<p>Ingress is a powerful component of any Kubernetes application. It exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.</p>
<p>You can deploy a bunch of ingress rules, but nothing will happen unless you have a controller that can process them. An Ingress Controller is a pod that is configured to interpret ingress rules. <code>ingress-nginx</code> is an Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer.</p>
<p>Considering what you can do with Ingress, it is easy to think of an Ingress controller as an API gateway, reducing the need for a separate API gateway resource in your cloud architecture.</p>
<h3>Nginx command line</h3>
<p>NGINX has only a few command-line parameters, and the configuration is done entirely via the configuration file (<code>/usr/local/etc/nginx/nginx.conf</code>).</p>
<table>
<thead><tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>nginx</td>
<td>start NGINX (<code>brew install nginx</code>)</td>
</tr>
<tr>
<td>nginx -s stop</td>
<td>quick shutdown</td>
</tr>
<tr>
<td>nginx -s quit</td>
<td>graceful shutdown</td>
</tr>
<tr>
<td>nginx -s reload</td>
<td>reloade the configuration file</td>
</tr>
<tr>
<td>nginx -c filename</td>
<td>specify a configuration file which is not default</td>
</tr>
<tr>
<td>nginx -t</td>
<td>don’t run, just test the configuration file </td>
</tr>
<tr>
<td>nginx -v</td>
<td>print version</td>
</tr>
<tr>
<td>nginx -V</td>
<td>print NGINX version, compiler version and configure parameters</td>
</tr>
</tbody>
</table>
<p>The tool you'll ever need to configure your NGINX server: <a href="https://do.co/nginxconfig">https://do.co/nginxconfig</a></p>
