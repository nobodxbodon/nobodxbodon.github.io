<h2><a href="https://github.com/kexizeroing/kexizeroing.github.io/blob/master/src/posts/how-semantic-image-search-works.md">仓库源文</a>，<a href="https://kexizeroing.github.io/how-semantic-image-search-works">站点原文</a></h2>
<hr/>
<p>layout: "../layouts/BlogPost.astro"
title: "How semantic image search works"
slug: how-semantic-image-search-works
description: ""
added: "Jun 27 2024"
tags: [AI, code]</p>
<h2>updatedDate: "Jun 30 2024"</h2>
<h2>Image search with embed metadata</h2>
<p>Vercel recently shipped an open-source ai-powered image search app. The general idea here is to generate the metadata for the images we want to index using LLMs, and store them as embeddings in a vector database. So that when someone does a query, we would convert it into an embed and do a cosine similarity search.</p>
<ul>
<li>Source code: <a href="https://github.com/vercel-labs/semantic-image-search">https://github.com/vercel-labs/semantic-image-search</a></li>
<li>Demo App: <a href="https://semantic-image-search.vercel.app">https://semantic-image-search.vercel.app</a></li>
</ul>
<h3>Upload Images</h3>
<pre><code class="lang-js">import { list, put } from "@vercel/blob";

async function main() {
  const basePath = "images-to-index";
  const files = await getJpgFiles(basePath);
  const { blobs } = await list();

  for (const file of files) {
    const exists = blobs.some((blob) =&gt; blob.pathname === file);
    if (exists) {
      console.log(`File (${file}) already exists in Blob store`);
      continue;
    }
    const filePath = basePath + "/" + file;
    const fileContent = fs.readFileSync(filePath);

    await put(file, fileContent, { access: "public" });
    console.log(`Uploaded ${file}`);
  }
}

async function getJpgFiles(dir) {
  const files = await fs.promises.readdir(dir);
  const jpgFiles = files.filter(
    (file) =&gt; path.extname(file).toLowerCase() === ".jpg",
  );
  return jpgFiles;
}
</code></pre>
<h3>Generate Metadata</h3>
<pre><code class="lang-js">import { openai } from "@ai-sdk/openai";
import { generateObject } from "ai";
import { z } from "zod";
import { list } from "@vercel/blob";

async function main() {
  const blobs = await list();
  const files = blobs.blobs.map((b) =&gt; b.url);

  const images = [];

  for (const file of files) {
    const result = await generateObject({
      model: openai("gpt-4o"),
      schema: z.object({
        image: z.object({
          title: z.string().describe("an artistic title for the image"),
          description: z
            .string()
            .describe("A one sentence description of the image"),
        }),
      }),
      maxTokens: 512,
      messages: [
        {
          role: "user",
          content: [
            { type: "text", text: "Describe the image in detail." },
            {
              type: "image",
              image: file,
            },
          ],
        },
      ],
    });
    images.push({ path: file, metadata: result.object.image });
  }

  await writeAllMetadataToFile(images, "images-with-metadata.json");
}
</code></pre>
<h3>Database Setup</h3>
<pre><code class="lang-js">import { varchar, index, pgTable, vector, text } from "drizzle-orm/pg-core";
import { nanoid } from "nanoid";

export const images = pgTable(
  "images",
  {
    id: varchar("id", { length: 191 })
      .primaryKey()
      .$defaultFn(() =&gt; nanoid()),
    title: text("title").notNull(),
    description: text("description").notNull(),
    path: text("path").notNull(),
    embedding: vector("embedding", { dimensions: 1536 }).notNull(),
  },
  (table) =&gt; ({
    embeddingIndex: index("embeddingIndex").using(
      "hnsw",
      table.embedding.op("vector_cosine_ops"),
    ),
  }),
);

export const dbImageSchema = z.object({
  id: z.string(),
  embedding: z.array(z.number()),
  title: z.string(),
  path: z.string(),
  description: z.string(),
  similarity: z.number().optional(),
});
</code></pre>
<h3>Embed Metadata and Save to Database</h3>
<pre><code class="lang-js">import { openai } from "@ai-sdk/openai";
import { embed } from "ai";
import { drizzle } from "drizzle-orm/postgres-js";
import postgres from "postgres";

export const embeddingModel = openai.embedding("text-embedding-3-small");
export const client = postgres(process.env.POSTGRES_URL!);
export const db = drizzle(client);

async function main() {
  const imagesWithMetadata = await getMetadataFile("images-with-metadata.json");

  for (const image of imagesWithMetadata) {
    const { embedding } = await embed({
      model: embeddingModel,
      value: image.metadata.title + "\n" + image.metadata.description,
    });

    try {
      await saveImage({
        title: image.metadata.title,
        description: image.metadata.description,
        id: nanoid(),
        path: image.path,
        embedding,
      });
    } catch (e) {
      console.error(e);
    }
  }
}

const saveImage = async (image) =&gt; {
  const safeImage = dbImageSchema.parse(image);
  const [savedImage] = await db.insert(images).values(safeImage);
  return savedImage;
};
</code></pre>
<h3>Query Images</h3>
<pre><code class="lang-js">import { kv } from "@vercel/kv";

const { embedding: _, ...rest } = getTableColumns(images);
const imagesWithoutEmbedding = {
  ...rest,
  embedding: sql&lt;number[]&gt;`ARRAY[]::integer[]`,
};

const getImages = async (query) =&gt; {
  const formattedQuery = query
    ? "q:" + query?.replaceAll(" ", "_")
    : "all_images";

  const cached = await kv.get(formattedQuery);
  if (cached) {
    return { images: cached };
  } else {
    if (query === undefined || query.length &lt; 3) {
      const allImages = await db
        .select(imagesWithoutEmbedding)
        .from(images)
        .limit(20);
      await kv.set("all_images", JSON.stringify(allImages));
      return { images: allImages };
    } else {
      const directMatches = await findImageByQuery(query);
      const semanticMatches = await findSimilarContent(query);
      const allMatches = uniqueItemsByObject(
        [...directMatches, ...semanticMatches].map((image) =&gt; ({
          ...image.image,
          similarity: image.similarity,
        })),
      );

      await kv.set(formattedQuery, JSON.stringify(allMatches));
      return { images: allMatches };
    }
  }
};

export const findImageByQuery = async (query) =&gt; {
  const result = await db
    .select({ image: imagesWithoutEmbedding, similarity: sql&lt;number&gt;`1` })
    .from(images)
    .where(
      or(
        sql`title ILIKE ${"%" + query + "%"}`,
        sql`description ILIKE ${"%" + query + "%"}`,
      ),
    );
  return result;
};

export const findSimilarContent = async (description) =&gt; {
  const embedding = await generateEmbedding(description);
  const similarity = sql&lt;number&gt;`1 - (${cosineDistance(images.embedding, embedding)})`;
  const similarGuides = await db
    .select({ image: imagesWithoutEmbedding, similarity })
    .from(images)
    .where(gt(similarity, 0.28)) // experiment with this value based on your embedding model
    .orderBy((t) =&gt; desc(t.similarity))
    .limit(10);

  return similarGuides;
};
</code></pre>
<h2>Image Embeddings 101</h2>
<p>Embeddings are different from images in their raw form. An image file contains RGB data that says exactly what color each pixel is. Embeddings encode information that represents the contents of an image. Embeddings are calculated by computer vision models which are usually trained with large datasets of pairs of text and image. The goal of such a model is to build an "understanding" of the relationship between images and text.</p>
<ul>
<li>CLIP (Contrastive Language-Image Pretraining) is a multimodal embedding model developed by OpenAI and released in 2019. CLIP was trained using over 400 million pairs of images and text.</li>
<li>ImageBind, a new embedding model released in 2023 by Meta Research, is pioneering an approach to AI embeddings that allows you to encode information across many different modalities, from images to text to audio to depth information.</li>
</ul>
<p>The text embedding and image embedding are both calculated by an embedding model. You can compare text and image embeddings to see how similar text is to an image. You can save embeddings in a vector database to run efficient searches between image and text embeddings. This efficient searching enables you to build a semantic image search engine.</p>
