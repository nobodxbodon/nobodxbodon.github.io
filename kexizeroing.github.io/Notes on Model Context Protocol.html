<h2><a href="https://github.com/kexizeroing/kexizeroing.github.io/blob/master/src/blog/notes-on-mcp.md">仓库源文</a>，<a href="https://kexizeroing.github.io/notes-on-mcp">站点原文</a></h2>
<h3>Historical context: The Path to MCP</h3>
<p>Early AI assistants were limited to text generation, unable to interact with external tools or real-time data. The introduction of function calling and plugins in 2023 allowed models to execute code, browse the web, and interact with APIs, marking the shift toward AI agents. However, each integration was fragmented, requiring custom implementations for different tools, making scaling difficult.</p>
<p>MCP, introduced by Anthropic in late 2024, solves this problem by providing a unified protocol for AI-tool interactions. Instead of custom adapters for each tool, MCP allows developers to expose functionality once, making it accessible to any AI supporting MCP. It also eliminates the inefficiencies of tool-specific APIs by offering a structured, self-describing interface. This enables seamless, scalable AI-tool connectivity, much like how USB standardized device connections.</p>
<blockquote><p>This isn't something that you couldn't do before. You could technically write a bunch of code to provide any model with relevant function call definitions, and then implement those functions to do the things the model asks for. But for one, this was very tedious. You'd have to figure out how to do it from scratch each time. Each implementation might be different. And this would all be in code, your Claude desktop app couldn't access these functions.</p>
<p>MCP adds another layer: your tools are hosted outside your app, on a separate server. MCP servers make sense when you want your context and tools to be shared across many apps, models, or environments.</p>
</blockquote>
<h3>MCP is not magic</h3>
<p>MCP isn't magic — it's a standard way for AI to discover and use tools without learning every API's specific details. An MCP server is like a menu of tools. Each tool has a name, a description, a schema defining what info it needs, and the actual code that makes the API calls. AI applications can dynamically query these servers to execute tasks such as reading files, querying databases, or creating new integrations.</p>
<p>Whether you’re using OpenAI, Anthropic, or another LLM, MCP stays the same. The code becomes more maintainable (since you’re calling a generic <code>mcpClient.request</code> rather than service-specific code). Debugging is also easier, since you can monitor the JSON-RPC messages to see exactly what was requested and returned, rather than parsing model-generated text for clues.</p>
<h3>MCP server and client</h3>
<p>MCP uses a client-server design where applications can connect to multiple resources.</p>
<p>The <strong>MCP host</strong> is the program that's going to access the MCP servers. This might be Claude Desktop, Cursor, Windsurf, or any other application that supports MCP. Any application implementing the MCP protocol to allow connections to MCP servers is a host.</p>
<p>On this host, you're going to run one or multiple <strong>MCP clients</strong> - each client will maintain a relationship to a single MCP server. The host creates a client instance to communicate with a specific MCP server, and the client handles the low-level details of the protocol.</p>
<p>The <strong>MCP server</strong> is the most interesting concept for 99% of us. The server is the program that exposes a set of capabilities to the host application. If you want to allow a host to read emails, you can connect it to a Gmail MCP Server. If you want the host to post in Slack, you connect it to a Slack MCP Server. If you have some custom functionality you want an LLM to perform, you can build a new MCP server. The server could be running locally, or it could be running on a remote server.</p>
<p>The client connects to its server using a <strong>transport</strong>. This transport is responsible for sending messages between the client and the server. There are currently two supported transports. You can communicate via <code>stdio</code> - in other words, via the terminal. Or you can communicate through HTTP via server-sent events. This is useful if you want to run your server on a remote machine.</p>
<blockquote><p>Transports are how the model talks to your MCP server. Today, StreamableHTTP is the main one. The model uses standard HTTP to hit a URL and create a connection. Optionally, it also allows the use of SSE on the side for real time notifications. The other popular transport is stdio, used in local or CLI environments where the model and tools share the same process.</p>
</blockquote>
<p>The <strong>protocol</strong> defines JSON message formats, based on JSON-RPC, for communication between client and server. This simple contract allows for incredible flexibility. Your server doesn’t need to know anything about the LLM, and the LLM doesn’t need to know anything about your server’s internal implementation. They just need to speak the common language of MCP.</p>
<pre><code class="lang-json">// To list available tools, an MCP client sends a request like this:
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "tools/list",
  "params": {}
}

// Server would reply with a structured JSON listing the tools
// (each with a name, description, and input schema)
{
  "jsonrpc": "2.0",
  "id": 1,
  "tools": [
    {
      "name": "createGitHubIssue",
      "description": "Create a GitHub issue",
      "inputSchema": {
        ...
      },
    }
  ]
}

// Client invokes a call
{
  "jsonrpc": "2.0",
  "id": 2,
  "method": "tools/call",
  "params": {
    "name": "createGitHubIssue",
    "arguments": {
      "title": "My Issue",
      "body": "This is the body of my issue",
      "labels": ["bug"]
    }
  }
}

// Server executes the function
// returns the result in a structured JSON response
{
  "jsonrpc": "2.0",
  "id": 2,
  "content": [
    {
      "type": "text",
      "text": "Issue 143 created successfully!"
    }
  ],
  "isError": false
}

// The client can then feed that result back into the model’s context or response.
</code></pre>
<h3>The simplest MCP server</h3>
<p>One of the standout features of MCP is its flexibility in server development. Developers can use any programming language that can print to stdout or serve an HTTP endpoint, allowing them to choose their preferred language and technology stack.</p>
<pre><code class="lang-js">// npm i @modelcontextprotocol/sdk zod
// server-logic.js
import { McpServer } from "@modelcontextprotocol/sdk/server/mcp.js";
import { z } from "zod";

export const server = new McpServer({
  name: "weather-mcp-server",
  version: "1.0.0",
});

server.tool(
  "getWeather",
  "Tool to get the weather for a city",
  { city: z.string().describe("The city to get the weather for") },
  async ({ city }) =&gt; {
    // await fetch('wheather API')

    return {
      content: [
        {
          type: "text",
          text: `The weather in ${city} is sunny!`,
        },
      ],
    };
  }
);
</code></pre>
<blockquote><p>The <code>register*</code> methods (<code>registerTool</code>, <code>registerPrompt</code>, <code>registerResource</code>) are the recommended approach for new code. The older methods (<code>tool</code>, <code>prompt</code>, <code>resource</code>) remain available for backwards compatibility. The optional <code>title</code> field for better UI presentation. The title is used as a display name, while <code>name</code> remains the unique identifier.</p>
</blockquote>
<pre><code class="lang-js">// Using registerTool (recommended)
server.registerTool(
  "fetch-weather",
  {
    title: "Weather Fetcher",
    description: "Get weather data for a city",
    inputSchema: { city: z.string() },
  },
  async ({ city }) =&gt; {
    const response = await fetch(`https://api.weather.com/${city}`);
    const data = await response.text();
    return {
      content: [{ type: "text", text: data }],
    };
  }
);
</code></pre>
<p>The stdio transport enables communication through standard input/output streams. This is particularly useful for local integrations and command-line tools.</p>
<pre><code class="lang-js">// 1. use stdio
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import { server } from "./server-logic.js";

const transport = new StdioServerTransport();
await server.connect(transport);
</code></pre>
<p>SSE transport enables server-to-client streaming with HTTP POST requests for client-to-server communication.</p>
<pre><code class="lang-js">// 2. use sse
import { SSEServerTransport } from "@modelcontextprotocol/sdk/server/sse.js";
import express from "express";
import { server } from "./server-logic.js";

const app = express();

let transport;
app.get("/sse", async (req, res) =&gt; {
  transport = new SSEServerTransport("/messages", res);
  await server.connect(transport);
});

app.post("/messages", async (req, res) =&gt; {
  await transport.handlePostMessage(req, res);
});

const port = process.env.PORT || 8081;
app.listen(port, () =&gt; {
  console.log(`MCP SSE Server is running on http://localhost:${port}/sse`);
});
</code></pre>
<p>Claude Desktop is the first MCP-compatible app, and it's the easiest way to test MCP. Open your Claude Desktop App configuration at <code>~/Library/Application Support/Claude/claude_desktop_config.json</code> in a text editor. After updating your configuration file, you need to restart Claude for Desktop. See the <a href="https://modelcontextprotocol.io/quickstart/user">documentation</a> for more details.</p>
<pre><code class="lang-json">{
  "mcpServers": {
    "weather-example": {
      "command": "node",
      "args": ["/ABSOLUTE/PATH/TO/PARENT/FOLDER/weather/build/index.js"]
    },
    "sequential-thinking": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-sequential-thinking"]
    }
  }
}
</code></pre>
<p>For Claude Code (only supports <code>stdio</code> transport), you can run it with a single command: <code>claude mcp add "weather-example" npx tsx "/path-to-the-file.ts"</code>. This tells Claude that in order to run the file, it should call <code>npx tsx /path-to-the-file.ts</code>.</p>
<pre><code class="lang-sh">claude mcp list
# No MCP servers configured.

claude mcp add "weather-example" npx tsx index.ts
# Added stdio MCP server weather-example...

claude mcp list
# weather-example: npx tsx index.ts

claude
# Actually run Claude Code
</code></pre>
<h3>AI SDK MCP clients</h3>
<p>The SDK supports connecting to MCP servers via either stdio (for local tools) or SSE (for remote servers). Once connected, you can use MCP tools directly with the AI SDK. The client exposes a <code>tools</code> method for retrieving tools from a MCP server.</p>
<pre><code class="lang-js">import { openai } from "@ai-sdk/openai";
import { experimental_createMCPClient as createMCPClient } from "ai";

const mcpClient = await createMCPClient({
  transport: {
    type: "sse",
    url: "http://localhost:8081/sse",
  },
  name: "My MCP Server",
});

// The client's tools method acts as an adapter between MCP tools and AI SDK tools.
// https://sdk.vercel.ai/docs/ai-sdk-core/tools-and-tool-calling#using-mcp-tools
const response = await generateText({
  model: openai("gpt-4o"),
  tools: await mcpClient.tools(),
  prompt: "Find products under $100",
});
</code></pre>
<pre><code class="lang-js">// Sometimes it's good to hint to the AI that you want it to use tools.
// export default async function getTools() {
//   const tools = await mcpClient.tools();
//   return {
//     ...tools,
//     getProducts,
//     recommendGuitar,
//   };
// }
const SYSTEM_PROMPT = `You are an AI for a music store.

There are products available for purchase. You can recommend a product to the user.
You can get a list of products by using the getProducts tool.

You also have access to a fulfillment server that can be used to purchase products.
You can get a list of products by using the getInventory tool.
You can purchase a product by using the purchase tool.
`;
</code></pre>
<blockquote><p>Update from Anthropic on 2025.5.2:
Until now, support for MCP was limited to Claude Desktop through local servers. Today, we're introducing Integrations, allowing Claude to work seamlessly with remote MCP servers across the web and desktop apps. Developers can build and host servers that enhance Claude’s capabilities, while users can discover and connect any number of these to Claude.</p>
<p>What this means is that you can bring your own remote MCP server to claude.ai. Users just need a URL to equip the LLM with new tools and capabilities.</p>
</blockquote>
<h3>Your API is not an MCP</h3>
<p>Consider the difference in practice. An API-shaped MCP server might expose four tools, and the LLM has to call each tool in sequence, pass IDs between calls, and handle potential failures at each step. The solution is building tools around complete user goals rather than API capabilities. Instead of four separate tools, create one tool that handles the entire workflow internally.</p>
<ol>
<li>LLMs are terrible at selection from a long list of tools.</li>
<li>Your existing API descriptions are probably not ready for LLM consumption.</li>
<li>Each conversation starts fresh with no memory of previous conversations. While they can see tool results within the current conversation, they have to figure out the right sequence of tools to use based on what's available. When those tools are low-level API wrappers, the LLM has to orchestrate multiple calls and manage the complexity of chaining them together each time.</li>
</ol>
<p>Each tool should do one thing and do it well. The <code>name</code> and <code>description</code> of your tools and their parameters are your primary interface with the LLM. Be clear, concise, and unambiguous. Log every single tool invocation. Record the tool name, the exact parameters it was called with, and the result it returned. This is invaluable for debugging.</p>
<h3>References and further reading</h3>
<ul>
<li><a href="https://github.com/modelcontextprotocol/typescript-sdk">https://github.com/modelcontextprotocol/typescript-sdk</a></li>
<li><a href="https://www.aihero.dev/model-context-protocol-tutorial">https://www.aihero.dev/model-context-protocol-tutorial</a></li>
<li><a href="https://glama.ai/blog/2024-11-25-model-context-protocol-quickstart">https://glama.ai/blog/2024-11-25-model-context-protocol-quickstart</a></li>
<li><a href="https://github.com/modelcontextprotocol/servers">https://github.com/modelcontextprotocol/servers</a></li>
<li><a href="https://github.com/punkpeye/awesome-mcp-servers">https://github.com/punkpeye/awesome-mcp-servers</a></li>
<li><a href="https://glama.ai/mcp/servers">https://glama.ai/mcp/servers</a></li>
<li><a href="https://www.pulsemcp.com">https://www.pulsemcp.com</a></li>
<li><a href="https://github.com/github/github-mcp-server">https://github.com/github/github-mcp-server</a></li>
<li><a href="https://github.com/modelcontextprotocol/inspector">https://github.com/modelcontextprotocol/inspector</a></li>
<li><a href="https://github.com/vercel-labs/mcp-for-next.js">https://github.com/vercel-labs/mcp-for-next.js</a></li>
<li><a href="https://github.com/punkpeye/fastmcp">https://github.com/punkpeye/fastmcp</a></li>
<li><a href="https://developer.chrome.com/blog/chrome-devtools-mcp">https://developer.chrome.com/blog/chrome-devtools-mcp</a></li>
<li><a href="https://developer.chrome.com/blog/autofix-runtime-devtools-mcp">https://developer.chrome.com/blog/autofix-runtime-devtools-mcp</a></li>
<li><a href="https://developers.openai.com/apps-sdk/build/mcp-server">https://developers.openai.com/apps-sdk/build/mcp-server</a></li>
</ul>
