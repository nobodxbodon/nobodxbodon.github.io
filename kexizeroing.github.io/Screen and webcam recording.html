<h2><a href="https://github.com/kexizeroing/kexizeroing.github.io/blob/master/src/blog/screen-webcam-recording.md">仓库源文</a>，<a href="https://kexizeroing.github.io/screen-webcam-recording">站点原文</a></h2>
<p>Let's explore how far browser technology has come in the way of screen sharing and recording, and attempt to create a tool that would allow us to quickly record a short video of our screen or webcam. All of this is powered by browser APIs using no external services.</p>
<h3>Capturing the screen</h3>
<p>To allow the user to capture their screen, we can use the <code>MediaDevices.getDisplayMedia()</code> method available to us on the <code>navigator</code> global in modern browsers.</p>
<pre><code class="lang-js">const captureScreen = async () =&gt; {
  // will prompt the user for a screen to share
  const screenShareStream = await navigator.mediaDevices.getDisplayMedia({
    video: true,
  });
};
</code></pre>
<p>Now we have reference to a <code>MediaStream</code> instance returned from <code>getDisplayMedia</code>, and we’ll eventually want to use this stream to draw the captured display onto our canvas. However, the canvas API does not handle any sort of video decoding – so we’ll pass this media stream into an HTML5 <code>&lt;video&gt;</code> element as its <code>srcObject</code> property.</p>
<pre><code class="lang-js">let screenShareVideoRef: HTMLVideoElement;

const captureScreen = async () =&gt; {
  const screenShareStream = await navigator.mediaDevices.getDisplayMedia({
    video: true,
  });

  screenShareVideoRef.srcObject = screenShareStream;
}
</code></pre>
<blockquote><p><code>srcObject</code> is used to assign a a live media stream (like from a webcam or screen share) to a media element. You can convert a stream to a <code>src</code> only if you record the stream into a Blob, and then use <code>URL.createObjectURL</code>. But for live video, always use <code>srcObject</code>.</p>
<p><code>src</code> used with URLs: File URLs, Remote URLs, Blob URLs</p>
</blockquote>
<h3>Capturing the webcam</h3>
<p>We want to allow the user to select which webcam they’d like to use, but in order to do this we need to show the user which webcams are available to choose from. In order to show the user which webcams are available, we first need to request permission to access the list of available webcams.</p>
<pre><code class="lang-js">// list webcam devices
const listWebcamDevices = async () =&gt; {
  // first ensure user's given permission to "enumerate" their video devices
  await navigator.mediaDevices.getUserMedia({
    video: true,
    audio: false
  });

  // fetch all devices, and then filter for videoinput devices
  const allDevices = await navigator.mediaDevices.enumerateDevices();
  return allDevices.filter((device) =&gt; device.kind === "videoinput");
}

// connect to a specific device by passing `device.deviceId` from the device list
const connectToWebcam = async (deviceId: string) =&gt; {
  const webcamStream = await navigator.mediaDevices.getUserMedia({
    video: {
      deviceId: { exact: deviceId }
    }
  });
}
</code></pre>
<blockquote><p><code>getUserMedia()</code> is a powerful feature which can only be used in secure contexts; in insecure contexts, <code>navigator.mediaDevices</code> is undefined. A secure context is a page loaded using HTTPS or the <code>file://</code> URL scheme, or a page loaded from <code>localhost</code>.</p>
</blockquote>
<p>Similar to our screen share stream, we want to use the result of this video stream in our canvas, so we’ll need to pass this stream along to a <code>&lt;video&gt;</code> element for decoding for later use in our canvas drawing.</p>
<pre><code class="lang-js">let webcamVideoRef: HTMLVideoElement;

const connectToWebcam = async (deviceId: string) =&gt; {
  const webcamStream = await navigator.mediaDevices.getUserMedia(/* ... */);
  webcamVideoRef.srcObject = webcamStream;
}
</code></pre>
<h3>Capturing the mic</h3>
<p>The microphone is another type of “user media”, so we can follow a similar pattern as with capturing the webcam.</p>
<pre><code class="lang-js">// list audio input devices
const listAudioInputDevices = async () =&gt; {
  await navigator.mediaDevices.getUserMedia({
    video: false,
    audio: true
  });

  // filter for audioinput devices
  const allDevices = await navigator.mediaDevices.enumerateDevices();
  return allDevices.filter((device) =&gt; device.kind === "audioinput");
}

// connect to a specific device by passing `device.deviceId` from the device list.
const connectToAudioInput = async (deviceId: string) =&gt; {
  const micStream = await navigator.mediaDevices.getUserMedia({
    audio: {
      deviceId: { exact: deviceId },
      // we'll use some browser niceties to polish our audio input
      echoCancellation: true,
      noiseSuppression: true,
      autoGainControl: true,
    },
  });
}
</code></pre>
<p>Since our mic stream does not produce any sort of visuals, we do not need to attach this media stream to any sort of <code>&lt;video&gt;</code> or <code>&lt;audio&gt;</code> element.</p>
<h3>Combining the screen and webcam streams</h3>
<p>At this point we’ve got video streams for our screen share and webcam. We want to combine these two video streams together in a way that we can record the output and add nice visual effects. To do this, we’ll use HTML canvas. We’ll use the <code>drawImage</code> method of our canvas context to paint our video streams onto our canvas. Our general approach will be to set up a <code>requestAnimationFrame</code> loop for painting our video displays onto our canvas so that our canvas stays up to date with our video streams.</p>
<pre><code class="lang-js">const canvas = document.querySelector("canvas");
const ctx = canvas.getContext("2d");

const startDrawing = () =&gt; {
  requestAnimationFrame(loop);
};

const loop = () =&gt; {
  draw();
  requestAnimationFrame(loop);
};

const draw = () =&gt; {
  const { width, height } = canvas;

  // clear out the entire canvas and paint from scratch
  ctx.clearRect(0, 0, width, height);

  // draw our screen share in top-left
  // would need to do real math to get proper aspect ratio.
  ctx.drawImage(screenShareVideoRef, 0, 0, 500, 400);

  // draw our webcam in bottom-right
  // would need to do real math to get proper aspect ratio.
  ctx.drawImage(webcamVideoRef, width - 200, height - 100, 200, 100);
};
</code></pre>
<blockquote><p>The <code>CanvasRenderingContext2D.drawImage()</code> method of the Canvas 2D API provides different ways to draw an image onto the canvas. The specification permits any canvas image source, specifically, an <code>HTMLImageElement</code>, an <code>SVGImageElement</code>, an <code>HTMLVideoElement</code> or an <code>HTMLCanvasElement</code>.</p>
</blockquote>
<h3>Creating a Recording</h3>
<p>Once we have our pixels dancing on our canvas, and our microphone audio stream captured, we can start to stitch these together to create an actual recording. Something we could upload to, say, YouTube.</p>
<p>Modern browsers have some support for the <code>MediaRecorder</code> API which is an interface for recording media. The <code>MediaRecorder</code> API works by consuming a single <code>MediaStream</code> and outputting <code>Blob</code> chunks over time. We can then use those <code>Blob</code> chunks to create a video output.</p>
<p>First we need to generate a <code>MediaStream</code> from our canvas element, which we can do via the <code>canvas.captureStream</code> method, and then combine that with our mic <code>MediaStream</code> to create a single combined media stream. To combine <code>MediaStream</code> instances, we extract out the <code>MediaStreamTrack</code> instances we want to combine together and pass them in an array to the constructor of <code>MediaStream</code> to create a new stream with those specific tracks.</p>
<pre><code class="lang-js">// create a MediaStream from our canvas
// the `30` here is frames per second, feel free to set your own FPS
const canvasStream = canvas.captureStream(30);

// combine the canvas stream and mic stream by collecting tracks from each
const combinedStream = new MediaStream([
  ...canvasStream.getTracks(),
  ...micStream.getTracks(),
]);
</code></pre>
<p>Now that we have a combined media stream, we can use the <code>MediaRecorder</code> API to record it. The general flow for using the <code>MediaRecorder</code> is roughly as follows:</p>
<ol>
<li>create a <code>MediaRecorder</code> instance;</li>
<li>register a callback to the <code>MediaRecorder.ondataavailable</code> event to capture emitted <code>Blobs</code> and stored those <code>Blob</code> chunks in an array;</li>
<li>when the recorder’s <code>onstop</code> event is called, use the collected <code>Blob</code> chunks to create a video file to be downloaded. You can call <code>MediaRecorder.stop</code> manually to stop a recording.</li>
</ol>
<pre><code class="lang-js">const chunks: Blob[] = [];

// create a recorder
const recorder = new MediaRecorder(combinedStream, {
  // requested media type, basically limited to webm
  mimeType: "video/webm;codecs=vp9"
});

// collect blobs when available
recorder.ondataavailable = (evt) =&gt; {
  chunks.push(evt.data);
}

// when recorder stops (via recorder.stop()), handle blobs
recorder.onstop = () =&gt; {
  const recordedBlob = new Blob(chunks, { type: chunks[0].type });
  // do something with this blob...
}
</code></pre>
<p>The <code>MediaRecorder</code> <code>mimeType</code> option supposedly allows you to specify a media type, but Chromium-based browsers and FireFox <a href="https://source.chromium.org/chromium/chromium/src/+/main:third_party/blink/web_tests/fast/mediarecorder/MediaRecorder-isTypeSupported.html?q=MediaRecorder-isTypeSupported&amp;ss=chromium">only seem to support webm</a>. This means you’re more-or-less forced into creating <code>.webm</code> videos – a video format that’s not as widely adopted as other formats like <code>.mp4</code>. Many video editing softwares do not handle <code>.webm</code> videos, therefore it’s a struggle to do any sort of post-processing or editing on the generated video files.</p>
<blockquote><p>WebM is an open media file format designed for the web. It is an open-source project sponsored by Google. WebM files consist of video streams compressed with the VP8 or VP9 video codec, audio streams compressed with the Vorbis or Opus audio codecs, and WebVTT text tracks. Read more at: <a href="https://www.webmproject.org/about/faq">https://www.webmproject.org/about/faq</a></p>
</blockquote>
<p>The <code>.webm</code> video blobs generated from the <code>MediaRecorder</code> API in Chromium-based browsers are missing the <code>duration</code> video metadata – which means browsers or video players cannot properly seek these videos because they don’t know how long the video is, and many platforms will reject them as uploads. Open source to the rescue. <a href="https://github.com/yusitnikov/fix-webm-duration">fix-webm-duration</a> is a library that allows you to manually pass in a video duration and it’ll adjust the blob’s metadata accordingly. Therefore, to get a usable video file – you’ll need to manually track the start/end time of the video recording – and then monkey patch the duration metadata accordingly.</p>
<pre><code class="lang-js">import fixWebmDuration from "fix-webm-duration";

// helper to patch our blob
const patchBlob = (blob: Blob, duration: number): Promise&lt;Blob&gt; =&gt; {
  return new Promise(resolve =&gt; {
    fixWebmDuration(blob, duration, newBlob =&gt; resolve(newBlob));
  });
}

// when starting the recording, track the start time
let startTime: number;
recorder.onstart = () =&gt; {
  startTime = performance.now();
}

recorder.onstop = async () =&gt; {
  const recordedBlob = new Blob(chunks, { type: chunks[0].type });

  // manually compute duration, and patch the blob
  const duration = performance.now() - startTime;
  const patchedBlob = patchBlob(recordedBlob, duration);
}
</code></pre>
<h3>Generating a download</h3>
<p>To automatically download the blob as a video file, we’ll use the standard technique of using <code>URL.createObjectURL</code> to create an object URL, generating an anchor DOM element with this URL as its <code>href</code>, simulating a click on that anchor tag (which will trigger a download of the blob), and then discard the anchor tag.</p>
<pre><code class="lang-js">recorder.onstop = async () =&gt; {
  const recordedBlob = new Blob(chunks, { type: chunks[0].type });
  const duration = performance.now() - startTime;
  const patchedBlob = patchBlob(recordedBlob, duration);

  // turn the blob into a data URL
  const data = URL.createObjectURL(patchedBlob);

  // generate a link, simulate a click on it
  const link = document.createElement("a");
  link.href = data;
  link.download = "recording.webm";
  link.dispatchEvent(
    new MouseEvent("click", { view: window });
  );

  // don't forget to clean up
  setTimeout(() =&gt; {
    URL.revokeObjectURL(data);
    link.remove();
  }, 500);
}
</code></pre>
<p>And there we go! After our recording finishes, we’ve got a <code>.webm</code> video file downloading to our Downloads folder.</p>
<h3>MDN docs and links</h3>
<ul>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getDisplayMedia">https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getDisplayMedia</a></li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia">https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia</a></li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/enumerateDevices">https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/enumerateDevices</a></li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement/srcObject">https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement/srcObject</a></li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/drawImage">https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/drawImage</a></li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaRecorder">https://developer.mozilla.org/en-US/docs/Web/API/MediaRecorder</a></li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaStream">https://developer.mozilla.org/en-US/docs/Web/API/MediaStream</a></li>
<li><a href="https://www.mux.com/blog">https://www.mux.com/blog</a></li>
<li><a href="https://github.com/alyssaxuu/screenity">https://github.com/alyssaxuu/screenity</a></li>
<li><a href="https://clips.formidable.dev">https://clips.formidable.dev</a></li>
</ul>
