<h2><a href="https://github.com/kexizeroing/kexizeroing.github.io/blob/master/src/posts/screen-webcam-recording.md">仓库源文</a>，<a href="https://kexizeroing.github.io/screen-webcam-recording">站点原文</a></h2>
<hr/>
<h2>layout: "../layouts/BlogPost.astro"
title: "Screen and webcam recording"
slug: screen-and-webcam-recording
description: ""
added: "Oct 14 2022"
tags: [web]
updatedDate: "Jun 21 2023"</h2>
<p>Let's explore how far browser technology has come in the way of screen sharing and recording, and attempt to create a tool that would allow us to quickly create short-form technical video content. All of this is powered by browser APIs using no external services. The original article is <a href="https://formidable.com/blog/2022/screen-webcam-mixing-recording">here</a>.</p>
<p>This experimental web app is <a href="https://clips.formidable.dev">Clips</a> and it supports:</p>
<ul>
<li>capturing/sharing your screen/window</li>
<li>capturing your webcam</li>
<li>capturing your mic audio</li>
<li>adjusting the position/sizing of the captured screen/webcam on the video canvas</li>
<li>choosing from different background/layout options, including audio-visualization background</li>
<li>recording the video in your browser</li>
</ul>
<h3>Capturing the screen</h3>
<p>To allow the user to capture their screen, we can use the <code>MediaDevices.getDisplayMedia()</code> method available to us on the <code>navigator</code> global in modern browsers.</p>
<pre><code class="language-js">const captureScreen = async () =&gt; {
  // will prompt the user for a screen to share
  const screenShareStream = await navigator.mediaDevices.getDisplayMedia({
    video: true,
  });
}
</code></pre>
<p>Now we have reference to a <code>MediaStream</code> instance returned from <code>getDisplayMedia</code>, and we’ll eventually want to use this stream to draw the captured display onto our canvas. However, the canvas API does not handle any sort of video decoding – so we’ll pass this media stream into an HTML5 <code>&lt;video&gt;</code> element as its <code>srcObject</code>, and then use a reference to this <code>&lt;video&gt;</code> element to draw the video’s pixels onto our canvas via <code>CanvasRenderingContext2D.drawImage</code>.</p>
<pre><code class="language-js">let screenShareVideoRef: HTMLVideoElement;

const captureScreen = async () =&gt; {
  const screenShareStream = await navigator.mediaDevices.getDisplayMedia({
    video: true,
  });

  // connect stream to video element via srcObject
  screenShareVideoRef.srcObject = screenShareStream;
}
</code></pre>
<h3>Capturing the webcam</h3>
<p>We want to allow the user to select which webcam they’d like to use, but in order to do this we need to show the user which webcams are available to choose from. In order to show the user which webcams are available, we first need to request permission to access the list of available webcams.</p>
<pre><code class="language-js">// list webcam devices
const listWebcamDevices = async () =&gt; {
  // first ensure user's given permission to "enumerate" their video devices
  await navigator.mediaDevices.getUserMedia({
    video: true,
    audio: false
  });

  // fetch all devices, and then filter for videoinput devices
  const allDevices = await navigator.mediaDevices.enumerateDevices();
  return allDevices.filter((device) =&gt; device.kind === "videoinput");
}

// connect to a specific device by passing `device.deviceId` from the device list
const connectToWebcam = async (deviceId: string) =&gt; {
  const webcamStream = await navigator.mediaDevices.getUserMedia({
    video: {
      deviceId: { exact: deviceId }
    }
  });
}
</code></pre>
<p>Similar to our screen share stream, we want to use the result of this video stream in our canvas, so we’ll need to pass this stream along to a <code>&lt;video&gt;</code> element for decoding for later use in our canvas drawing.</p>
<pre><code class="language-js">let webcamVideoRef: HTMLVideoElement;

const connectToWebcam = async (deviceId: string) =&gt; {
  const webcamStream = await navigator.mediaDevices.getUserMedia(/* ... */);
  // use input stream as src for video element
  webcamVideoRef.srcObject = webcamStream;
}
</code></pre>
<h3>Capturing the mic</h3>
<p>The microphone is another type of “user media”, so we can follow a similar pattern as with capturing the webcam.</p>
<pre><code class="language-js">// list audio input devices
const listAudioInputDevices = async () =&gt; {
  await navigator.mediaDevices.getUserMedia({
    video: false,
    audio: true
  });

  // filter for audioinput devices
  const allDevices = await navigator.mediaDevices.enumerateDevices();
  return allDevices.filter((device) =&gt; device.kind === "audioinput");
}

// connect to a specific device by passing `device.deviceId` from the device list.
const connectToAudioInput = async (deviceId: string) =&gt; {
  const micStream = await navigator.mediaDevices.getUserMedia({
    audio: {
      deviceId: { exact: deviceId },
      // we'll use some browser niceties to polish our audio input
      echoCancellation: true,
      noiseSuppression: true,
      autoGainControl: true,
    },
  });
}
</code></pre>
<p>Since our mic stream does not produce any sort of visuals, we do not need to attach this media stream to any sort of <code>&lt;video&gt;</code> or <code>&lt;audio&gt;</code> element.</p>
<h3>Combining the screen and webcam streams</h3>
<p>At this point we’ve got video streams for our screen share and webcam. We want to combine these two video streams together in a way that we can record the output and add nice visual effects. To do this, we’ll use HTML canvas and <code>CanvasRenderingContext2D</code>. We’ll use the <code>drawImage</code> method of our canvas context to paint our video streams onto our canvas. Our general approach will be to set up a <code>requestAnimationFrame</code> loop for painting our video displays onto our canvas so that our canvas stays up to date with our video streams.</p>
<pre><code class="language-js">const canvas = document.querySelector("canvas");
const ctx = canvas.getContext("2d");

// kick off the drawing process 
const startDrawing = () =&gt; {
  requestAnimationFrame(loop);
}

// requestAnimationFrame loop. Each frame, we draw to the canvas.
const loop = () =&gt; {
  draw();	
  requestAnimationFrame(loop);
}

// our drawing function
const draw = () =&gt; {
  const { width, height } = canvas;

  // clear out the entire canvas and paint from scratch
  ctx.clearRect(0, 0, width, height);

  // draw our screen share in top-left
  // would need to do real math to get proper aspect ratio.
  ctx.drawImage(screenShareVideoRef, 0, 0, 500, 400);

  // draw our webcam in bottom-right
  // would need to do real math to get proper aspect ratio.
  ctx.drawImage(webcamVideoRef, width - 200, height - 100, 200, 100);
}
</code></pre>
<blockquote>
<p>The <code>CanvasRenderingContext2D.drawImage()</code> method of the Canvas 2D API provides different ways to draw an image onto the canvas. The specification permits any canvas image source, specifically, an <code>HTMLImageElement</code>, an <code>SVGImageElement</code>, an <code>HTMLVideoElement</code> or an <code>HTMLCanvasElement</code>.</p>
</blockquote>
<h3>Creating a Recording</h3>
<p>Once we have our pixels dancing on our canvas, and our microphone audio stream captured, we can start to stitch these together to create an actual recording. Something we could upload to, say, YouTube.</p>
<p>Modern browsers have some support for the <code>MediaRecorder</code> API which is an interface for recording media. The <code>MediaRecorder</code> API works by consuming a single <code>MediaStream</code> and outputting <code>Blob</code> chunks over time. We can then use those <code>Blob</code> chunks to create a video output.</p>
<p>We need to generate a <code>MediaStream</code> from our canvas element, which we can do via the <code>canvas.captureStream</code> method – and then combine that with our mic <code>MediaStream</code> to create a single combined media stream. To combine <code>MediaStream</code> instances, we extract out the <code>MediaStreamTrack</code> instances we want to combine together and pass them in an array to the constructor of <code>MediaStream</code> to create a new stream with those specific tracks.</p>
<pre><code class="language-js">// create a MediaStream from our canvas
// the `30` here is frames per second, feel free to set your own FPS
const canvasStream = canvas.captureStream(30);

// combine the canvas stream and mic stream by collecting tracks from each
const combinedStream = new MediaStream([
  ...canvasStream.getTracks(),
  ...micStream.getTracks()
]);
</code></pre>
<p>Now that we have a combined media stream, we can use the <code>MediaRecorder</code> API to record it. The general flow for using the <code>MediaRecorder</code> is roughly as follows:</p>
<ol>
<li>create a <code>MediaRecorder</code> instance;</li>
<li>register a callback to the <code>MediaRecorder.ondataavailable</code> event to capture emitted <code>Blobs</code> and stored those <code>Blob</code> chunks in an array;</li>
<li>when the recorder’s <code>onstop</code> event is called, use the collected <code>Blob</code> chunks to create a video file to be downloaded. You can call <code>MediaRecorder.stop</code> manually to stop a recording.</li>
</ol>
<pre><code class="language-js">const chunks: Blob[] = [];

// create a recorder
const recorder = new MediaRecorder(combinedStream, {
  // requested media type, basically limited to webm
  mimeType: "video/webm;codecs=vp9"
});

// collect blobs when available
recorder.ondataavailable = (evt) =&gt; {
  chunks.push(evt.data);
}

// when recorder stops (via recorder.stop()), handle blobs
recorder.onstop = () =&gt; {
  const recordedBlob = new Blob(chunks, { type: chunks[0].type });
  // do something with this blob...
}
</code></pre>
<p>The <code>MediaRecorder</code> <code>mimeType</code> option supposedly allows you to specify a media type, but Chromium-based browsers and FireFox <a href="https://source.chromium.org/chromium/chromium/src/+/main:third_party/blink/web_tests/fast/mediarecorder/MediaRecorder-isTypeSupported.html?q=MediaRecorder-isTypeSupported&amp;ss=chromium">only seem to support webm</a>. This means you’re more-or-less forced into creating <code>.webm</code> videos – a video format that’s not as widely adopted as other formats like <code>.mp4</code>. Many video editing softwares do not handle <code>.webm</code> videos, therefore it’s a struggle to do any sort of post-processing or editing on the generated video files.</p>
<p>The <code>.webm</code> video blobs generated from the <code>MediaRecorder</code> API in Chromium-based browsers are missing the <code>duration</code> video metadata – which means browsers or video players cannot properly seek these videos because they don’t know how long the video is, and many platforms will reject them as uploads. Open source to the rescue. <a href="https://github.com/yusitnikov/fix-webm-duration">fix-webm-duration</a> is a library that allows you to manually pass in a video duration and it’ll adjust the blob’s metadata accordingly. Therefore, to get a usable video file – you’ll need to manually track the start/end time of the video recording – and then monkey patch the duration metadata accordingly.</p>
<pre><code class="language-js">import fixWebmDuration from "fix-webm-duration";

// helper to patch our blob
const patchBlob = (blob: Blob, duration: number): Promise&lt;Blob&gt; =&gt; {
  return new Promise(resolve =&gt; {
    fixWebmDuration(blob, duration, newBlob =&gt; resolve(newBlob));
  });
}

// when starting the recording, track the start time
let startTime: number;
recorder.onstart = () =&gt; {
  startTime = performance.now();
}

recorder.onstop = async () =&gt; {
  const recordedBlob = new Blob(chunks, { type: chunks[0].type });
  
  // manually compute duration, and patch the blob
  const duration = performance.now() - startTime;
  const patchedBlob = patchBlob(recordedBlob, duration);
}
</code></pre>
<blockquote>
<p>WebM is an open media file format designed for the web. It is an open-source project sponsored by Google. WebM files consist of video streams compressed with the VP8 or VP9 video codec, audio streams compressed with the Vorbis or Opus audio codecs, and WebVTT text tracks. Read more at: https://www.webmproject.org/about/faq</p>
<p>VP8 and VP9 are highly-efficient video compression technologies developed by the WebM Project; Vorbis and Opus are open-source audio compression technologies.</p>
</blockquote>
<h3>Generating a download</h3>
<p>To automatically download the blob as a video file, we’ll use the standard technique of using <code>URL.createObjectURL</code> to create an object URL, generating an anchor DOM element with this URL as its <code>href</code>, simulating a click on that anchor tag (which will trigger a download of the blob), and then discard the anchor tag.</p>
<pre><code class="language-js">recorder.onstop = async () =&gt; {
  const recordedBlob = new Blob(chunks, { type: chunks[0].type });
  const duration = performance.now() - startTime;
  const patchedBlob = patchBlob(recordedBlob, duration);

  // turn the blob into a data URL
  const data = URL.createObjectURL(patchedBlob);
  
  // generate a link, simulate a click on it
  const link = document.createElement("a");
  link.href = data;
  link.download = "recording.webm";
  link.dispatchEvent(
    new MouseEvent("click", { view: window });
  );

  // don't forget to clean up
  setTimeout(() =&gt; {
    URL.revokeObjectURL(data);
    link.remove();
  }, 500);
}
</code></pre>
<p>And there we go! After our recording finishes, we’ve got a <code>.webm</code> video file downloading to our Downloads folder.</p>
<h3>MDN docs and links</h3>
<ul>
<li>https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getDisplayMedia</li>
<li>https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia</li>
<li>https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/enumerateDevices</li>
<li>https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement/srcObject</li>
<li>https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/drawImage</li>
<li>https://developer.mozilla.org/en-US/docs/Web/API/MediaRecorder</li>
<li>https://developer.mozilla.org/en-US/docs/Web/API/MediaStream</li>
<li>https://www.mux.com/blog</li>
<li>https://github.com/alyssaxuu/screenity</li>
<li>https://blog.tomayac.com/2020/05/15/the-requestvideoframecallback-api</li>
</ul>
<h3>FFMPEG.WASM</h3>
<p>WebAssembly enables developers to bring new performant functionality to the web from other languages. <a href="https://ffmpegwasm.netlify.app">FFmpeg.wasm</a> (WebAssembly / JavaScript port of FFmpeg) is one of a showcasing of the <a href="https://web.dev/wasm-libraries/">new functionality</a> being made available thanks to WebAssembly. It enables video &amp; audio record, convert and stream right inside browsers. Check browser examples at https://github.com/ffmpegwasm/ffmpeg.wasm/tree/master/examples/browser</p>
<p>There are two components inside <code>ffmpeg.wasm</code>: <code>@ffmpeg/ffmpeg</code> and <code>@ffmpeg/core</code>. <code>@ffmpeg/ffmpeg</code> contains kind of a wrapper to handle the complexity of loading core and calling low-level APIs. <code>@ffmpeg/core</code> contains WebAssembly code which is transpiled from original FFmpeg C code with minor modifications.</p>
<pre><code class="language-js">// AVI to MP4 Demo
import { writeFile } from 'fs/promises';
import { createFFmpeg, fetchFile } from '@ffmpeg/ffmpeg';

const ffmpeg = createFFmpeg({ log: true });

(async () =&gt; {
  await ffmpeg.load();
  ffmpeg.FS('writeFile', 'test.avi', await fetchFile('./test.avi'));
  await ffmpeg.run('-i', 'test.avi', 'test.mp4');
  await fs.promises.writeFile('./test.mp4', ffmpeg.FS('readFile', 'test.mp4'));
  process.exit(0);
})();
</code></pre>
<p><a href="https://ffmpeg-online.vercel.app">ffmpeg-online</a> is an online version of ffmpeg based on <code>ffmpeg.wasm</code>, which can be used to process audio and video online. The most straightforward example <code>ffmpeg -i input.mp4 output.avi</code> is used to convert an input media file to a different format.</p>
<p><a href="https://github.com/amiaopensource/ffmprovisr">ffmprovisr</a> is a repository of useful FFmpeg commands. A list of all <a href="https://github.com/amiaopensource/ffmprovisr/blob/gh-pages/recipes.txt">recipes</a> is provided as well.</p>
<p><a href="https://github.com/Tameyer41/liftoff">Mock Interview Simulator</a> is an AI-powered interview prep app to practice your tech interview skills. Under the hood, it uses FFmpeg to transcode the raw video into MP3. Then we send the audio directly to be transcribed by OpenAI's Whisper endpoint, and then stream feedback from the edge using OpenAI's gpt-3.5-turbo.</p>
