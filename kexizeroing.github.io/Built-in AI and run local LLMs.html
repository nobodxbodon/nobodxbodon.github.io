<h2><a href="https://github.com/kexizeroing/kexizeroing.github.io/blob/master/src/blog/built-in-ai-and-run-local-llms.md">仓库源文</a>，<a href="https://kexizeroing.github.io/built-in-ai-and-run-local-llms">站点原文</a></h2>
<h2>Chrome built-in AI</h2>
<p>With built-in AI, your website or web application can perform AI-powered tasks, without needing to deploy, manage, or self-host AI models.</p>
<ul>
<li>Local processing of sensitive data</li>
<li>Snappy user experience</li>
<li>Greater access to AI</li>
<li>Offline AI usage</li>
</ul>
<p>You'll access built-in AI capabilities primarily with task APIs, such as a translation API or a summarization API. Task APIs are designed to run inference against the best model for the assignment. In Chrome, these APIs are built to run inference against Gemini Nano with fine-tuning or an expert model.</p>
<ul>
<li><a href="https://github.com/webmachinelearning/prompt-api">https://github.com/webmachinelearning/prompt-api</a></li>
<li><a href="https://github.com/webmachinelearning/writing-assistance-apis">https://github.com/webmachinelearning/writing-assistance-apis</a></li>
<li><a href="https://github.com/webmachinelearning/translation-api">https://github.com/webmachinelearning/translation-api</a></li>
<li><a href="https://chrome.dev/web-ai-demos">https://chrome.dev/web-ai-demos</a></li>
</ul>
<h3>Get started</h3>
<p>The Prompt API, Summarizer API, Writer API, and Rewriter API download Gemini Nano, which is designed to run locally on desktop and laptop computers. These APIs don't work on mobile devices.</p>
<p>API status:
<a href="https://developer.chrome.com/docs/ai/built-in-apis#api_status">https://developer.chrome.com/docs/ai/built-in-apis#api_status</a></p>
<p>There are several built-in AI APIs available at different stages of development. Some are in Chrome stable, others are available to all developers in origin trials, and some are only available to Early Preview Program (EPP) participants.</p>
<table>
<thead><tr>
<th>API</th>
<th>Web</th>
<th>Extensions</th>
</tr>
</thead>
<tbody>
<tr>
<td>Translator API</td>
<td>Chrome 138</td>
<td>Chrome 138</td>
</tr>
<tr>
<td>Language Detector API</td>
<td>Chrome 138</td>
<td>Chrome 138</td>
</tr>
<tr>
<td>Summarizer API</td>
<td>Chrome 138</td>
<td>Chrome 138</td>
</tr>
<tr>
<td>Writer API</td>
<td>Origin trial</td>
<td>Origin trial</td>
</tr>
<tr>
<td>Rewriter API</td>
<td>Origin trial</td>
<td>Origin trial</td>
</tr>
<tr>
<td>Prompt API</td>
<td>In EPP</td>
<td>Chrome 138</td>
</tr>
</tbody>
</table>
<p>To confirm Gemini Nano has downloaded and works as intended, open DevTools and type <code>await LanguageModel.availability();</code> into the console. This should return <code>available</code>.</p>
<blockquote><p>If Gemini Nano doesn't work as expect, follow these steps:</p>
<ol>
<li>Restart Chrome.</li>
<li>Go to <code>chrome://components</code></li>
<li>Confirm that <strong>Optimization Guide On Device Model</strong> is present. This means Gemini Nano is either available or downloading. If there's no version number listed, click <strong>Check for update</strong> to force the download.</li>
</ol>
</blockquote>
<p>To trigger the model download and create the language model session, call the asynchronous <code>LanguageModel.create()</code> function. If the response to <code>availability()</code> was <code>'downloadable'</code>, it's best practice to listen for download progress. This way, you can inform the user in case the download takes time.</p>
<pre><code class="lang-js">const session = await LanguageModel.create({
  monitor(m) {
    m.addEventListener('downloadprogress', (e) =&gt; {
      console.log(`Downloaded ${e.loaded * 100}%`);
    });
  },
});
</code></pre>
<h3>API usage</h3>
<p>Before translating text from one language to another, you must first determine what language is used in the given text.</p>
<pre><code class="lang-js">if ('LanguageDetector' in self) {
  // The Language Detector API is available.
}

const languageDetector = await LanguageDetector.create();
const result = await languageDetector.detect("I’m a software engineer and I have a great interest in web development.");

console.log(result[0]);
// {
//   "confidence": 0.9994762539863586,
//   "detectedLanguage": "en"
// }
</code></pre>
<p>Users can participate in support chats in their first language, and your site can translate it into the language your support agents use, before it leaves the user's device.</p>
<pre><code class="lang-js">if ('Translator' in self) {
  // The Translator API is supported.
}

const translator = await Translator.create({
  sourceLanguage: "en",
  targetLanguage: "de",
  // monitor(m) {
  //   m.addEventListener('downloadprogress', (e) =&gt; {
  //     console.log(`Downloaded ${e.loaded * 100}%`);
  //   });
  // },
});
const result = await translator.translate("I’m a software engineer and I have a great interest in web development.");

console.log(result);
// '我是一名软件工程师，我对 Web 开发非常感兴趣。'
</code></pre>
<p>The Summarizer API can be used to generate different types of summaries in varied lengths and formats, such as sentences, paragraphs, bullet point lists, and more.</p>
<pre><code class="lang-js">if ('Summarizer' in self) {
  // The Summarizer API is supported.
}

const options = {
  sharedContext: 'This is a user’s profile.', // Additional shared context that can help the summarizer.
  type: 'key-points', // key-points (default), tldr, teaser, and headline
  format: 'markdown', // markdown (default) and plain-text
  length: 'medium',  // short, medium (default), and long
};

const summarizer = await Summarizer.create(options);
const result = await summarizer.summarize("I’m a software engineer and I have a great interest in web development. I love building web applications and exploring new technologies. My goal is to create user-friendly and efficient software solutions.");

console.log(result);
// * The user is a software engineer with an interest in web development. 
// * Their goal is to create user-friendly and efficient software solutions. 
// * They enjoy building web applications and exploring new technologies.
// * The text provides a brief introduction to the user's professional background and interests. 
// * The profile lacks further details such as specific projects or accomplishments.
</code></pre>
<p>With the Prompt API, you can send natural language requests to Gemini Nano in the browser.</p>
<pre><code class="lang-js">await LanguageModel.params();
// {defaultTopK: 3, maxTopK: 8, defaultTemperature: 1, maxTemperature: 2}

const session = await LanguageModel.create();

// Prompt the model and wait for the whole result to come back.
const result = await session.prompt("Write me a poem.");
console.log(result);

// Prompt the model and stream the result:
const stream = session.promptStreaming("Write me a poem.");
for await (const chunk of stream) {
  console.log(chunk);
}
</code></pre>
<h2>Run open-source LLMs locally on your computer</h2>
<ol>
<li><p>Ollama + OpenWebUI</p>
<ul>
<li><a href="https://github.com/kexiZeroing/langchain-llamaindex-ollama">https://github.com/kexiZeroing/langchain-llamaindex-ollama</a></li>
<li><a href="https://github.com/ollama/ollama-js">https://github.com/ollama/ollama-js</a></li>
<li><a href="https://github.com/open-webui/open-webui">https://github.com/open-webui/open-webui</a></li>
<li><a href="https://simonwillison.net/2024/Dec/27/open-webui">https://simonwillison.net/2024/Dec/27/open-webui</a></li>
</ul>
</li>
<li><p>LM Studio</p>
<ul>
<li><a href="https://lmstudio.ai">https://lmstudio.ai</a></li>
<li><a href="https://huggingface.co/blog/yagilb/lms-hf">https://huggingface.co/blog/yagilb/lms-hf</a></li>
</ul>
</li>
<li><p>GPT4All</p>
<ul>
<li><a href="https://github.com/nomic-ai/gpt4all">https://github.com/nomic-ai/gpt4all</a></li>
</ul>
</li>
<li><p>Transformers.js uses ONNX Runtime to run models in the browser</p>
<ul>
<li><a href="https://huggingface.co/docs/transformers.js/index">https://huggingface.co/docs/transformers.js/index</a></li>
<li><a href="https://huggingface.co/onnx-community">https://huggingface.co/onnx-community</a></li>
</ul>
</li>
</ol>
<pre><code class="lang-js">import { pipeline } from "@huggingface/transformers";
const segmenter = await pipeline(
  "background-removal",       // Task
  "onnx-community/BEN2-ONNX", // Custom model
);
const result = await segmenter("input.png");
</code></pre>
<p>You can run any GGUF (GPT-Generated Unified Format), a binary format that is optimized for quick loading and saving of models, on the Hugging Face Hub directly with ollama. <em>(Tools like Ollama and LM Studio abstract away much of the complexity, often relying on GGUF models behind the scenes.)</em> All you need to do is:</p>
<pre><code class="lang-sh"># https://huggingface.co/docs/hub/en/ollama
ollama run hf.co/{username}/{repository}

# run the Llama 3.2 1B
ollama run hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:latest
</code></pre>
<blockquote><p>Mirror site of huggingface.co in China</p>
<ul>
<li><a href="https://hf-mirror.com">https://hf-mirror.com</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/663712983">https://zhuanlan.zhihu.com/p/663712983</a></li>
</ul>
</blockquote>
