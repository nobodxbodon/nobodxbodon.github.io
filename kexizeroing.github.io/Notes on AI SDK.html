<h2><a href="https://github.com/kexizeroing/kexizeroing.github.io/blob/master/src/blog/notes-on-ai-sdk.md">仓库源文</a>，<a href="https://kexizeroing.github.io/notes-on-ai-sdk">站点原文</a></h2>
<p>This is my learning notes from the <a href="https://www.youtube.com/watch?v=kDlqpN1JyIw">AI Engineer workshop</a> tutorial by Nico Albanese.</p>
<blockquote><p>AI SDK is like an ORM for LLMs. It provides a simple interface to interact with different LLM providers, making it easy to switch between them without changing your code.</p>
<p>Here is an Gemini AI SDK Cheatsheet: <a href="https://patloeber.com/gemini-ai-sdk-cheatsheet">https://patloeber.com/gemini-ai-sdk-cheatsheet</a>. It has code snippets to start building with Gemini and the AI SDK.</p>
</blockquote>
<h2>Generate Text</h2>
<p>Make your first LLM call.</p>
<pre><code class="lang-js">import { openai } from '@ai-sdk/openai'
import { generateText } from 'ai'
import 'dotenv/config'

const main = async () =&gt; {
  const result = await generateText({
    model: openai('gpt-4o-mini'),
    prompt: 'Hello, world!',
  })
  console.log(result.text)
}

main()
</code></pre>
<p><code>generateText</code> can take either <code>prompt</code> or <code>messages</code> as input.</p>
<pre><code class="lang-js">const result = await generateText({
  model: openai('gpt-4o-mini'),
  messages: [
    { role: 'user', content: 'Hello, world!' },
  ],
})
</code></pre>
<p>Changing providers with the AI SDK is as simple as changing two lines of code. We can pick a model that has web search built in, like <code>perplexity</code> or <code>gemini</code>, and we can even see what sources were used to generate the text.</p>
<pre><code class="lang-js">import { google } from '@ai-sdk/google'
import { generateText } from 'ai'
import 'dotenv/config'

const main = async () =&gt; {
  const result = await generateText({
    model: google('gemini-1.5-flash', { useSearchGrounding: true }),
    prompt: 'When is the AI Engineer summit in 2025?',
  })

  console.log(result.text)
  console.log(result.sources)
}

main()
</code></pre>
<h2>Stream Text</h2>
<pre><code class="lang-js">import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

const result = streamText({
  model: openai('gpt-4o'),
  prompt: 'Invent a new holiday and describe its traditions.',
});

for await (const textPart of result.textStream) {
  process.stdout.write(textPart);
}
</code></pre>
<h2>Tools (or Function Calling)</h2>
<p>At the core, we give the model a prompt and also pass a list of tools that available. Each of these tools will be provided with a name, a description so the model knows when to use it, and any data it requires to run.</p>
<pre><code class="lang-js">import { openai } from "@ai-sdk/openai";
import { generateText, tool } from "ai";
import "dotenv/config";
import { z } from "zod";

const main = async () =&gt; {
  const result = await generateText({
    model: openai("gpt-4o"),
    prompt: "What's 10 + 5?",
    tools: {
      addNumbers: tool({
        description: "Add two numbers together",
        parameters: z.object({
          num1: z.number(),
          num2: z.number(),
        }),
        execute: async ({ num1, num2 }) =&gt; {
          return num1 + num2;
        },
      }),
    },
  });
  console.log(result.toolResults);
  // [
  //   {
  //     type: 'tool-result',
  //     toolCallId: '...',
  //     toolName: 'addNumbers',
  //     args: { num1: 10, num2: 5},
  //     result: 15,
  //   }
  // ]
};

main();
</code></pre>
<p>Now we only have the tool results and the model hasn't actually answered the question (<code>result.text</code> is empty). How can we get the model incorporate the tool results into a generated text answer?</p>
<p>When <code>maxSteps</code> is set to a number greater than 1 and the model generates a tool call, the AI SDK will trigger a new generation passing in the tool result until there are no further tool calls or the maximum number of tool steps is reached.</p>
<blockquote><p>If you just need the tool's call result, you can directly access it from <code>message.toolInvocations</code> (no need for <code>maxSteps</code>). It's when you need to feed the result of the tool invocation back to LLM for it to interpret and respond that's when you need <code>maxSteps</code>.</p>
</blockquote>
<pre><code class="lang-js">const main = async () =&gt; {
  const result = await generateText({
    model: openai("gpt-4o"),
    prompt: "What's 10 + 5?",
    maxSteps: 2,
    tools: {
      addNumbers: tool({
        description: "Add two numbers together",
        parameters: z.object({
          num1: z.number(),
          num2: z.number(),
        }),
        execute: async ({ num1, num2 }) =&gt; {
          return num1 + num2;
        },
      }),
    },
  });

  console.log(result.text);  // 10 + 5 equals 15.
  console.log(JSON.stringify(result.steps, null, 2));
  // step 1: The model generates a tool call, and the tool is executed.
  // step 2: The tool result is sent to the model, and the model generates a response.
}
</code></pre>
<p>We can have multiple tools over multiple steps.</p>
<pre><code class="lang-js">const main = async () =&gt; {
  const result = await generateText({
    model: openai("gpt-4o"),
    prompt: "Get the weather in SF and NY, then add them together.",
    maxSteps: 3,
    tools: {
      addNumbers: tool({
        description: "Add two numbers together",
        parameters: z.object({
          num1: z.number(),
          num2: z.number(),
        }),
        execute: async ({ num1, num2 }) =&gt; {
          return num1 + num2;
        },
      }),
      getWeather: tool({
        description: "Get the current weather at a location",
        parameters: z.object({
          latitude: z.number(),
          longitude: z.number(),
          city: z.string(),
        }),
        execute: async ({ latitude, longitude, city }) =&gt; {
          const response = await fetch(
            `https://api.open-meteo.com/v1/forecast?latitude=${latitude}&amp;longitude=${longitude}&amp;current=temperature_2m,weathercode,relativehumidity_2m&amp;timezone=auto`,
          );

          const weatherData = await response.json();
          return {
            temperature: weatherData.current.temperature_2m,
            weatherCode: weatherData.current.weathercode,
            humidity: weatherData.current.relativehumidity_2m,
            city,
          };
        },
      }),
    },
  });
  console.log(result.steps.length);
  console.log(result.text);
};

main();
</code></pre>
<p>You may ask we are not providing the <code>latitude</code> and <code>longitude</code>. It is the inference capablity we can use, so we can let the language model infer these parameters from the context of the conversation.</p>
<h2>Structured Output</h2>
<p>There are two ways to generate structured output with the AI SDK. One is using experimental output option with <code>generateText</code>, and the other is using <code>generateObject</code> function.</p>
<pre><code class="lang-js">const main = async () =&gt; {
  const result = await generateText({
    model: openai('gpt-4o'),
    prompt: 'Get the weather in SF and NY, then add them together.',
    maxSteps: 3,
    experimental_output: Output.object({
      schema: z.object({ sum: z.string() }),
    }),
    tools: {
      addNumbers: tool({...}),
      getWeather: tool({...}),
    },
  })

  console.log(result.experimental_output)
  // { sum: 27.5 }
}
</code></pre>
<pre><code class="lang-js">const main = async () =&gt; {
  const result = await generateObject({
    model: openai('gpt-4o-mini'),
    prompt: 'Please come up with 3 definitions for AI agents.',
    schema: z.object({
      definitions: z.array(z.string()),
    }),
  })
  console.log(result.object.definitions)
  // [
  //   'An AI agent is ...',
  //   'An AI agent is ...',
  //   'An AI agent is ...'
  // ]
}
</code></pre>
<p>Furthermore, we can use the <code>describe</code> function to help refine the generation.</p>
<pre><code class="lang-js">const main = async () =&gt; {
  const result = await generateObject({
    model: openai("gpt-4o-mini"),
    prompt: "Please come up with 3 definitions for AI agents.",
    schema: z.object({
      definitions: z.array(z.string().describe("Use as much jargon as possible. It should be completely incoherent.")),
    }),
  })
  console.log(result.object.definitions)
}
</code></pre>
<h2>Deep Research</h2>
<p>The rough steps will be:</p>
<ol>
<li>Take the initial input</li>
<li>Generate search queries</li>
<li>Map through each query and<ul>
<li>Search the web for a relevant result</li>
<li>Analyze the result for learnings and follow-up questions</li>
<li>If depth &gt; 0, follow-up with a new query</li>
</ul>
</li>
</ol>
<p>Let's start by creating a function to generate search queries.</p>
<pre><code class="lang-js">const generateSearchQueries = async (query: string, n: number = 3) =&gt; {
  const {
    object: { queries },
  } = await generateObject({
    model: openai('gpt-4o'),
    prompt: `Generate ${n} search queries for the following query: ${query}`,
    schema: z.object({
      queries: z.array(z.string()).min(1).max(5),
    }),
  })
  return queries
}

const main = async () =&gt; {
  const prompt = 'What do you need to be a D1 shotput athlete?'
  const queries = await generateSearchQueries(prompt)
  // [
  //   'requirements to be a D1 shotput athlete',
  //   'training regimen for D1 shotput athletes',
  //   'qualifications for NCAA Division 1 shotput',
  // ]
}

main()
</code></pre>
<p>Now we need to map these queries to web search results. We use <a href="https://exa.ai">Exa</a> for this.</p>
<pre><code class="lang-js">import Exa from 'exa-js'

const exa = new Exa(process.env.EXA_API_KEY)

type SearchResult = {
  title: string
  url: string
  content: string
}

const searchWeb = async (query: string) =&gt; {
  const { results } = await exa.searchAndContents(query, {
    numResults: 1,
    livecrawl: 'always', // not use cache
  })
  return results.map(
    (r) =&gt;
      ({
        title: r.title,
        url: r.url,
        content: r.text,
      }) as SearchResult
  )
}
</code></pre>
<p>Next thing is to give the model two tools, one to search the web, the other to evaluate the relevance of that tool call. This is the most complicated part of entire workflow, also the agentic part of workflow.</p>
<pre><code class="lang-js">const searchAndProcess = async (query: string) =&gt; {
  const pendingSearchResults: SearchResult[] = []
  const finalSearchResults: SearchResult[] = []
  await generateText({
    model: openai('gpt-4o'),
    prompt: `Search the web for information about ${query}`,
    system:
      'You are a researcher. For each query, search the web and then evaluate if the results are relevant and will help answer the following query',
    maxSteps: 5,
    tools: {
      searchWeb: tool({
        description: 'Search the web for information about a given query',
        parameters: z.object({
          query: z.string().min(1),
        }),
        async execute({ query }) {
          const results = await searchWeb(query)
          pendingSearchResults.push(...results)
          return results
        },
      }),
      evaluate: tool({
        description: 'Evaluate the search results',
        parameters: z.object({}),
        async execute() {
          const pendingResult = pendingSearchResults.pop()!
          const { object: evaluation } = await generateObject({
            model: openai('gpt-4o'),
            prompt: `Evaluate whether the search results are relevant and will help answer the following query: ${query}. If the page already exists in the existing results, mark it as irrelevant.

            &lt;search_results&gt;
            ${JSON.stringify(pendingResult)}
            &lt;/search_results&gt;
            `,
            output: 'enum',
            enum: ['relevant', 'irrelevant'],
          })
          if (evaluation === 'relevant') {
            finalSearchResults.push(pendingResult)
          }
          console.log('Found:', pendingResult.url)
          console.log('Evaluation:', evaluation)
          return evaluation === 'irrelevant'
            ? 'Search results are irrelevant. Please search again with a more specific query.'
            : 'Search results are relevant. End research for this query.'
        },
      }),
    },
  })
  return finalSearchResults
}

for (const query of queries) {
  console.log(`Searching the web for: ${query}`)
  const searchResults = await searchAndProcess(query)
}
</code></pre>
<p>The next step is to generate learnings and follow-up questions, and then add recursion to the <code>deepResearch</code> function.</p>
<pre><code class="lang-js">const generateLearnings = async (query: string, searchResult: SearchResult) =&gt; {
  const { object } = await generateObject({
    model: openai('gpt-4o'),
    prompt: `The user is researching "${query}". The following search result were deemed relevant.
    Generate a learning and a follow-up question from the following search result:

    &lt;search_result&gt;
    ${JSON.stringify(searchResult)}
    &lt;/search_result&gt;
    `,
    schema: z.object({
      learning: z.string(),
      followUpQuestions: z.array(z.string()),
    }),
  })
  return object
}
</code></pre>
<pre><code class="lang-js">const deepResearch = async (
  query: string,
  depth: number = 1,
  breadth: number = 3
) =&gt; {
  const queries = await generateSearchQueries(query)

  for (const query of queries) {
    console.log(`Searching the web for: ${query}`)
    const searchResults = await searchAndProcess(query)
    for (const searchResult of searchResults) {
      console.log(`Processing search result: ${searchResult.url}`)
      const learnings = await generateLearnings(query, searchResult)
      // call deepResearch recursively with decrementing depth and breadth
    }
  }
}
</code></pre>
