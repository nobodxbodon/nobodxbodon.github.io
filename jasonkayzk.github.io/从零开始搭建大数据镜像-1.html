<h2>原文：<a href="https://jasonkayzk.github.io/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%95%9C%E5%83%8F-1.html">从零开始搭建大数据镜像-1</a></h2>
<hr/>
<h2>title: 从零开始搭建大数据镜像-1
toc: true
cover: 'https://img.paulzzh.com/touhou/random?55'
date: 2021-08-21 11:58:40
categories: Docker
tags: [Docker, 大数据]
description: 一直想要有那种开箱即用的大数据Docker镜像，但是找了很久感觉使用体验都不好；最近又搞起了大数据，感觉还是自己搞一个大数据的镜像组集群比较好；</h2>
<p>一直想要有那种开箱即用的大数据Docker镜像，但是找了很久感觉使用体验都不好；</p>
<p>最近又搞起了大数据，感觉还是自己搞一个大数据的镜像组集群比较好；</p>
<p>软件来源：</p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1rL411E7uz">【尚硅谷】2021最新电商数仓V4.0版丨大数据数据仓库项目实战</a></li>
</ul>
<p>Github地址：</p>
<ul>
<li>https://github.com/JasonkayZK/docker_repo/tree/hadoop-3.1.3-cluster</li>
</ul>
<p>DockerHub镜像：</p>
<ul>
<li>https://hub.docker.com/u/jasonkay</li>
<li>https://hub.docker.com/r/jasonkay/big-data-1</li>
<li>https://hub.docker.com/r/jasonkay/big-data-2</li>
<li>https://hub.docker.com/r/jasonkay/big-data-3</li>
</ul>
<p>系列文章：</p>
<ul>
<li><a href="/2021/08/21/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%95%9C%E5%83%8F-1/">《从零开始搭建大数据镜像-1》</a></li>
</ul>
<p>&lt;br/&gt;</p>
<p>&lt;!--more--&gt;</p>
<h1><strong>前言</strong></h1>
<p>本系列会从最开始构建Hadoop开始，逐步添加大数据组件，来构建一个大数据集群；</p>
<p>整个集群共分为三个节点（后面可能会根据情况增加）；</p>
<p>本篇作为最开始，先来构建了三节点的Hadoop集群；</p>
<p><strong>最基础的镜像使用的是：</strong></p>
<ul>
<li><strong>centos:centos7.9.2009</strong></li>
</ul>
<p><strong>软件压缩包存放在：</strong></p>
<ul>
<li><strong>/opt/software</strong></li>
</ul>
<p><strong>软件放在：</strong></p>
<ul>
<li><strong>/opt/module</strong></li>
</ul>
<p><strong>工具放在：</strong></p>
<ul>
<li><strong>/root/bin</strong></li>
</ul>
<p>废话不多说，直接开始吧！</p>
<p>&lt;br/&gt;</p>
<h1><strong>网络规划</strong></h1>
<p>保证大数据中各个节点的网络互通是非常重要的！</p>
<p>因此我们首先来创建一个专属于这个大数据集群的子网：</p>
<pre><code class="language-bash"># 大数据子网
docker network create --subnet 172.30.0.0/24 --gateway 172.30.0.1 big-data
</code></pre>
<p>我们选择了<code>172.30.0.0/24</code>作为子网，<code>172.30.0.1</code>为对应网关；</p>
<p>对于不同节点规划如下：</p>
<p>| <strong>节点</strong>       | <strong>IP</strong>      | <strong>说明</strong>       |
| -------------- | ----------- | -------------- |
| big-data-model | 172.30.0.10 | 大数据基础镜像 |
| big-data-1     | 172.30.0.11 | 节点1          |
| big-data-2     | 172.30.0.12 | 节点2          |
| big-data-3     | 172.30.0.13 | 节点3          |</p>
<p>下面构建基础镜像；</p>
<p>&lt;br/&gt;</p>
<h1><strong>构建基础镜像</strong></h1>
<p>使用下面的命令创建基础镜像容器：</p>
<pre><code class="language-bash"># 最基础镜像
docker run -itd --name big-data-model --net big-data --ip 172.30.0.10  --hostname big-data-model --privileged  centos:centos7.9.2009 /usr/sbin/init

docker exec -it big-data-model /bin/bash
</code></pre>
<blockquote>
<p>&lt;font color="#f00"&gt;<strong>注：<code>--privileged</code>和<code>/usr/sbin/init</code>是必须的，否则会存在容器权限不足的问题！</strong>&lt;/font&gt;</p>
</blockquote>
<p>进入镜像后执行：</p>
<pre><code class="language-bash"># 更新软件源
yum install -y epel-release
yum update -y

# 安装和配置SSH
yum install -y openssh-server
systemctl start sshd
systemctl enable sshd

## 增加配置内容
vi /etc/ssh/sshd_config
UseDNS no
PermitRootLogin yes #允许root登录
PermitEmptyPasswords no #不允许空密码登录
PasswordAuthentication yes # 设置是否使用口令验证

systemctl restart sshd

# 安装SSH客户端
yum -y install openssh-clients

# 安装Vim
yum install -y vim

# 安装网络工具
yum install -y net-tools

# 修改hosts，增加节点内容
vi /etc/hosts
172.30.0.11     big-data-1
172.30.0.12     big-data-2
172.30.0.13     big-data-3
</code></pre>
<p>上面的内容执行完后，执行<code>docker commit</code>将容器保存为镜像：</p>
<pre><code class="language-bash">docker commit --message "基本镜像：添加ssh、net-tools等工具"  big-data-model jasonkay/big-data-model:v0.1

# 向DockerHub推送镜像（可选）
docker push jasonkay/big-data-model:v0.1
</code></pre>
<p>&lt;br/&gt;</p>
<p>下面测试基础镜像：</p>
<pre><code class="language-bash"># 基础镜像测试
docker run -itd --name big-data-1 --net big-data --ip 172.30.0.11  --hostname big-data-1 --privileged  jasonkay/big-data-model:v0.1 /usr/sbin/init
docker run -itd --name big-data-2 --net big-data --ip 172.30.0.12  --hostname big-data-2 --privileged  jasonkay/big-data-model:v0.1 /usr/sbin/init
docker run -itd --name big-data-3 --net big-data --ip 172.30.0.13  --hostname big-data-3 --privileged  jasonkay/big-data-model:v0.1 /usr/sbin/init
</code></pre>
<p>使用基础镜像创建三个大数据容器：</p>
<ul>
<li>big-data-1</li>
<li>big-data-2</li>
<li>big-data-3</li>
</ul>
<p>进入容器1：</p>
<pre><code class="language-bash">docker exec -it big-data-1 /bin/bash
</code></pre>
<p>Ping其他容器：</p>
<pre><code class="language-bash">[root@big-data-1 software]# ping big-data-2
PING big-data-2 (172.30.0.12) 56(84) bytes of data.
64 bytes from big-data-2.big-data (172.30.0.12): icmp_seq=1 ttl=64 time=0.054 ms
64 bytes from big-data-2.big-data (172.30.0.12): icmp_seq=2 ttl=64 time=0.056 ms
64 bytes from big-data-2.big-data (172.30.0.12): icmp_seq=3 ttl=64 time=0.063 ms
^C
--- big-data-2 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 0.054/0.057/0.063/0.009 ms
[root@big-data-1 software]# ping big-data-3
PING big-data-3 (172.30.0.13) 56(84) bytes of data.
64 bytes from big-data-3.big-data (172.30.0.13): icmp_seq=1 ttl=64 time=0.067 ms
64 bytes from big-data-3.big-data (172.30.0.13): icmp_seq=2 ttl=64 time=0.091 ms
64 bytes from big-data-3.big-data (172.30.0.13): icmp_seq=3 ttl=64 time=0.063 ms
^C
--- big-data-3 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2000ms
rtt min/avg/max/mdev = 0.063/0.073/0.091/0.015 ms
</code></pre>
<p>网络畅通！</p>
<p>&lt;br/&gt;</p>
<h1><strong>安装和配置软件及脚本</strong></h1>
<h2><strong>配置SSH</strong></h2>
<p>每个容器执行，生成SSH Key：</p>
<pre><code class="language-bash">ssh-keygen -t rsa
</code></pre>
<p>将各个容器SSH公钥放在其他容器中：</p>
<pre><code class="language-bash">cat ~/.ssh/id_rsa.pub &gt;&gt; authorized_keys
</code></pre>
<p>测试：</p>
<pre><code class="language-bash">[root@big-data-1 software]# ssh big-data-2
Last login: Wed Aug 18 06:48:24 2021
[root@big-data-2 ~]# exit
logout
Connection to big-data-2 closed.
[root@big-data-1 software]# ssh big-data-3
Last login: Wed Aug 18 01:41:21 2021 from 172.30.0.10
[root@big-data-3 ~]# exit
logout
Connection to big-data-3 closed.
</code></pre>
<p>&lt;br/&gt;</p>
<h2><strong>容器集群内同步文件脚本xsync</strong></h2>
<p>首先安装rsync：</p>
<pre><code class="language-bash">yum install -y rsync
</code></pre>
<p>编辑创建xsync文件：</p>
<pre><code class="language-bash">cd ~
mkdir bin
cd ~/bin
vim xsync

#!/bin/bash
# 1. check param num
if [ $# -lt 1 ]
then
  echo Not Enough Arguement!
  exit;
fi
# 2.traverse all mechine
for host in "big-data-1" "big-data-2" "big-data-3"
do
  echo ====================  $host  ====================
  # 3.traverse dir for each file
  for file in $@
  do
    # 4.check file exist
    if [ -e $file ]
    then
      # 5.get parent dor
      pdir=$(cd -P $(dirname $file); pwd)
      # 6.get file name
      fname=$(basename $file)
      ssh $host "mkdir -p $pdir"
      rsync -av $pdir/$fname $host:$pdir
    else
      echo $file does not exists!
    fi
  done
done

# 增加执行权限
chmod +x xsync
</code></pre>
<p>测试：</p>
<pre><code class="language-bash">[root@big-data-1 bin]# xsync xsync 
==================== big-data-1 ====================
sending incremental file list

sent 38 bytes  received 12 bytes  100.00 bytes/sec
total size is 596  speedup is 11.92
==================== big-data-2 ====================
sending incremental file list

sent 38 bytes  received 12 bytes  100.00 bytes/sec
total size is 596  speedup is 11.92
==================== big-data-3 ====================
sending incremental file list

sent 38 bytes  received 12 bytes  100.00 bytes/sec
total size is 596  speedup is 11.92
</code></pre>
<p>成功！</p>
<p>&lt;br/&gt;</p>
<h2><strong>JDK安装</strong></h2>
<p>各个容器内执行：</p>
<pre><code class="language-bash">mkdir -p /opt/software
mkdir -p /opt/module
</code></pre>
<p>创建目录；</p>
<p>将宿主机中的JDK发送到容器中：</p>
<pre><code class="language-bash">docker cp jdk-8u212-linux-x64.tar.gz big-data-1:/opt/software
docker cp jdk-8u212-linux-x64.tar.gz big-data-2:/opt/software
docker cp jdk-8u212-linux-x64.tar.gz big-data-3:/opt/software
</code></pre>
<p>各个容器内解压缩：</p>
<pre><code class="language-bash">cd /opt/software
tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/

# 配置环境变量
vim /etc/profile.d/my_env.sh

#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_212
export PATH=$PATH:$JAVA_HOME/bin

# 立即生效
source /etc/profile.d/my_env.sh
# 校验
java -version
</code></pre>
<p>&lt;br/&gt;</p>
<h2><strong>创建模拟数据</strong></h2>
<p>各个容器内执行：</p>
<pre><code class="language-bash">mkdir /opt/module/applog
</code></pre>
<p>宿主机将文件copy进容器中：</p>
<pre><code class="language-bash">docker cp 日志.zip big-data-1:/opt/module/applog
docker cp 日志.zip big-data-2:/opt/module/applog
docker cp 日志.zip big-data-3:/opt/module/applog
</code></pre>
<p>各个容器内解压缩：</p>
<pre><code class="language-bash">yum install -y unzip
cd  /opt/module/applog
unzip 日志.zip
rm -rf *.zip
</code></pre>
<p>各个容器内生成日志：</p>
<pre><code class="language-bash">java -jar gmall2020-mock-log-2021-01-22.jar
</code></pre>
<blockquote>
<p><strong>① 配置集群日志生成脚本</strong></p>
<p>配置环境变量：</p>
<pre><code class="language-bash">vim /etc/profile.d/my_env.sh
export PATH=$PATH:$JAVA_HOME/bin:.:~/bin
source /etc/profile.d/my_env.sh
</code></pre>
<p>创建生成脚本：</p>
<pre><code class="language-bash">vim ~/bin/lg.sh

# 脚本内容
#!/bin/bash
for i in "big-data-1" "big-data-2" "big-data-3"; do
    echo "========== $i =========="
    ssh $i "cd /opt/module/applog/; java -jar gmall2020-mock-log-2021-01-22.jar &gt;/dev/null 2&gt;&amp;1 &amp;"
done

chmod u+x ~/bin/lg.sh
</code></pre>
</blockquote>
<blockquote>
<p><strong>② 集群所有进程查看脚本</strong></p>
<p>各个容器中：</p>
<pre><code class="language-bash">vim ~/bin/xcall.sh

#! /bin/bash
for i in "big-data-1" "big-data-2" "big-data-3"
do
    echo --------- $i ----------
    ssh $i "$*"
done

chmod 777 ~/bin/xcall.sh
</code></pre>
<p>测试：</p>
<pre><code class="language-bash">[root@big-data-1 /]# ~/bin/xcall.sh jps
--------- big-data-1 ----------
204 Jps
--------- big-data-2 ----------
152 Jps
--------- big-data-3 ----------
155 Jps
</code></pre>
</blockquote>
<p>&lt;br/&gt;</p>
<h2><strong>Hadoop安装和配置</strong></h2>
<h3><strong>软件安装</strong></h3>
<p>宿主机传输文件到容器：</p>
<pre><code class="language-bash">docker cp hadoop-3.1.3.tar.gz big-data-1:/opt/software
docker cp hadoop-3.1.3.tar.gz big-data-2:/opt/software
docker cp hadoop-3.1.3.tar.gz big-data-3:/opt/software
</code></pre>
<p>各容器解压缩并配置：</p>
<pre><code class="language-bash">cd /opt/software
tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/

# 配置环境变量
vim /etc/profile.d/my_env.sh

# HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

export HDFS_NAMENODE_USER="root"
export HDFS_DATANODE_USER="root"
export HDFS_SECONDARYNAMENODE_USER="root"
export YARN_RESOURCEMANAGER_USER="root"
export YARN_NODEMANAGER_USER="root"

# 立即生效
source /etc/profile.d/my_env.sh
</code></pre>
<p>&lt;br/&gt;</p>
<h3><strong>Hadoop配置</strong></h3>
<p>进入Hadoop目录：</p>
<pre><code class="language-bash"># big-data-1节点
cd $HADOOP_HOME/etc/hadoop
</code></pre>
<p>修改核心配置文件：</p>
<pre><code class="language-bash">vim core-site.xml

&lt;configuration&gt;
&lt;!-- 指定NameNode的地址 --&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://big-data-1:8020&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 指定hadoop数据的存储目录 --&gt;
        &lt;property&gt;
            &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
            &lt;value&gt;/opt/module/hadoop-3.1.3/data&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 配置HDFS网页登录使用的静态用户为root --&gt;
        &lt;property&gt;
            &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;
            &lt;value&gt;root&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 配置该root(superUser)允许通过代理访问的主机节点 --&gt;
        &lt;property&gt;
            &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;
            &lt;value&gt;*&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 配置该root(superUser)允许通过代理用户所属组 --&gt;
        &lt;property&gt;
            &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;
            &lt;value&gt;*&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 配置该root(superUser)允许通过代理的用户--&gt;
        &lt;property&gt;
            &lt;name&gt;hadoop.proxyuser.root.users&lt;/name&gt;
            &lt;value&gt;*&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>HDFS配置文件：</p>
<pre><code class="language-bash">vim hdfs-site.xml

&lt;configuration&gt;
&lt;!-- nn web端访问地址--&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;
        &lt;value&gt;big-data-1:9870&lt;/value&gt;
    &lt;/property&gt;
    
&lt;!-- 2nn web端访问地址--&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
        &lt;value&gt;big-data-3:9868&lt;/value&gt;
    &lt;/property&gt;
&lt;!-- 测试环境指定HDFS副本的数量1 --&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>YARN配置文件：</p>
<pre><code class="language-bash">vim yarn-site.xml

&lt;configuration&gt;
&lt;!-- 指定MR走shuffle --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;
    
    &lt;!-- 指定ResourceManager的地址--&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
        &lt;value&gt;big-data-2&lt;/value&gt;
    &lt;/property&gt;
    
    &lt;!-- 环境变量的继承 --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;
&lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;
    &lt;/property&gt;
    
    &lt;!-- yarn容器允许分配的最大最小内存 --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;
        &lt;value&gt;512&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;
        &lt;value&gt;4096&lt;/value&gt;
    &lt;/property&gt;
    
    &lt;!-- yarn容器允许管理的物理内存大小 --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
        &lt;value&gt;4096&lt;/value&gt;
    &lt;/property&gt;
    
    &lt;!-- 关闭yarn对虚拟内存的限制检查 --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;
        &lt;value&gt;false&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>MapReduce配置文件：</p>
<pre><code class="language-bash">vim mapred-site.xml

&lt;configuration&gt;
&lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>配置workers：</p>
<pre><code class="language-bash">vim /opt/module/hadoop-3.1.3/etc/hadoop/workers

big-data-1
big-data-2
big-data-3
</code></pre>
<p>配置历史服务器：</p>
<pre><code class="language-bash">vim mapred-site.xml

&lt;!-- 历史服务器端地址 --&gt;
&lt;property&gt;
    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
    &lt;value&gt;big-data-1:10020&lt;/value&gt;
&lt;/property&gt;

&lt;!-- 历史服务器web端地址 --&gt;
&lt;property&gt;
    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
    &lt;value&gt;big-data-1:19888&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<p>配置日志的聚集：</p>
<pre><code class="language-bash">vim yarn-site.xml

&lt;!-- 开启日志聚集功能 --&gt;
&lt;property&gt;
    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;!-- 设置日志聚集服务器地址 --&gt;
&lt;property&gt;  
    &lt;name&gt;yarn.log.server.url&lt;/name&gt;  
    &lt;value&gt;http://big-data-1:19888/jobhistory/logs&lt;/value&gt;
&lt;/property&gt;

&lt;!-- 设置日志保留时间为7天 --&gt;
&lt;property&gt;
    &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;
    &lt;value&gt;604800&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<p>分发Hadoop：</p>
<pre><code class="language-bash"># 在big-data-1执行
xsync /opt/module/hadoop-3.1.3/
</code></pre>
<p>&lt;br/&gt;</p>
<h3><strong>启动Hadoop集群</strong></h3>
<blockquote>
<p>&lt;font color="#f00"&gt;<strong>如果集群是第一次启动，需要在big-data-1节点格式化NameNode（注意格式化之前，一定要先停止上次启动所有namenode和datanode进程，然后再删除data和log数据）！</strong>&lt;/font&gt;</p>
</blockquote>
<p>格式化节点：</p>
<pre><code class="language-bash"># 在big-data-1执行
cd $HADOOP_HOME
./bin/hdfs namenode -format
</code></pre>
<p>启动HDFS：</p>
<pre><code class="language-bash">sbin/start-dfs.sh
</code></pre>
<p>在配置了ResourceManager的节点（big-data-2）启动YARN：</p>
<pre><code class="language-bash"># 在big-data-2执行
sbin/start-yarn.sh
</code></pre>
<p>查看结果：</p>
<pre><code class="language-bash">curl big-data-1:9870

# 返回类似于：&lt;title&gt;Hadoop Administration&lt;/title&gt; 则成功；
</code></pre>
<blockquote>
<p>或者：宿主机访问<code>172.30.0.11:9870</code>；</p>
<p>&lt;font color="#f00"&gt;<strong>注：如果你的宿主机是虚拟机或者其他服务器，可以在宿主机配置Nginx反向代理，进而在本机直接访问宿主机即可！</strong>&lt;/font&gt;</p>
<p>具体方法：</p>
<pre><code class="language-bash"># 宿主机编辑Nginx配置
vi /etc/nginx/conf.d/hadoop_admin.conf

server {
    listen 9870;
    server_name localhost;
    location / {
        proxy_pass http://172.30.0.11:9870;
    }
}
</code></pre>
<p>添加映射即可；</p>
<p>然后本机访问：<code>&lt;宿主机IP&gt;:9870</code>；</p>
<p>效果如下：</p>
<p><img alt="" src="https://raw.gitmirror.com/JasonkayZK/blog_static/master/images/big-data-1.png"/></p>
</blockquote>
<p>接下来我们配置Hadoop集群启动脚本；</p>
<p>&lt;br/&gt;</p>
<h3><strong>配置Hadoop集群启动脚本</strong></h3>
<pre><code class="language-bash">vim ~/bin/hdp.sh

#!/bin/bash
if [ $# -lt 1 ]
then
    echo "No Args Input..."
    exit ;
fi
case $1 in
"start")
        echo " =================== Start  Hadoop Cluster ==================="

        echo " --------------- Start hdfs ---------------"
        ssh big-data-1 "/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"
        echo " --------------- Start yarn ---------------"
        ssh big-data-2 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"
        echo " --------------- Start historyserver ---------------"
        ssh big-data-1 "/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver"
;;
"stop")
        echo " =================== Close Hadoop Cluster ==================="

        echo " --------------- Close historyserver ---------------"
        ssh big-data-1 "/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver"
        echo " --------------- Close yarn ---------------"
        ssh big-data-2 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"
        echo " --------------- Close hdfs ---------------"
        ssh big-data-1 "/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"
;;
*)
    echo "Input Args Error..."
;;
esac

# 修改权限
chmod 777 hdp.sh
</code></pre>
<p>脚本测试：</p>
<p>启动：</p>
<pre><code class="language-bash">[root@big-data-1 bin]# ./hdp.sh start
 =================== Start  Hadoop Cluster ===================
 --------------- Start hdfs ---------------
Starting namenodes on [big-data-1]
Last login: Sat Aug 21 05:17:52 UTC 2021 from 172.30.0.11 on pts/2
Starting datanodes
localhost: datanode is running as process 530.  Stop it first.
Last login: Sat Aug 21 06:37:11 UTC 2021
Starting secondary namenodes [big-data-3]
Last login: Sat Aug 21 06:37:13 UTC 2021
 --------------- Start yarn ---------------
Starting resourcemanager
Last login: Sat Aug 21 05:17:56 UTC 2021 from 172.30.0.11 on pts/1
Starting nodemanagers
Last login: Sat Aug 21 06:37:20 UTC 2021
 --------------- Start historyserver ---------------
</code></pre>
<p>关闭：</p>
<pre><code class="language-bash">[root@big-data-1 /]# ~/bin/hdp.sh stop
 =================== Close Hadoop Cluster ===================
 --------------- Close historyserver ---------------
 --------------- Close yarn ---------------
Stopping nodemanagers
Last login: Sat Aug 21 06:37:22 UTC 2021
Stopping resourcemanager
Last login: Sat Aug 21 06:47:52 UTC 2021
 --------------- Close hdfs ---------------
Stopping namenodes on [big-data-1]
Last login: Sat Aug 21 06:37:16 UTC 2021
Stopping datanodes
Last login: Sat Aug 21 06:47:56 UTC 2021
Stopping secondary namenodes [big-data-3]
Last login: Sat Aug 21 06:47:57 UTC 2021
</code></pre>
<p>&lt;br/&gt;</p>
<h2><strong>创建Big-Data镜像</strong></h2>
<p>经过上面的安装和配置，我们已经创建了一个三节点的Hadoop集群；</p>
<p>接下来我们将这几个容器提交为镜像：</p>
<pre><code class="language-bash">docker commit --message "大数据集群基本镜像：完成Hadoop和Yarn部分" big-data-1 jasonkay/big-data-1:v1.0
docker commit --message "大数据集群基本镜像：完成Hadoop和Yarn部分" big-data-2 jasonkay/big-data-2:v1.0
docker commit --message "大数据集群基本镜像：完成Hadoop和Yarn部分" big-data-3 jasonkay/big-data-3:v1.0
</code></pre>
<p>&lt;br/&gt;</p>
<h1><strong>Hadoop测试</strong></h1>
<blockquote>
<p>&lt;font color="#f00"&gt;<strong>测试之前请确保已经停止并清除了所有正在Run的容器！</strong>&lt;/font&gt;</p>
</blockquote>
<h2><strong>通过命令行启动多个容器</strong></h2>
<h3><strong>启动测试</strong></h3>
<p>直接通过命令行启动容器：</p>
<pre><code class="language-bash">docker run -itd --name big-data-1 --net big-data --ip 172.30.0.11  --hostname big-data-1 --privileged  jasonkay/big-data-1:v1.0 /usr/sbin/init
docker run -itd --name big-data-2 --net big-data --ip 172.30.0.12  --hostname big-data-2 --privileged  jasonkay/big-data-2:v1.0 /usr/sbin/init
docker run -itd --name big-data-3 --net big-data --ip 172.30.0.13  --hostname big-data-3 --privileged  jasonkay/big-data-3:v1.0 /usr/sbin/init
</code></pre>
<p>进入容器<code>big-data-1</code>：</p>
<pre><code class="language-bash">docker exec -it big-data-1 /bin/bash
</code></pre>
<p>容器中启动Hadoop：</p>
<pre><code class="language-bash">[root@big-data-1 /]# ~/bin/hdp.sh start
</code></pre>
<p>查看结果：</p>
<p><img alt="" src="https://raw.gitmirror.com/JasonkayZK/blog_static/master/images/big-data-2.png"/></p>
<h3><strong>功能测试</strong></h3>
<p>数据准备：</p>
<pre><code class="language-bash">cd ~/
vi data.txt

# 写入内容
hello hadoop
hello World
Hello Java
Hey man
i am a programmer
</code></pre>
<p>写入HDFS：</p>
<pre><code class="language-bash"># 创建/input目录
hdfs dfs -mkdir /input 
# 写入hdfs
hdfs dfs -put data.txt /input 
# 查看HDFS
hdfs dfs -ls /input
Found 1 items
-rw-r--r--   1 root supergroup         62 2021-08-18 06:42 /input/data.txt
</code></pre>
<p>Word Count测试：</p>
<pre><code class="language-bash">cd /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/
hadoop jar hadoop-mapreduce-examples-3.1.3.jar wordcount /input/data.txt /output
</code></pre>
<p>查看结果：</p>
<pre><code class="language-bash">hdfs dfs -cat /output/part-r-00000

2021-08-21 07:00:46,337 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Hello   1
Hey     1
Java    1
World   1
a       1
am      1
hadoop  1
hello   2
i       1
man     1
programmer      1
</code></pre>
<p>成功！</p>
<p>&lt;br/&gt;</p>
<h2><strong>通过Docker-Compose启动多个容器</strong></h2>
<p>编辑docker-compose.yml</p>
<p>docker-compose.yml</p>
<pre><code class="language-yaml">version: '3.4'

services:
  big-data-1:
    image: jasonkay/big-data-1:v1.0
    hostname: big-data-1
    container_name: big-data-1
    privileged: true
    links:
      - big-data-2
      - big-data-3
    depends_on:
      - big-data-2
      - big-data-3
    ports:
      - 9870:9870
      - 22
    entrypoint: ["/usr/sbin/init"]
    networks:
      big-data:
        ipv4_address: 172.30.0.11

  big-data-2:
    image: jasonkay/big-data-2:v1.0
    hostname: big-data-2
    container_name: big-data-2
    privileged: true
    entrypoint: ["/usr/sbin/init"]
    ports:
      - 22
    networks:
      big-data:
        ipv4_address: 172.30.0.12

  big-data-3:
    image: jasonkay/big-data-3:v1.0
    hostname: big-data-3
    container_name: big-data-3
    privileged: true
    entrypoint: ["/usr/sbin/init"]
    ports:
      - 22
    networks:
      big-data:
        ipv4_address: 172.30.0.13

networks:
  big-data:
    external:
      name: big-data
</code></pre>
<p>启动三个节点：</p>
<pre><code class="language-bash">[root@localhost docker-repo]# docker-compose  up -d
Creating big-data-3 ... done
Creating big-data-2 ... done
Creating big-data-1 ... done
</code></pre>
<p>进入<code>big-data-1</code>启动集群：</p>
<pre><code class="language-bash">[root@big-data-1 bin]# ./hdp.sh start
 =================== Start  Hadoop Cluster ===================
 --------------- Start hdfs ---------------
Starting namenodes on [big-data-1]
Last login: Wed Aug 18 04:48:01 UTC 2021
Starting datanodes
Last login: Sun Aug 22 08:13:26 UTC 2021
Starting secondary namenodes [big-data-3]
Last login: Sun Aug 22 08:13:28 UTC 2021
 --------------- Start yarn ---------------
Starting resourcemanager
Last login: Wed Aug 18 04:47:56 UTC 2021
Starting nodemanagers
Last login: Sun Aug 22 08:13:35 UTC 2021
 --------------- Start historyserver ---------------
</code></pre>
<p>查看状态：</p>
<pre><code class="language-bash">[root@big-data-1 ~]# ./xcall.sh jps
--------- big-data-1 ----------
1137 Jps
811 NodeManager
940 JobHistoryServer
492 DataNode
271 NameNode
--------- big-data-2 ----------
694 NodeManager
317 ResourceManager
142 DataNode
1054 Jps
--------- big-data-3 ----------
145 DataNode
229 SecondaryNameNode
518 Jps
316 NodeManager
</code></pre>
<p>启动成功！</p>
<p>&lt;br/&gt;</p>
<h1><strong>附录</strong></h1>
<p>软件来源：</p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1rL411E7uz">【尚硅谷】2021最新电商数仓V4.0版丨大数据数据仓库项目实战</a></li>
</ul>
<p>Github地址：</p>
<ul>
<li>https://github.com/JasonkayZK/docker_repo/tree/hadoop-3.1.3-cluster</li>
</ul>
<p>DockerHub镜像：</p>
<ul>
<li>https://hub.docker.com/u/jasonkay</li>
<li>https://hub.docker.com/r/jasonkay/big-data-1</li>
<li>https://hub.docker.com/r/jasonkay/big-data-2</li>
<li>https://hub.docker.com/r/jasonkay/big-data-3</li>
</ul>
<p>系列文章：</p>
<ul>
<li><a href="/2021/08/21/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%95%9C%E5%83%8F-1/">《从零开始搭建大数据镜像-1》</a></li>
</ul>
<p>&lt;br/&gt;</p>
