<h2><a href="https://github.com/jasonkayzk/jasonkayzk.github.io/blob/master/source/_posts/%E5%9C%A8k3s%E9%9B%86%E7%BE%A4%E4%B8%8A%E9%83%A8%E7%BD%B2ClickHouse%E9%9B%86%E7%BE%A4.md">仓库源文</a>，<a href="https://jasonkayzk.github.io/2022/10/25/%E5%9C%A8k3s%E9%9B%86%E7%BE%A4%E4%B8%8A%E9%83%A8%E7%BD%B2ClickHouse%E9%9B%86%E7%BE%A4">站点原文</a></h2>
<hr/>
<h2>title: 在k3s集群上部署ClickHouse集群
toc: true
cover: 'https://img.paulzzh.com/touhou/random?33'
date: 2022-10-25 08:38:34
categories: ClickHouse
tags: [ClickHouse, k3s, Kubernetes]
description: ClickHouse是一个用于联机分析(OLAP)的列式数据库管理系统(DBMS)，由俄罗斯搜索引擎Yandex开源；本文在前文搭建了k3s集群的基础之上，部署ClickHouse；</h2>
<p>ClickHouse是一个用于联机分析(OLAP)的列式数据库管理系统(DBMS)，由俄罗斯搜索引擎Yandex开源；</p>
<p>本文在前文 <a href="https://jasonkayzk.github.io/2022/10/22/%E5%8D%95%E6%9C%BA%E9%83%A8%E7%BD%B2autok3s/">《单机部署autok3s》</a> 搭建了k3s集群的基础之上，部署ClickHouse；</p>
<p>clickhouse-operator repo：</p>
<ul>
<li>https://github.com/Altinity/clickhouse-operator</li>
</ul>
<p>clickhouse-operator文档：</p>
<ul>
<li>https://docs.altinity.com/clickhouseonkubernetes/kubernetesquickstartguide/quickcluster/</li>
</ul>
<p>ClickHouse文档：</p>
<ul>
<li>https://clickhouse.com/docs/zh/</li>
</ul>
<p>&lt;br/&gt;</p>
<p>&lt;!--more--&gt;</p>
<h1><strong>在k3s集群上部署ClickHouse集群</strong></h1>
<h2><strong>安装clickhouse-operator</strong></h2>
<p>如果直接部署 ClickHouse 集群，需要手动修改一些配置，比较麻烦；</p>
<p>我们可以借助：<a href="https://github.com/Altinity/clickhouse-operator">clickhouse-operator</a> 来管理和部署 ClickHouse 集群；</p>
<blockquote>
<p><strong>通常情况下，xxx-operator 都提供了一系列组件，例如：</strong></p>
<ul>
<li><strong>Cluster 定义：通过 CRD（<code>CustomResourceDefinition</code>）定义 <code>Cluster</code> 等自定义资源，使得 Kubernetes 世界认识该 Cluster 并让其与 <code>Deployment</code>、<code>StatefulSet</code> 一同享受 Kubernetes 的头等公民待遇；</strong></li>
<li><strong>控制器<code>xxx-controller-manager</code>：包含一组自定义控制器，控制器通过循环不断比对被控制对象的期望状态与实际状态，并通过自定义的逻辑驱动被控制对象达到期望状态（类似于K8S中的Controller）；</strong></li>
<li><strong>调度器<code>scheduler</code>：通常情况下，调度器是一个 Kubernetes 调度器扩展，它为 Kubernetes 调度器注入集群特有的调度逻辑，如：为保证高可用，任一 Node 不能调度超过集群半数以上的实例等；</strong></li>
</ul>
<p><strong><a href="https://github.com/pingcap/tidb">tidb</a> 也提供了对应的：<a href="https://github.com/pingcap/tidb-operator">tidb-operator</a>！</strong></p>
</blockquote>
<p>因此，在部署 ClickHouse 之前，我们应当先安装 <a href="https://github.com/Altinity/clickhouse-operator">clickhouse-operator</a>；</p>
<p>可以使用配置文件直接安装：</p>
<pre><code class="language-bash">kubectl apply -f https://github.com/Altinity/clickhouse-operator/raw/0.18.3/deploy/operator/clickhouse-operator-install-bundle.yaml
</code></pre>
<blockquote>
<p><strong>上面在应用配置时指定了版本为 <code>0.18.3</code>，这也是官方推荐的方式；</strong></p>
</blockquote>
<blockquote>
<p><strong>同时注意：</strong></p>
<p><strong>如果还存在使用 clickhouse-operator 部署的Click House，此时不要使用 <code>kubectl delete</code> 删除 operator！</strong></p>
<p>详见：</p>
<ul>
<li><a href="https://github.com/Altinity/clickhouse-operator/issues/830">Altinity/clickhouse-operator#830</a></li>
</ul>
</blockquote>
<p>上面的命令执行成功后会输出：</p>
<pre><code>customresourcedefinition.apiextensions.k8s.io/clickhouseinstallations.clickhouse.altinity.com created
customresourcedefinition.apiextensions.k8s.io/clickhouseinstallationtemplates.clickhouse.altinity.com created
customresourcedefinition.apiextensions.k8s.io/clickhouseoperatorconfigurations.clickhouse.altinity.com created
serviceaccount/clickhouse-operator created
clusterrole.rbac.authorization.k8s.io/clickhouse-operator-kube-system created
clusterrolebinding.rbac.authorization.k8s.io/clickhouse-operator-kube-system created
configmap/etc-clickhouse-operator-files created
configmap/etc-clickhouse-operator-confd-files created
configmap/etc-clickhouse-operator-configd-files created
configmap/etc-clickhouse-operator-templatesd-files created
configmap/etc-clickhouse-operator-usersd-files created
deployment.apps/clickhouse-operator created
service/clickhouse-operator-metrics created
</code></pre>
<p>表示成功创建了一系列资源：</p>
<ul>
<li><a href="https://docs.altinity.com/clickhouseonkubernetes/kubernetesoperatorguide/operatorresources/">Altinity Kubernetes Operator Resources</a></li>
</ul>
<p>我们可以通过命令查看：</p>
<pre><code class="language-bash">$ kubectl -n kube-system get po | grep click
clickhouse-operator-857c69ffc6-njw97      2/2     Running     0          33h
</code></pre>
<p>下面通过 <code>clickhouse-operator</code> 安装 ClickHouse；</p>
<p>&lt;br/&gt;</p>
<h2><strong>安装ClickHouse</strong></h2>
<p>首先创建一个 namespace：</p>
<pre><code class="language-bash">$ kubectl create ns my-ch

namespace/my-ch created
</code></pre>
<p>随后声明资源：</p>
<p>sample01.yaml</p>
<pre><code class="language-yaml">apiVersion: "clickhouse.altinity.com/v1"
kind: "ClickHouseInstallation"
metadata:
  name: "demo-01"
spec:
  configuration:
    clusters:
      - name: "demo-01"
        layout:
          shardsCount: 1
          replicasCount: 1
</code></pre>
<p>这里就用到了我们在之前安装 clickhouse-operator 时加载的组件配置；</p>
<p>随后直接应用配置：</p>
<pre><code class="language-bash">$ kubectl apply -n my-ch -f sample01.yaml

clickhouseinstallation.clickhouse.altinity.com/demo-01 created
</code></pre>
<p>即可成功部署：</p>
<pre><code class="language-bash">$ kubectl -n my-ch get chi -o wide
NAME      VERSION   CLUSTERS   SHARDS   HOSTS   TASKID                                 STATUS      UPDATED   ADDED   DELETED   DELETE   ENDPOINT
demo-01   0.18.1    1          1        1       6d1d2c3d-90e5-4110-81ab-8863b0d1ac47   Completed             1                          clickhouse-demo-01.test.svc.cluster.local
</code></pre>
<p>同时可以查看服务：</p>
<pre><code class="language-bash">NAME                      TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                         AGE
chi-demo-01-demo-01-0-0   ClusterIP      None           &lt;none&gt;        8123/TCP,9000/TCP,9009/TCP      2s
clickhouse-demo-01        LoadBalancer   10.111.27.86   &lt;pending&gt;     8123:31126/TCP,9000:32460/TCP   19s
</code></pre>
<p>此时可以通过进入容器连接：</p>
<pre><code class="language-bash">$ kubectl -n my-ch exec -it chi-demo-01-demo-01-0-0-0 -- clickhouse-client
ClickHouse client version 22.1.3.7 (official build).
Connecting to localhost:9000 as user default.
Connected to ClickHouse server version 22.1.3 revision 54455.

chi-demo-01-demo-01-0-0-0.chi-demo-01-demo-01-0-0.my-ch.svc.cluster.local :)
</code></pre>
<p>同时也可以远程连接，默认账号、密码：</p>
<ul>
<li>Default Username: <code>clickhouse_operator</code></li>
<li>Default Password: <code>clickhouse_operator_password</code></li>
</ul>
<p>&lt;br/&gt;</p>
<h2><strong>升级集群为2个分片</strong></h2>
<p>Copy sample01.yaml 为 sample02.yaml：</p>
<p>sample02.yaml</p>
<pre><code class="language-yaml">apiVersion: "clickhouse.altinity.com/v1"
kind: "ClickHouseInstallation"
metadata:
  name: "demo-01"
spec:
  configuration:
    clusters:
      - name: "demo-01"
        layout:
          shardsCount: 2
          replicasCount: 1
</code></pre>
<p><strong>注意：由于我们没有改 name 配置，所以k8s知道我们是在更新配置；</strong></p>
<p>应用最新配置：</p>
<pre><code class="language-bash">kubectl apply -n my-ch -f sample02.yaml

clickhouseinstallation.clickhouse.altinity.com/demo-01 configured
</code></pre>
<p>此时我们有了两个分片：</p>
<pre><code class="language-bash">$ kubectl get service -n my-ch
NAME                      TYPE           CLUSTER-IP     EXTERNAL-IP                                   PORT(S)                         AGE
clickhouse-demo-01        LoadBalancer   10.43.93.132   172.19.0.2,172.19.0.3,172.19.0.4,172.19.0.5   8123:30842/TCP,9000:31655/TCP   33h
chi-demo-01-demo-01-0-0   ClusterIP      None           &lt;none&gt;                                        8123/TCP,9000/TCP,9009/TCP      33h
chi-demo-01-demo-01-1-0   ClusterIP      None           &lt;none&gt;                                        8123/TCP,9000/TCP,9009/TCP      33h
</code></pre>
<p>查看集群信息：</p>
<pre><code class="language-bash">$ kubectl -n my-ch exec -it chi-demo-01-demo-01-0-0-0 -- clickhouse-client
ClickHouse client version 22.1.3.7 (official build).
Connecting to localhost:9000 as user default.
Connected to ClickHouse server version 22.1.3 revision 54455.

chi-demo-01-demo-01-0-0-0.chi-demo-01-demo-01-0-0.my-ch.svc.cluster.local :) SELECT * FROM system.clusters
                                                                             

SELECT *
FROM system.clusters

Query id: 587358e9-aeed-4df0-abe7-ee32543c418c

┌─cluster─────────────────────────────────────────┬─shard_num─┬─shard_weight─┬─replica_num─┬─host_name───────────────┬─host_address─┬─port─┬─is_local─┬─user────┬─default_database─┬─errors_count─┬─slowdowns_count─┬─estimated_recovery_time─┐
│ all-replicated                                  │         1 │            1 │           1 │ chi-demo-01-demo-01-0-0 │ 127.0.0.1    │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ all-replicated                                  │         1 │            1 │           2 │ chi-demo-01-demo-01-1-0 │ 10.42.1.15   │ 9000 │        0 │ default │                  │            0 │               0 │                       0 │
│ all-sharded                                     │         1 │            1 │           1 │ chi-demo-01-demo-01-0-0 │ 127.0.0.1    │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ all-sharded                                     │         2 │            1 │           1 │ chi-demo-01-demo-01-1-0 │ 10.42.1.15   │ 9000 │        0 │ default │                  │            0 │               0 │                       0 │
│ demo-01                                         │         1 │            1 │           1 │ chi-demo-01-demo-01-0-0 │ 127.0.0.1    │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ demo-01                                         │         2 │            1 │           1 │ chi-demo-01-demo-01-1-0 │ 10.42.1.15   │ 9000 │        0 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_one_shard_three_replicas_localhost │         1 │            1 │           1 │ 127.0.0.1               │ 127.0.0.1    │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_one_shard_three_replicas_localhost │         1 │            1 │           2 │ 127.0.0.2               │ 127.0.0.2    │ 9000 │        0 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_one_shard_three_replicas_localhost │         1 │            1 │           3 │ 127.0.0.3               │ 127.0.0.3    │ 9000 │        0 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_two_shards                         │         1 │            1 │           1 │ 127.0.0.1               │ 127.0.0.1    │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_two_shards                         │         2 │            1 │           1 │ 127.0.0.2               │ 127.0.0.2    │ 9000 │        0 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_two_shards_internal_replication    │         1 │            1 │           1 │ 127.0.0.1               │ 127.0.0.1    │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_two_shards_internal_replication    │         2 │            1 │           1 │ 127.0.0.2               │ 127.0.0.2    │ 9000 │        0 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_two_shards_localhost               │         1 │            1 │           1 │ localhost               │ ::1          │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_two_shards_localhost               │         2 │            1 │           1 │ localhost               │ ::1          │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ test_shard_localhost                            │         1 │            1 │           1 │ localhost               │ ::1          │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ test_shard_localhost_secure                     │         1 │            1 │           1 │ localhost               │ ::1          │ 9440 │        0 │ default │                  │            0 │               0 │                       0 │
│ test_unavailable_shard                          │         1 │            1 │           1 │ localhost               │ ::1          │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ test_unavailable_shard                          │         2 │            1 │           1 │ localhost               │ ::1          │    1 │        0 │ default │                  │            0 │               0 │                       0 │
└─────────────────────────────────────────────────┴───────────┴──────────────┴─────────────┴─────────────────────────┴──────────────┴──────┴──────────┴─────────┴──────────────────┴──────────────┴─────────────────┴─────────────────────────┘

19 rows in set. Elapsed: 0.001 sec.
</code></pre>
<p>可以看到，通过 clickhouse-operator 安装 ClickHouse 是非常简单的！</p>
<blockquote>
<p>更多关于安装：</p>
<ul>
<li>https://docs.altinity.com/clickhouseonkubernetes/kubernetesquickstartguide/</li>
</ul>
</blockquote>
<p>&lt;br/&gt;</p>
<h1><strong>附录</strong></h1>
<p>clickhouse-operator repo：</p>
<ul>
<li>https://github.com/Altinity/clickhouse-operator</li>
</ul>
<p>clickhouse-operator文档：</p>
<ul>
<li>https://docs.altinity.com/clickhouseonkubernetes/kubernetesquickstartguide/quickcluster/</li>
</ul>
<p>ClickHouse文档：</p>
<ul>
<li>https://clickhouse.com/docs/zh/</li>
</ul>
<p>&lt;br/&gt;</p>
