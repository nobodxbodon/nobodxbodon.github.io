<h2><a href="https://github.com/hijiangtao/hijiangtao.github.io/blob/master/_posts/2014-05-10-SparkInstallNode.markdown">仓库源文</a>，<a href="https://hijiangtao.github.io/2014/05/10/SparkInstallNode.markdown">站点原文</a></h2>
<p><strong>前言</strong>：Spark作为最有可能代替mapreduce的分布式计算框架，正受到广泛的关注。相比Hadoop来说，Spark的中间计算结果存于内存无疑给计算过程节省了很多时间，于是想试试看其与Hadoop有什么不一样的地方，就有了这篇Spark的单机部署与测试笔记。</p>
<hr/>
<h2>一、硬件环境</h2>
<ul>
<li>操作系统： ubuntu-13.04-desktop-i386</li>
<li>JAVA： jdk1.7</li>
<li>SSH配置： openssh-server</li>
</ul>
<hr/>
<h2>二、资源准备</h2>
<p>什么是Spark?以下为Spark官网的一句话简介：</p>
<p>Apache Spark™ is a fast and general engine for large-scale data processing.</p>
<p>Spark包资源下载地址：<a href="http://spark.apache.org/downloads.html">点击进入下载页面</a></p>
<p>我安装的版本是：0.9.1版本，源码包为：spark-0.9.1.tgz</p>
<p>Spark有以下四种运行模式：</p>
<ul>
<li><strong>local</strong>：本地单进程模式，用于本地开发测试Spark代码</li>
<li><strong>standalone</strong>：分布式集群模式，Master-Worker架构，Master负责调度，Worker负责具体Task的执行</li>
<li><strong>on yarn/mesos</strong>：运行在yarn/mesos等资源管理框架之上，yarn/mesos提供资源管理，spark提供计算调度，并可与其他计算框架(如MapReduce/MPI/Storm)共同运行在同一个集群之上</li>
<li><strong>on cloud(EC2)</strong>：运行在AWS的EC2之上</li>
</ul>
<p>Spark支持local模式和cluster模式，local不需要安装mesos；如果需要将spark运行在cluster上，需要安装mesos。</p>
<hr/>
<h2>三、安装部署</h2>
<p>先把Scala和git装好，因为之后的sbt/sbt执行的是使用spark自带的sbt编译/打包。</p>
<pre><code>sudo apt-get update
sudo apt-get install scala
</code></pre>
<p>我们需要做的其实就两步，解压缩与编译。</p>
<pre><code>$tar -zxvf spark-0.9.1.tgz -C /home/hadoop/software/spark
$cd /home/hadoop/software/spark/spark-0.9.1
$sbt/sbt assembly
</code></pre>
<p>这一段时间等的会比较长，耐心些。</p>
<hr/>
<h2>四、检验测试</h2>
<p>Spark有两种运行模式。</p>
<h3>4.1 Spark-shell</h3>
<p>此模式用于interactive programming，具体使用方法如下(先进入bin文件夹)。</p>
<pre><code>$ ./spark-shell
</code></pre>
<p>出现如下信息：</p>
<pre><code>    14/05/10 14:18:23 INFO HttpServer: Starting HTTP Server
    Welcome to
          ____              __
         / __/__  ___ _____/ /__
        _\ \/ _ \/ _ `/ __/  '_/
       /___/ .__/\_,_/_/ /_/\_\   version 0.9.1
          /_/

    Using Scala version 2.10.3 (Java HotSpot(TM) Server VM, Java 1.7.0_51)
    Type in expressions to have them evaluated.
    Type :help for more information.
    14/05/10 14:18:34 INFO Slf4jLogger: Slf4jLogger started
    14/05/10 14:18:34 INFO Remoting: Starting remoting
    14/05/10 14:18:34 INFO Remoting: Remoting started; 
    ……
    Created spark context..
    Spark context available as sc.
</code></pre>
<p>然后输入如下信息：</p>
<pre><code>scala&gt; val days = List("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")
days: List[java.lang.String] = List(Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday)

scala&gt; val daysRDD = sc.parallelize(days)
daysRDD: spark.RDD[java.lang.String] = ParallelCollectionRDD[0] at  parallelize at &lt;console&gt;:14

scala&gt; daysRDD.count()
</code></pre>
<p>在经过一系列计算后，显示如下信息：</p>
<blockquote><p>res0: Long = 7</p>
</blockquote>
<h3>4.2 Run脚本</h3>
<p>用于运行已经生成的jar包中的代码，如Spark自带的example中的SparkPi.</p>
<pre><code>$./bin/run-example org.apache.spark.examples.SparkPi local[3]
</code></pre>
<p>local代表本地，[3]表示3个线程跑。</p>
<p>计算结果如下：</p>
<blockquote><p>Pi is roughly 3.1444</p>
</blockquote>
<hr/>
<h2>五、学习建议</h2>
<p>在配置过程中看到他人给的一些建议，于是搜集起来供以后学习参考。</p>
<ul>
<li>如何写一些spark application？</li>
</ul>
<p>多看一些spark例子，如：<a href="http://www.spark-project.org/examples.html">http://www.spark-project.org/examples.html</a>,<a href="https://github.com/mesos/spark/tree/master/examples">https://github.com/mesos/spark/tree/master/examples</a></p>
<ul>
<li>遇到问题怎么办？</li>
</ul>
<p>首先是google遇到的问题，如果还是解决不了就可以到spark google group去向作者提问题：<a href="http://groups.google.com/group/spark-users?hl=en">http://groups.google.com/group/spark-users?hl=en</a></p>
<ul>
<li>想深入理解spark怎么办？</li>
</ul>
<p>阅读spark的理论paper:<a href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2011/EECS-2011-82.pdf">http://www.eecs.berkeley.edu/Pubs/TechRpts/2011/EECS-2011-82.pdf</a></p>
<p>阅读spark源代码：<a href="https://github.com/mesos/spark">https://github.com/mesos/spark</a></p>
<hr/>
<h2>声明</h2>
<p>本文已经成功投稿至36大数据网站，于2014-05-13发表。<a href="http://www.36dsj.com/archives/8001">链接地址</a></p>
