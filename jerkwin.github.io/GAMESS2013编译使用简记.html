<h2><a href="https://github.com/jerkwin/jerkwin.github.io//blob/master/_posts/2014-02-23-GAMESS2013编译使用简记.md">仓库源文</a>，<a href="https://jerkwin.github.io/2014/02/23/GAMESS2013编译使用简记">站点原文</a></h2>
<hr/>
<p>layout: post
 title: GAMESS2013编译使用简记
 categories:</p>
<ul>
<li>科
tags:</li>
<li>gamess</li>
</ul>
<hr/>
<h2>2014-02-23 21:14:14 初稿</h2>
<h2>2014-03-08 12:09:09 修订</h2>
<h3>编译</h3>
<ol>
<li><p>解压 <code>tar -xzvf gamess-current.tar.gz</code></p>
</li>
<li><p>环境配置 <code>./config</code></p>
<ol>
<li>目标平台: 利用<code>uname -a</code>可知, 一般为linux64</li>
<li>源码路径: 当前路径</li>
<li>安装路径: 当前路径, 也可使用其他, 如<code>./bin</code></li>
<li>版本号: 用作不同版本标识, 默认00</li>
<li>FORTRAN编译器: 根据系统选择. 若使用ifort, 须选择版本</li>
<li>数学库: 若使用ifort, 则选MKL, 并指定路径如<code>/share/apps/intel/composer_xe_2013.2.146/composer_xe_2013.2.146/mkl</code>.<br/>
 若该路径下不存在多个版本MKL, 下一步直接skip</li>
<li>激活代码编译: 编译actvte.f, 成功, 此程序用于后面编译前源代码的处理<br/>
<code>ifort -o /home/jicun/GMS/tools/actvte.x actvte.f</code></li>
<li>网络设置: 并行, 选择mpi</li>
<li>mpi库: 若使用ifort, 可用impi, 并设定路径如<code>/share/apps/intel/impi/4.1.0.030</code></li>
<li><p>使用LIBCCHEM加速MP2和CCSD(T)与否, 须GPU支持. 一般不使用</p>
<p>配置成功, 会在安装路径内产生install.info和Makefile两个文件, 并提示参看<code>./machines/readme.unix</code></p>
<p>实际上, <code>config</code>脚本只适合用于第一次配置环境, 得到install.info和Makefile后, 若想更改设置再次编译, 直接编辑已有的install.info和Makefile即可.
下面是两个文件的内容</p>
<p>install.info(GMS_MKL_VERNO为12不影响链接时使用MKL 13.0)</p>
&lt;pre class="line-numbers" data-start="0"&gt;&lt;code class="language-bash"&gt;# Language: bash
#!/bin/csh
#   compilation configuration for GAMESS
#   generated on compute1131
#   generated at Sat Mar  8 12:22:27 CST 2014
setenv GMS_PATH            /home/jicun/GMS
setenv GMS_BUILD_DIR       /home/jicun/GMS
#         machine type
setenv GMS_TARGET          linux64
#         FORTRAN compiler setup
setenv GMS_FORTRAN         ifort
setenv GMS_IFORT_VERNO     13
#         mathematical library setup
setenv GMS_MATHLIB         mkl
setenv GMS_MATHLIB_PATH    /share/apps/intel/composer_xe_2013.2.146/composer\_xe\_2013.2.146/mkl/lib/intel64
setenv GMS_MKL_VERNO       12
#         parallel message passing model setup
setenv GMS_DDI_COMM        mpi
setenv GMS_MPI_LIB         impi
setenv GMS_MPI_PATH        /share/apps/intel/impi/4.1.0.030
#         LIBCCHEM CPU/GPU code interface
setenv GMS_LIBCCHEM        false
&lt;/code&gt;&lt;/pre&gt;<p>若使用sockets, 最后几行改为</p>
&lt;pre class="line-numbers" data-start="0"&gt;&lt;code class="language-bash"&gt;# Language: bash
#         parallel message passing model setup
setenv GMS_DDI_COMM        sockets
&lt;/code&gt;&lt;/pre&gt;<p>Makefile</p>
&lt;pre class="line-numbers" data-start="0"&gt;&lt;code class="language-bash"&gt;# Language: bash
GMS_PATH = /home/jicun/GMS
GMS_VERSION = 00
GMS_BUILD_PATH = /home/jicun/GMS
include $(GMS_PATH)/Makefile.in
&lt;/code&gt;&lt;/pre&gt;</li>
</ol>
</li>
<li><p>编译 <code>./compall &gt;&amp; compall.log</code>. 请耐心.<br/>
 ifort的编译选项, 可在comp文件1870行修改,<br/>
 可增加<code>-xHost -axAVX -opt_report -static</code><br/>
 优化默认为<code>-O2</code>, 也可使用<code>-O3 -fast</code>, 但须测试</p>
</li>
<li><p>编译ddi库: <code>cd ddi; ./compddi</code></p>
</li>
<li><p>链接 <code>./lked GAMESS 00 &gt;&amp; lked.log</code><br/>
 静态链接, 可在1393行增加<code>-static-intel</code><br/>
 使用<code>-static</code>可能会选项<code>-lrt</code>冲突, 从而导致链接失败</p>
</li>
</ol>
<h3>使用</h3>
<p><code>rungms File.inp #version Ncpu PPN</code></p>
<p>脚本中需要修改的地方</p>
<ul>
<li>62-65: 设定mpi和路径</li>
<li><p>95: 若使用的PBS系统不能自动创建以作业号为名的文件夹, 须修改为</p>
&lt;pre class="line-numbers" data-start="0"&gt;&lt;code class="language-bash"&gt;# Language: bash
  set SCR=$SCR/$PBS_JOBID
  mkdir -p $SCR
  &lt;/code&gt;&lt;/pre&gt;</li>
<li><p>828: 设定<code>mpiexec</code>路径</p>
</li>
<li>843: 此行以后, 清除运行过程中产生的中间文件, 可惜同时也会把输出文件清除, 可添加<code>exit</code>语句, 保留所有中间文件, 自行处理.</li>
</ul>
<p>实际上, <code>rungms</code>脚本为照顾不同系统使用, 添加了很多与系统相关的代码, 若只在特定系统下运行, 可将其他系统相关的代码删除.</p>
<h3>IMPI并行时可能的问题</h3>
<ul>
<li><p><a href="http://software.intel.com/en-us/forums/topic/393416">open_hca: device mlx4_0 not found</a> <a href="http://software.intel.com/en-us/forums/topic/379510">Unwanted output</a></p>
</li>
<li><p>计算完成后, 最终输出显示</p>
<p><code>gamess.00.x: dapl/common/dapl_evd_util.c:1187: dapli_evd_cqe_to_event:</code></p>
<p><code>Assertion !"Invalid Operation type"' failed.</code></p>
<p>同时dump一个很大的core.xxx. 原因未明. 可能与IMPI库有关. 可利用<code>ulimit -c 0</code>关闭吐核.</p>
</li>
<li><p><a href="http://software.intel.com/en-us/forums/topic/287135">undefined references when compiling with -fast on Intel MPI</a></p>
</li>
<li><p><code>rungms</code>脚本中, Nproc = Nmpi = 2 Ncpu = 2 Ncore, 源于GMS运行两个进程, 一个计算, 另一个做data serve</p>
</li>
<li><p>GMS使用内存计算方法:  MEM(GB/CPU) = [MW + DDI/Ncpu]/128</p>
</li>
</ul>
