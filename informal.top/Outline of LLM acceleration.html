<h2><a href="https://github.com/wa008/wa008.github.io/blob/master/_posts/2024-11-11--1111-LLM-acceleration-1021-summary.md">仓库源文</a>，<a href="https://informal.top/2024/11/11/-1111-LLM-acceleration-1021-summary">站点原文</a></h2>
<h2>Summary</h2>
<h3>Methods</h3>
<p>There are two main methods to acclerate LLM and another tricky methods</p>
<ul>
<li>low-rank: reduce dimension of matrix</li>
<li>block: compute matrix with block</li>
<li>trick: update model structure or change training process</li>
</ul>
<p>already read papers: 12</p>
<h3>Reference</h3>
<ul>
<li><a href="https://github.com/facebookresearch/xformers">xformers</a>: collection of optimized transformers</li>
<li><a href="https://ben.bolte.cc/fast-attention">fast attention collection</a></li>
<li><a href="https://unsloth.ai/blog/mistral-benchmark">unsloth</a></li>
<li><a href="https://github.com/galeselee/Awesome_LLM_System-PaperList">awesome LLM system</a></li>
</ul>
<h2>Categories</h2>
<h3>Low-rank</h3>
<h4><a href="https://arxiv.org/abs/2106.09685">LoRA</a></h4>
<p>Low-Rank of large matrices when fine-tune</p>
<p>informaiton</p>
<ul>
<li>Jun 2021</li>
<li>70%</li>
<li><a href="https://informal.top/posts/paper-LoRA/">note</a></li>
</ul>
<p>reference</p>
<ul>
<li>Measuring the Intrinsic Dimension of Objective Landscapes</li>
<li>Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning</li>
</ul>
<h4><a href="https://arxiv.org/abs/2006.04768">Linformer</a></h4>
<p>SVD decomposition for large QKV projection matrices to reduce required memory</p>
<ul>
<li>Jun 2020</li>
<li>30%</li>
<li><a href="https://github.com/lucidrains/linformer">code</a>: hold</li>
</ul>
<h4><a href="https://arxiv.org/abs/2009.14794">Performers</a></h4>
<p>low-rank projection with a novel method named FAVOR</p>
<ul>
<li>Sep 2020</li>
<li>10%</li>
<li><a href="https://github.com/lucidrains/performer-pytorch">code</a>: hold</li>
</ul>
<h3>Block</h3>
<h4><a href="https://arxiv.org/abs/2205.14135">FlashAttention</a></h4>
<p>Matrices multiplication by blocks</p>
<ul>
<li><a href="https://informal.top/posts/notes-of-FlashAttention/">note</a></li>
<li>May 2020</li>
<li>70%</li>
<li><a href="https://github.com/Dao-AILab/flash-attention">cod</a>: updating</li>
</ul>
<h4><a href="https://arxiv.org/abs/2112.05682">Self-attention Does Not Need O(n2) Memory</a></h4>
<p>attention calculation with blocks</p>
<ul>
<li>Dec 2021</li>
<li>70%</li>
</ul>
<h4>+ <a href="https://arxiv.org/abs/2311.01282">FlashDecoding++</a></h4>
<p>FlashDecoding++: Faster Large Language Model Inference on GPUs, three parts</p>
<ul>
<li><p>Softmax with block and Unified Maximum Value, result of block softmax can be directly used and merging is unnecesary. Optimized from FlashAttention.</p>
<p><img alt="scalability" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/informal.top/images/2024/1113-01.png"/>{: width="600"}</p>
</li>
<li><p>Flat GEMM(small batch size when reference) Optimization with Double Buffering. [didn't understand]</p>
</li>
<li><p>Heuristic Dataflow with Hardware Resource Adaption, choose difference optimizaiton methods for different M value(batch size and sequence length) [didn't total understand]</p>
<p><img alt="scalability" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/informal.top/images/2024/1113-02.png"/>{: width="600"}</p>
</li>
<li><p>reference</p>
<ul>
<li>cuBLAS / <a href="https://github.com/NVIDIA/cutlass">CUTLASS</a></li>
<li>flat GEMM: method in current paper</li>
<li>fastGEMV</li>
</ul>
</li>
<li>No code(2024.11)</li>
</ul>
<h3>Basic</h3>
<ul>
<li><a href="https://informal.top/posts/validated-example/">Fast Cross Entropy Loss</a></li>
<li><a href="https://informal.top/posts/try-of-torchview-to-accelerate-finetune-new/">Matrix Multiplication</a></li>
<li><a href="https://unsloth.ai/blog/mistral-benchmark">Reduce data upcasting</a></li>
<li><a href="https://unsloth.ai/blog/mistral-benchmark">Bitsandbytes bfloat16</a></li>
</ul>
<h3>Parallelization</h3>
<p><strong>New Solutions on LLM Acceleration, Optimization, and Application</strong></p>
<p>1) Medusa: output top-k predictions for next multiple positions parallelly through adding LM heads for next several positons, which can reduce inference latency.</p>
<p><img alt="scalability" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/informal.top/images/2024/1112-01.png"/>{: width="600"}</p>
<p>2) SnapKV: compress KV cacha for long sequence tasks</p>
<h3>Infrastructure</h3>
<p><strong><a href="https://github.com/triton-lang/triton">triton</a></strong></p>
<ul>
<li>An alternative language for cuda, designed for deep neural network</li>
<li>published in 2019, purchase by OpenAI</li>
<li>reasons why it's great<ul>
<li>designed for deep neural network</li>
<li>open-source, active project in Github </li>
<li>clients, like unsloth, other in Github issues</li>
<li>friendly to use and implentment, adding them into current Python code, Good to start</li>
<li>support for other chips</li>
</ul>
</li>
</ul>
<p><strong>Hardware Acceleration of LLMs: A comprehensive survey and comparison</strong>
Simple introduce and compare different hardware acceleration method in terms of efficiency and performance</p>
<ul>
<li>collect all method from 2020-2024</li>
<li>comparison with the same process technology</li>
<li>different choose for both efficiency and performance</li>
</ul>
<h3>Trick</h3>
<ul>
<li>Inference with Reference: Lossless Acceleration of Large Language Models: copy reference to inference because there many same text sentence betwee them to accelerate inference</li>
<li>SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention: select different experts matrices for every head in attention by input content to reduce computation and memory usage.<ul>
<li>published: 2024</li>
</ul>
</li>
<li><a href="https://arxiv.org/abs/2402.17812">DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation</a>:<ul>
<li>Drop Backward propagation based on sensitivity which is the difference between Backward update and not update. great idea!</li>
<li>change model constructure to have a 2^n submodels when drop some submodels</li>
<li>published: 2024</li>
</ul>
</li>
</ul>
<p><img alt="scalability" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/informal.top/images/2024/1123-01.png"/>{: width="800"}</p>
<h2>To Read</h2>
<p>Quantization</p>
<ul>
<li><a href="https://leimao.github.io/article/Neural-Networks-Quantization/">Quantization for Neural Networks</a></li>
</ul>
<p>Optimizer</p>
<ul>
<li><a href="https://arxiv.org/abs/2412.05270">APOLLO: SGD-like Memory, AdamW-level Performance</a></li>
</ul>
<p>RNN</p>
<ul>
<li><a href="https://arxiv.org/abs/2404.05892">RWKV</a>: RWKV is an RNN with transformer-level LLM performance</li>
</ul>
<p>Trick</p>
<ul>
<li><a href="https://arxiv.org/abs/2105.14103">An Attention Free Transformer</a></li>
</ul>
<p>Long sequence</p>
<ul>
<li>IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs</li>
</ul>
<p>2:4</p>
<ul>
<li>Accelerating Transformer Pre-training with 2:4 Sparsity</li>
</ul>
<p>Pruning</p>
<ul>
<li>Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning</li>
</ul>
<p>cache</p>
<ul>
<li>Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference</li>
</ul>
<p>trade-off</p>
<ul>
<li>AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration</li>
</ul>
<p>PE</p>
<ul>
<li><a href="https://arxiv.org/abs/2104.09864">RoPE</a></li>
</ul>
