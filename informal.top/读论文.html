<h2><a href="https://github.com/wa008/wa008.github.io/blob/master/blog/_posts/2022-04-03-读论文.md">仓库源文</a>，<a href="https://informal.top/blog/2022/04/03/读论文">站点原文</a></h2>
<hr/>
<p>layout: default</p>
<h2>title: 读论文</h2>
<h1>1、<a href="https://arxiv.org/abs/1705.02364">Sentence Representations from Natural Language Inference Data</a></h1>
<p>大意：利用有监督的自然语言推断数据训练句向量，利用句向量的拼接、求差做分类进行训练。求得的句子向量在情感分析等12项任务上进行测试，句向量+dense进行分类。对比了RNN、LSTM、GRU、CNN、self-attention等句向量表征结构，BiLSTM + max pooling 效果最好。相比之前最好的无监督 SkipThought 方法有明显提升。</p>
<p>有意思的点</p>
<ol>
<li>BiLSTM-max pooling 相比 mean pooling 在训练任务上提升6个点，迁移任务上提升2个点。隐层应该是在学习不同的信号，通过max 来传递到下一层。</li>
<li>动态调整学习率，当验证集 acc 下降时，降低学习率。attention 的论文中也有动态调学习率的做法。</li>
<li>Adam 相比 SGD 速度更快，但效果更差，因为Adam能更好的捕捉训练任务细节，在迁移任务上表现的不好。存疑。</li>
<li>训练任务上表现得好，不一定在迁移任务上表现就好。比如attention 比 BiLSTM 在训练任务上更好，但迁移任务上更差。猜测是attention 更关注具体的训练任务，而不是学习到一个更通用的句向量。</li>
</ol>
<p>参考资料</p>
<ol>
<li><a href="https://arxiv.org/abs/1506.06726">SkipThought vector</a> 训练方法</li>
<li><a href="https://arxiv.org/abs/1602.03483">FastSent</a> 无监督句向量训练</li>
<li><a href="https://arxiv.org/abs/1607.06450">layer-norm</a></li>
</ol>
<p>2022-01-16</p>
<h1>2、<a href="https://arxiv.org/abs/1506.06726">SkipThought vector</a></h1>
<p>提出了无监督训练句向量的一种方法，bow+seq2seq 的训练方法，利用中间的 sentence 来预测周围的 sentence。
<img alt="78944-hjmbg9o5ed9.png" src="/images/2022/01/2802567642.png"/>
seq 模型文中使用的是 GRU
2022-01-23</p>
<h1>3、<a href="https://arxiv.org/abs/1602.03483">FastSent</a></h1>
<p>对比了几种训练句向量方法的效果。但具体方法还要看对应论文。
结论：句向量的评估和使用主要有两种，加入下游有监督任务 和 距离度量求相似度。更深更复杂的模型在有监督的任务中往往效果更好；shallow log-linear model 在空间距离度量中效果最好。
2022-02-23</p>
<h1>4、<a href="https://arxiv.org/abs/1607.06450">layer normalization</a></h1>
<p>对比之前提出的 batch normalizaiton ，提出了 layer norm。
batch norm：1）相比全量 norm，资源消耗少，速度更快。2）随机性相当于给训练过程添加了正则，提升鲁棒性。
layer norm：1）解决 batch norm 应用到 RNN 上时，测试样本长度比训练样本长度更长，无法获取 norm 参数的问题。2）没有 batch 的限制，能用于 online learning task 和 batch 较小的场景。3）试验验证 laryer norm 在 RNN 结构的模型中效果较好，提升时效和效果。
2022-03-05</p>
<h1>5、softmax 变种</h1>
<p>主要增加了当前类别的识别，提升难度。（提高梯度？）
<a href="https://www.jianshu.com/p/06cc3f84aa85">https://www.jianshu.com/p/06cc3f84aa85</a>
2022-04-03</p>
