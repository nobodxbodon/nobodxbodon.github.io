<h2><a href="https://github.com/wa008/wa008.github.io/blob/master/_posts/2025-06-23-experience-about-llm-fine-tune.md">仓库源文</a>，<a href="https://informal.top/2025/06/23/experience-about-llm-fine-tune">站点原文</a></h2>
<p>Personal experience</p>
<ul>
<li>care the changing of loss/reward and test dataste performance, ensure they change with same trend, otherwise, reward hacking / invalid loss function appear</li>
<li>adjust learning-rate and regularization penalty by changing of loss with training steps</li>
<li>verfy idea with pure comparing experiment (scientific control)</li>
<li>retry same experiment to exclude influence of random</li>
<li>only change hyper-parameter when training process doesn't work well</li>
<li>make llm output process before output final answer</li>
<li>try to process regulazation by code rather than in prompt as much as possible</li>
</ul>
