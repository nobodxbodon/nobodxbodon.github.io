<h2><a href="https://github.com/wa008/wa008.github.io/blob/master/_posts/2025-06-23-experience-about-llm-fine-tune.md">仓库源文</a>，<a href="https://informal.top/2025/06/23/experience-about-llm-fine-tune">站点原文</a></h2>
<h2>Prompt</h2>
<p>Prompt is the most direct way to influence response, tips for good prompt:</p>
<ul>
<li>Clear instruction about our demand</li>
<li>Provide necessary context, role, tone, format</li>
<li>guide LLM output reasoning process before final answer</li>
<li>More instructions, less constraints</li>
<li>Exampler can ensure the constructure is as same as example</li>
</ul>
<p>The purpose of prompt in post-training is building best reasoning architecture in response, training could optimize other detailed contents in response</p>
<h2>One shot learning</h2>
<ul>
<li>One example in prompt (one shot) can ensure output architecture is as same as example. </li>
<li>In binary-class tasks, one example probably result in answer trend to that in example.</li>
<li>In binary-class tasks, two examples probably result in unstable of answer. </li>
</ul>
<h2>Experience / Conclusion</h2>
<ul>
<li>Model size of model to train is related with information volume of datasets<ul>
<li>Larger model need more information volum to fine-tune </li>
<li>We can use small-size model to test whether the solution is feasible with low cost</li>
</ul>
</li>
<li>Smaller model has better stability of response</li>
<li>Amount of data is positively correlated with model performance</li>
<li>Quality of data is positively correlated with model performance</li>
</ul>
<h2>Training process</h2>
<p>The purpose of training is to ensure performance on test dataset increase in stable trend and range.</p>
<ul>
<li>ensure the loss/reward curve and performance on test dataset change with same trend<ul>
<li>If performance of test dataset don't increase as expected, overfitting / reward hacking occur. </li>
<li>If loss cannot reduce as expected, there is something wrong in training dataset</li>
</ul>
</li>
<li>adjust learning-rate and regularization penalty by observing loss curve with training steps<ul>
<li>If loss decreased slowly, raise LR. If loss curve is unstable, lower LR.</li>
<li>When overfitting occur, raise regularization penalty. If loss can not increase in late stages, try to lower it.</li>
</ul>
</li>
<li>verify idea with pure control experiment</li>
<li>retry total same experiment to exclude influence of random</li>
<li>make LLM output intermediate reasoning process before output final answer</li>
<li>For specific task, put as much logit as in rule rather than in prompt if possible</li>
<li>Thinking rewrad is valid and necessary in RL</li>
<li>model reward even multi model reward is helpful in RL</li>
</ul>
<h2>Multi-stage training</h2>
<p>The purpose of dataset is to provide information to model to learn, in the late stages, model already know more than before, more extra information should be sent to model. So, in the late stage, we should increase information diversity</p>
<p>How to increase information diversity:</p>
<ul>
<li>put hard samples in late training stages</li>
<li>increase temperature in late training stages for GRPO</li>
<li>select samples which have unstable results for GRPO</li>
</ul>
<h2>Reference</h2>
<ul>
<li><a href="https://drive.google.com/file/d/1AbaBYbEa_EbPelsT40-vj64L-2IwUJHy/view">Google prompt engineering</a></li>
<li><a href="https://cobusgreyling.medium.com/six-key-elements-of-ai-agent-prompt-engineering-d33a1ff89890">Six Key Elements of AI Agent Prompt Engineering</a></li>
</ul>
