<h2><a href="https://github.com/wa008/wa008.github.io/blob/master/_posts/2023-04-25-auto-driver.md">仓库源文</a>，<a href="https://informal.top/2023/04/25/auto-driver">站点原文</a></h2>
<hr/>
<p>layout: default</p>
<h2>title: 自动驾驶的小玩具</h2>
<p>从多个方面来讲，我一直觉得强化学习是解决自动驾驶问题的一个非常好的思路，于是就动手做了一个小demo，总耗时一个月左右，现在遇到了瓶颈，来记录一下</p>
<h3>原由</h3>
<p><a href="http://informal.top/2022/11/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B.html">强化学习</a>在之前的博客中有介绍过，如果不引入神经网络，他是个可解释的模型，缺点是，不容易用可枚举的方式来表达当前的状态，但某些场景下，尽量用可枚举的方式表达当前的状态，强化学习就可行了。</p>
<p>以自动驾驶为例，当前的状态就是车所看到的所有信息，比如路灯、正前方、侧方的信息等信息，可以用基础的基础的计算视觉技术做识别，识别出车周围物体都有啥，比如路障、行人、自行车，把这些信息作为当前的状态，就能用强化学习建模了。</p>
<h3>建模</h3>
<p>以当前的状态为key，不断优化不同key下应该采取的最优的action（行为），所有的key-action 对，存在一个表里，就是强化学习中的 q_table；通过不断模拟action，获得反馈结果，来不断修正每个key对应的 action，也就是优化 q_table，只到 q_table 稳定</p>
<h3>优点</h3>
<p>可解释</p>
<p>可控</p>
<p>可模拟</p>
<p>其实可以类比AlphaGo，AlphaGo在和棋手做对抗，自动驾驶就是多个车和这个现实世界做对抗，通过不断模拟，优化这个系统</p>
<h3>实现</h3>
<p>原始的强化学习框架参考莫凡大佬的demo，可视化用 tkinter实现，已实现的功能：</p>
<ol>
<li>强化学习框架</li>
<li>路线的可视化</li>
<li>提前模型，多辆车之间不能碰撞；主要是来避免无意义的action，长期来看，可以把碰撞作为一种惩罚</li>
</ol>
<p>example:</p>
<p><a href="https://user-images.githubusercontent.com/29834520/232979083-2706dddb-43e2-41fd-ab37-ade04a40b127.mp4">https://user-images.githubusercontent.com/29834520/232979083-2706dddb-43e2-41fd-ab37-ade04a40b127.mp4</a></p>
<p>github: <a href="https://github.com/wa008/reinforcement-learning">https://github.com/wa008/reinforcement-learning</a></p>
<h3>todo</h3>
<p>tkinter 做可视化，需要保证基础的行列不超过25，就限制了只能有10条路左右，再大单机就跑不动了，所以需要效果更好的可视化工具，通过网友求助，已知的有 SumoPy，还有两个挺好的工具在网友的评论里，这会儿去看的时候已经被删了，emmm，<del>晚上回家找找浏览器的搜索记录，看还在不</del></p>
<p>性能问题解决之后，就能继续了</p>
<ol>
<li>使用更大的地图</li>
<li>加宽路线，改双向车道，设计路口转弯规则</li>
<li>加快车的速度</li>
<li>添加路人、自行车</li>
<li>添加车能看到的信息，比如周围一定距离的物体</li>
</ol>
<p>最近想看看大语言模型开源的几个项目了，过段时间再回来搞这个</p>
