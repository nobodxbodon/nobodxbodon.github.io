<h2><a href="https://github.com/wa008/wa008.github.io/blob/master/_posts/2024-10-23-notes-of-FlashAttention.md">仓库源文</a>，<a href="https://informal.top/2024/10/23/notes-of-FlashAttention">站点原文</a></h2>
<hr/>
<p>title: Notes of flash-attention
tags: [flash-attention, LLM, attention]
math: true</p>
<h2>comments: true</h2>
<h2>Backgroud</h2>
<p>There are two common kinds of bound which limited the speed of training in deep learning.</p>
<ul>
<li>Memeory-bound: time spent on memeory-access is bottlenecked</li>
<li>Computation-bound: time spent on computation is bottlenecked</li>
</ul>
<h2>Introduction</h2>
<p>Inspiration: just tiling?</p>
<p>Description: Split the <code>Q</code>, <code>K</code>, <code>V</code> into blocks and calculate output matrice <code>O</code> by block to avoid the store of softmax intermediate matrice which has size of <code>seq_len * seq_len</code> in HBM memory, which reduce the memory-bound, as a result, required memeroy of attention is almost linearly with senquence length of sentence.</p>
<p><img alt="scalability" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/informal.top/images/2024/10/23-01.png"/>{: width="800"}</p>
<p>Novelty: Making attention memory-efficient</p>
<h2>Benefits</h2>
<p>1) Faster model training, due to use SRAM more?</p>
<p>2) Higher quality modles in long sequence tasks.</p>
<p>3) New benchmarking attention, both faster and memory-efficient than existing attention method (2022.5)</p>
<p>4) Block-Sparse, only compute for no-zero block for attention_mask</p>
<h2>Further information</h2>
<p>1) Algorithm</p>
<p><img alt="Algorithm" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/informal.top/images/2024/10/23-02.png"/>{: width="600"}</p>
<p>2) Flash-attention has higher FLOP count compared to standard attention but is still faster because attention is memory-access-bound and falsh-attention has fewer HBM accesses.</p>
<h2>Not finished</h2>
<ul>
<li>Math about how to recalculate backward grad</li>
</ul>
<h2>Summary</h2>
<p>1) Purpose of auther?</p>
<p>Using blocks method to make memory-bound network memroy-efficient and faster</p>
<p>2) Key of new method?</p>
<p>Using block to avoid the store of large softmax attention maxtrice.</p>
<p>3) What is useful for me?</p>
<p>Using block to trade-off between memory and computation, which can be use based on memory-bound or computation-bound</p>
<p>4) What references is necessary to read?</p>
<ul>
<li><a href="https://arxiv.org/abs/2001.04451">Reformer: The Efficient Transformer</a>: sparse-approximation to reduce the computation and memory requirements</li>
<li><a href="https://arxiv.org/abs/2009.14794">Rethinking Attention with Performers</a>: low-rank approximation to do same thing</li>
</ul>
<p>5) new idea</p>
<p>What is the bottleneck right now, memory or computation? what is it for different models or module parts?</p>
