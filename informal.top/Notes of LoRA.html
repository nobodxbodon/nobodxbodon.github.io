<h2><a href="https://github.com/wa008/wa008.github.io/blob/master/_posts/2024-10-20-paper-LoRA.md">仓库源文</a>，<a href="https://informal.top/2024/10/20/paper-LoRA">站点原文</a></h2>
<h2>Introduction</h2>
<p>Inspiration: the change in weights during model adaptation have a low "intrinsic rank"</p>
<p>Description: Change small matrices <code>A</code> and <code>B</code> when fine-tune, adding <code>A * B</code> to weight <code>W</code>, which significantly reduce the trainable number of parameters because <code>r &lt;&lt; d</code></p>
<p><img alt="scalability" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/informal.top/images/2024/10/20-03.png"/>{: width="300"}</p>
<p>Novelty: Reducing Fine-tuen cost without additional latency and quality reduction or changing input consturction at the same time</p>
<h2>Benefits</h2>
<p>1) Efficiently switch models for different target tasks through switching LoRA matrices
2) Reduce consuming time and hardware requirement when Fine-tune
3) Without any inference lantency
4) Orthogonal with other adaptation method</p>
<h2>Further information</h2>
<p>1) LoRA have better scalability and performances</p>
<p><img alt="scalability" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/informal.top/images/2024/10/20-01.png"/>{: width="600"}</p>
<p>2) Adapting more weight matrices is preferable than adapting a single type of weights with a larger rank, 2 or 4 is a great option</p>
<p><img alt="scalability" src="/Users/xuanwu/work/聚聚/中文博客集锦/源数据/博客聚合/informal.top/images/2024/10/20-02.png"/>{: width="600"}</p>
<p>3) Similiarity of LoRA vectors among different rank(r) is higher when dimention is small, which prove (1)low-demension consists more information, (2) small r is enough</p>
<p>4) Matrices of LoRA is similiar with original weight mtraices, LoRA is a amplification of original information.</p>
<h2>Summary</h2>
<p>1) Purpose of auther?
reduce the cost of fine-tuen without any loss</p>
<p>2) Key of new method?
change of adaptation have a low "intrinsic rank"</p>
<p>3) What is useful for me?
large Matrices in LLM have a low "intrinsic rank"?
New fine-tune method</p>
<p>4) What references is necessary to read?
Where "intrinsic rank" comes from?</p>
<ul>
<li><a href="https://arxiv.org/abs/1804.08838">Measuring the Intrinsic Dimension of Objective Landscapes</a></li>
<li><a href="https://arxiv.org/abs/2012.13255">Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning</a></li>
</ul>
<blockquote><p>1-4 from Andrew NG</p>
</blockquote>
<p>5) new idea</p>
<p>rand-deficiency of delta_w suggests that w could be rank-deficient as well, which can be a source of inspiration of future of works.</p>
