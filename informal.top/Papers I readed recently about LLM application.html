<h2><a href="https://github.com/wa008/wa008.github.io/blob/master/_posts/2025-06-22-papers-about-LLM-applications.md">仓库源文</a>，<a href="https://informal.top/2025/06/22/papers-about-LLM-applications">站点原文</a></h2>
<p>How much do LLM memorize?</p>
<ul>
<li>key definition<ul>
<li>unintended memorization: memorize a specific dataset</li>
<li>generalization (intended memorization): contains about the true data-generation process</li>
<li>calculation method: by information entropy and mutual information</li>
</ul>
</li>
<li>double desent appear on the changing points from unintended memorization into generalization</li>
<li>GPT-models store 3.6bits data per parameters</li>
<li>value of float32 is 9% higher than float16</li>
</ul>
<p>Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</p>
<ul>
<li>trade-off between pre-train model size and inference-time(inference length)</li>
<li>performance can ouperform 14x size model</li>
<li>performance is better in easy and medium problem, judge easy, medium or hard question based on the pass rate</li>
<li>two ways to increase inference-compute-time<ul>
<li>best-of-N: sample N outputs parallel and choose the best one based on learned verifier or reward function. </li>
<li>revise response: revise original response</li>
</ul>
</li>
</ul>
<p>Prolonged Reinforcement Learning</p>
<ul>
<li>tempurature: increase tempurate to avoid entropy collapse</li>
<li>decoupled clip to increase exploration space</li>
<li>dynamic sampling: erase all truely right or wrong samples</li>
<li>calculate loss from sample level into token level -- DAPO</li>
<li>KL-regularization and reference model reset</li>
</ul>
<p>illusion of thinking</p>
<ul>
<li>for hanio tasks<ul>
<li>lower performance in simple-level question for reasoning model than general mdoel because it get wrong answer when thinking even already get correct answer [over thinking]</li>
<li>better performance in medium-level question</li>
<li>zero-performance in hard question</li>
</ul>
</li>
</ul>
<p>Gemini 2.5 tech report</p>
<ul>
<li>dataset<ul>
<li>ensure dataset quality</li>
<li>fliter and drop duplicates</li>
</ul>
</li>
<li>post-training<ul>
<li>verifiable reward and model-base generative rewards to provide sophisticated and scaleable feedback signals<ul>
<li>verifiable reward</li>
<li>model-base: <em>more sophisticated and scaleable feedback signals</em></li>
</ul>
</li>
<li>update LR-method to improve the stability of training</li>
<li>result: learning in complex space</li>
</ul>
</li>
</ul>
