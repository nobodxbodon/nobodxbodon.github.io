<h2><a href="https://github.com/wa008/wa008.github.io/blob/master/_posts/2022-05-31-孪生网络做 query 相似度任务.md">仓库源文</a>，<a href="https://informal.top/2022/05/31/孪生网络做-query-相似度任务">站点原文</a></h2>
<h1>起因</h1>
<p>工作中做一个 query 相似度任务时，偶然看到孪生网络的一片经典论文，<a href="https://aclanthology.org/W16-1617.pdf">《Learning Text Similarity with Siamese Recurrent Networks》</a>，用来做同类文本预料的相似度任务是极好的。</p>
<h1>介绍</h1>
<p>这篇文论主介绍孪生网络的基本思想，对于 query 相似度任务（同类型实体的相似度任务，比如图片、语音），可以设计一个网络结构 net，将两个 query 都经过相同的 net 投影到对应的空间，再该空间中对 net(x) 和 net(y) 求取相似度， net(x) 可以看到 x 在高纬空间的表征。
原 paper 中用 BiLSTM 网络结构来表征 query，模型结构如下：
<img alt="网络架构" src="http://informal.top/usr/uploads/2021/12/835350353.png"/></p>
<h1>实验</h1>
<p>数据取自百度千言的文本相似度评测数据 bq_corpus <a href="https://aistudio.baidu.com/aistudio/competition/detail/45/0/submit-result">数据集</a>。
该论文的 loss 比较像 MSE 的变种，判定正负例的阈值对超参 m 有明显影响，从试验结论看该值设置的越小越好。
<img alt="26015-zfotz81y52a.png" src="http://informal.top/usr/uploads/2021/12/2662121877.png"/></p>
<p>-|-|-|-|-
| 版本               | 训练集acc  | 验证集acc | 说明 
| ----              | ---       | --------- | ----|
| bert              | 0.979     | 0.841     | bert基准版本 |
| mse m = 0.4       |0.60       |0.59     |原loss | 
| mse m = 0          |0.723     |0.724    |降低超参m比较有用|
| mse m = 0 学习率3e-4|0.81      |0.758      |调整学习率，最优版本|
| norm + cos        | 0.722     | 0.721     | norm 没啥效果|
| BCEloss             | 0.637   |0.641     |修改loss效果不大|
-|-|-|-|-</p>
<p>以上 acc 的预测阈值为 0.5，即预测值 &gt; 0.5 算正例，否则算负例。
小结论如下：</p>
<ol>
<li>从实验数据看，需要根据预测阈值，来调整一个合适的超参 m，本文是越小越好。</li>
<li>设置合适的学习率，参考别人的结论设置 3e-4 较好，本文没有试验很多。</li>
<li>做cos之前添加 norm，没有明显提升，猜测是与具体的语料相关。</li>
<li>修改 loss 为 -log 损失函数，也没有明显提升，难道不应该梯度越大，收敛的越好吗？这个很奇怪。</li>
<li>孪生网络与 bert 验证集相差 9 个点左右。</li>
</ol>
<h1>结论</h1>
<p>孪生网络不同于 bert 把两个 query 合并在一起求二分类任务，定义了另一种解决相似度任务的方式。虽然从时间上看孪生网络的提出时间要更早一些。
另一个角度看孪生网络，更像是把一个构造 query 向量的任务，可以从这个角度寻找更多有意思的解法。</p>
<h1>进阶</h1>
<p>很多用 CNN 来做 query 理解的，很有意思，值得学习下</p>
<h1>参考</h1>
<p>[1]、<a href="https://aclanthology.org/W16-1617.pdf">https://aclanthology.org/W16-1617.pdf</a>
[2]、<a href="https://mp.weixin.qq.com/s/UDG5z4lcOiRquRN0H6ELCQ">https://mp.weixin.qq.com/s/UDG5z4lcOiRquRN0H6ELCQ</a></p>
