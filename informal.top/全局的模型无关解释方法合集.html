<h2><a href="https://github.com/wa008/wa008.github.io/blob/master/_posts/2022-11-04-全局的模型无关的解释方法合集.md">仓库源文</a>，<a href="https://informal.top/2022/11/04/全局的模型无关的解释方法合集">站点原文</a></h2>
<p>这几个看的比较糙，就写到一起了。</p>
<h1>Functional Decomposition 函数分解</h1>
<p>把模型做因式分解</p>
<p>$$ f(x1, x2, x3) = f(0) + f(x1) + f(x2) + f(x3) + f(x1, x2) + f(x1, x3) + f(x2, x3) + f(x1, x2, x3) $$</p>
<p>尽量拆解变量之间的关系，进行简化。方案比较理想。</p>
<h1>Permutation Feature Importance</h1>
<p>删除一个特征，看模型预测值的变化，来衡量特征重要性
用训练集还是测试集实验呢？测试集</p>
<ol>
<li>如果用训练集：模型过拟合的话，训练集上也会显示误差较大，会误以为特征比较重要。</li>
<li>如果用测试集：优点是，特征即使和预测值没有线性关心，只要被用到，依然会表现出合理的重要性取值。</li>
</ol>
<p>优点</p>
<ol>
<li>好理解，解释性强。</li>
<li>有全局视角</li>
<li>耗时短：不需要重训模型，重训对比的是不同的模型，而我们想解释的应该是当前的模型，不应该重训。</li>
</ol>
<p>缺点</p>
<ol>
<li>优点3是优点也是缺点，要看具体的应用场景。如果目标只是纯粹衡量特征的好快，与模型无关，那此方法就不合适。</li>
<li>当模型预测结果和 ground truth 差别较大是，误差大可能更优，误差大就可能代表不了特征不重要。</li>
<li>对相关性强的特征很不友好。比如两个身高和体重，可能都能衡量人的体型大小，但具体哪个特征重要，取决于模型使用了哪个特征，这个缺点在树模型中更加显著。</li>
</ol>
<h1>Global Surrogate</h1>
<p>描述：训练一个可解释的模型去拟合原模型，通过解释「可解释模型」来解释原模型。
看法：在原模型不可变的情况下比较适用。比如线上模型出故障了排查原因、线上模型由于各种原因不能修改。</p>
<h1>Prototypes and Criticisms</h1>
<p>描述：挑选原数据集中具有代表性的点（Prototypes），再挑选Prototypes 代表的不好的点Criticisms，利用这些代表点来解释模型，看是否被模型正确判定。
看法：</p>
<ol>
<li>是一个从数据角度解释的方式，并不是解释模型。</li>
<li>选点比较类似聚类</li>
</ol>
<p>参考：<a href="https://christophm.github.io/interpretable-ml-book/global-methods.html">https://christophm.github.io/interpretable-ml-book/global-methods.html</a></p>
