<h2>原文：<a href="https://gitee.com/Kenneth-Lee-2012/MySummary/blob/master/软件构架设计/对卷积的感性理解.rst">对卷积的感性理解</a></h2>
<p>.. Kenneth Lee 版权所有 2023</p>
<p>:Authors: Kenneth Lee
:Version: 0.1
:Date: 2023-12-19
:Status: Draft</p>
<p>对卷积的感性理解</p>
<hr/>
<p>最近做一个机器学习相关的内存性能优化时，对卷积这种运算，多了不少感性的了解，我
把这种“感性”的认识总结一下。</p>
<p>卷积本质是一种“权重”计算。比如我们看到一个事物，它有3个特征，我们总结它具有有
个特性的性质，那么一种最直接的认识就是一个这样的计算：::</p>
<p>[f1, f2, f3] <em> [w1,
                  w2,
                  w3] = f1</em>w1 + f2<em>w2 + f3</em>w3</p>
<p>你看，我们看到到三个特征(f1, f2, f3)，给它3个权重(w1, w2, w3)，我们对这个东西
的总结就是f1<em>w1+f2</em>w2+f3*w3，这是我们对这个东西的“综合评价”。这符合我们一般理
解世界的方式：这个学生语文能力8分，数学能力6分，体育能力10分。数学最重要，权重
5，语文和体育次之，所以语文3，体育2。这就构成我们对这个学生的理解：他是个综合
能力7.4分的学生。这包含偏见，但大部分时候，我们就是这样理解世界的。</p>
<p>这种方法有两个缺点，一个是数据线性化，这通常可以通过在综合分数乘上一个非线性函
数（比如softmax）来解决。这让它不要那么“生硬”，让结果相对“平滑”，这样容易找到
公式去微调这些权重，从而根据训练的结果更容易去修改权重。不过这个问题和我们讨论
卷积的问题无关，我们在本文中不讨论。</p>
<p>另一个问题就是这个依据比较单一，如果输入的特征有多个相关性，在A情况下f1权重比
较高，在B情况下，f2的权重比较高。这样你固定就只调整这一种权重，这样理解问题就
比较刻板。想象一下，我们还是考虑前面说的三个学生成绩，但在校运会和奥数比赛两个
表现中，人们对这个学生的“综合评价”不一样。如果你只能设置一个权重，最多就是这次
调高一点数学，下次调高一点体育，这个判断就怎么都不能贴合这个学生的“综合素质”了。</p>
<p>所以，我们要让我们对这些指标的判断更加“立体”一点，不能只有一个权重，比如，我们
把f1的权重设置成两个，这就是一个多维的矩阵乘法了：::</p>
<p>[f1, f2, f3] <em> [[w11, w12],
                  [w21, w22],
                  [w31, w32] = [f1</em>w11+f2<em>w21+f3</em>w31, f1<em>w12+f2</em>w22+f3*w33]</p>
<p>也就是说，我们用了两个可以在不同的情况（这个“不同情况”可能就是更多的f4, f5...
这些“更多”的要素）下做不同评价的w11, W12两个权重去评价f1了。矩阵乘法综合可以压
扁中间一维，所以它很容易组合起来。比如：::</p>
<p>1x3 <em> 3x4 </em> 4x3 <em> 3x100 </em> 100x1 = 1x1</p>
<p>两个矩阵相乘，中间的一层总是被压平，这样我们就可以任意安排这些层，最终把一个简
单的输入，展开到一个综合的评价体系中，这样我们可以通过不同的一次次“经验”（比如
学生不同的成绩表现得到不同的评价），获得一个最接近的参数集，从而就更容易“预期”
他的成绩和得到的综合评价的相关性了。当然，如果只有这三个参数，本来就没什么规律，
很难做出这种判断。但如果输入的是非常多的要素，可能相关性就变得强了。比如从一幅
图的点阵判断这幅图上画了什么（比如在人脸识别等应用中做的那样）。</p>
<p>所以卷积神经网络把两头的维度叫“显层”（比如上面例子的1x3矩阵和最后的100x1矩阵），
而把中间的卷积层叫“隐层”。在数学上不好理解，但结合一些现实就好理解了：显层就是
我们眼前看见的东西，做出的判断，比如看见一只老虎，得到结论：“快跑”，脑子里面怎
么这么快做出这个判断的？里面的判断过程是理不清逻辑的。老虎，和快跑就是这个判断
计算中显层，看起来是有“逻辑”的，而老虎变成一组权重，扩展到后面的层次中，这些都
是“隐层”，里面就可以理解为这种近似的卷积运算，这种运算的中间结果有什么“理智”的
解释？其实没有，因为它是很多分解过的权重的计算，并不能和现实相关，老虎这两个字
在阵列中传播到了很多个“权重参数”里面了，也就无法直接解释了。</p>
<p>我们可以用LLM（大语言模型）来对这个认识进行感性的理解。LLM最初来自翻译技术，比
如“This is Apple”，要翻译成中文。如果一个单词一个单词翻译，那么，最简单的对照
方法就是：::</p>
<p>This  -&gt; 这
  is    -&gt; 是
  Apple -&gt; 苹果</p>
<p>这样做的翻译就是没有隐层的，这非常有“逻辑”。“This”就是对应“这”，“is”就是对应
“是”。但这种放在是不准确的。比如，在这个上下文中，Apple大写了，而且前面没有an，
这更应该翻译为“苹果公司”。这种粗糙的一一对应的方式并不能合理地翻译这个句子。所
以，这里关键是解决这个“上下文”的问题，LLM的现在发展的主要技术叫Attention（关注
度），原理技术就是把整个句子前后的文字都和当前翻译的这个单词建立一个不同权重的
关联，这样这些文字都成了翻译的这个单词的一个“关注度”，这个算法会用每个上下文来
计算所有单词的QKV(Query-Key-Value），得到一个“1x单词表大小”的显层，然后后面挂
上几十层不同大小的隐层，最终吐出这个词的翻译结果。</p>
<p>那些用于填空、对话的大模型基本上也是这种原理：训练的时候用大量的文本输入到这个
计算权重阵列中（称为“神经网络”），让网络中的权重调整到符合那个文本的填空或者对
话的要求。比如你让它学习“人生自古谁无死”，它就用“人生”逼近“人生自古”，用“人生
自古”逼近“人生自古谁”，用“人生自古谁”逼近“人生自古谁无”。这样经过大量的训练后，
你再给这个模型输入新的句子，它就能根据这个新的句子给出下一个词语，然后用那个句
子和下一个词语作为输入，再吐出下下个词语，从而形成一个“对话”的过程。</p>
<p>以一个预训练对话大模型为例，假定你已经有一组权重阵列W了，你输入一句话“你是
谁？”，它把这句话拆成“你是”，“谁”，“？”，“&lt;问题结束&gt;”，四个词，然后按Attention
的算法生成一个关注度向量，输入到W中，算出下一个词：“我是”，下一步输入：“你是”，
“谁”，“？”，“&lt;问题结束&gt;”，“我是”，这回得到“AI”……如此反复，就形成一个对话的过程
了。训练的时候，就用训练的文本实际的“下一个词”来修正W里面的权重就好了。</p>
<p>这在这个过程中，“你是谁？”和“我是AI”，都是我们看懂的“显层”，但经过编码后，就不
是了，再通过神经网络后，信息被分散到网络的各个位置（权重）上，那个东西就无法被
用“理智”的方式去理解了。</p>
<p>这样各种机器学习问题，都变成你怎么建立一个模型，让关注的要素都能灌到一个计算网
络中（不一定是神经网络），然后不断训练它改变权重的过程了。</p>
