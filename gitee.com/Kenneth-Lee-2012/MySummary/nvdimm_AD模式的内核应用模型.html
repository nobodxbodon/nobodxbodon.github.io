<h2><a href="https://gitee.com/Kenneth-Lee-2012/MySummary/blob/master/%E8%BD%AF%E4%BB%B6%E6%9E%84%E6%9E%B6%E8%AE%BE%E8%AE%A1/nvdimm_AD%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%86%85%E6%A0%B8%E5%BA%94%E7%94%A8%E6%A8%A1%E5%9E%8B.rst">仓库源文</a></h2>
<p>.. Kenneth Lee 版权所有 2020</p>
<p>:Authors: Kenneth Lee
:Version: 1.0</p>
<p>nvdimm AD模式的内核应用模型</p>
<hr/>
<p>本文用于相关考虑展开第一层逻辑，作者只花了几个小时查看相关资料，里面的判断不一
定是对的，欢迎在读者参与讨论，并指出其中的错漏。</p>
<p>nvdimm是Non-Volatile DIMM的缩写，概念上表示把SSD等非易失存储实现成DIMM条，直接
插入内存插槽中使用。一般认为这种dimm条比内存慢，但比SSD块（因为挂的总线更近），
价钱比DRAM便宜，比SSD贵。</p>
<p>nvdimm有两种使用模式：Memory Mode（以下简称mm）和App Direct Mode（以下简称AD）
。前者表示直接当内存用，后者表示让应用感知，当存储设备用。</p>
<p>前者没有什么可说的，是个硬件方案，软件除了在优化上可能有点变动外，基本上可以当
它透明。后者是我们关注的重点。</p>
<p>当前Linux内核对这个特性的实现有三种抽象：</p>
<ol>
<li><p>dimm：一条dimm一个设备</p>
</li>
<li><p>blk：把dimm条中部分空间抽出来作为一个设备</p>
</li>
<li><p>pmem：把多条dimm条合并交织使用提高带宽</p>
</li>
</ol>
<p>三种抽象都可以给使用者呈现为region，region是一片“看来连续”的地址空间，应用程序
通过mmap这片空间对其中的内容进行访问。</p>
<p>Region不实现为一般磁盘所实现的block device。因为它不需要成块访问，也不需要使用
page cache机制。所以内核中创建了一个新的框架，称为dax，Direct Access for X，原
理就是直接mmap对应的物理空间，然后直接访问，中间没有块的组合和page cache的缓冲
。</p>
<p>但即使是DRAM，都会有数据仍在Cache上的情形，所以，断电仍可能造成静态数据的损坏，
所以使用者需要自己知道在什么时候进行原子操作，并保证数据被flush到nvdimm内存中。
nvdimm对此进行了另一层封装，称为BTT，block translation table。BTT以最大512G为单
位把数据进行规整化管理，把数据分成map区和数据区，保证数据更新是原子化的。</p>
<p>用户态程序通过libnvdimm对上述访问进行封装，主要提供helpper函数支持上述概念的访
问。现在这个支持已经扩大了，形成了一组开发库：</p>
<pre><code>    pmem.io: PMDK</code></pre>
<p>我们再花点时间讨论一下MM模式。有人想着把nvdimm用做普通内存，但很难接受它这么慢
，考虑把这个问题推给NUMA，把nvdimm的内存放到另一个NUMA节点上。但这就算不上是一
种“解决方案”，因为这本来就是你这个内存的“属性”，你最多就算招供，不算是解决方案
，对吧？</p>
<p>另一个角度来说，我们提出NUMA，可不是为了说这个内存慢，而是为了强调进程的内存的
距离。我们认知NUMA的存在，是希望把进程和内存靠近了放，然后你这个nvdimm跟谁都“不
近”，那没有什么意义。</p>
<p>我一时是想不到有什么应用，会接受把一部分内存故意放在远内存，一部分数据放在近内
存上的，除非我本来就打算老老实实做Cache，但如果我老老实实做Cache，我何必让软件
管这个呢？直接把DRAM做L4，NVDIMM做L5好了。</p>
<p>我个人是比较容易接受MM模式就是一种低价内存方案，或者就应该发挥它NV的特点，用在
AD模式上。</p>
