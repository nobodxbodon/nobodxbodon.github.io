<h2>原文：<a href="https://iphyer.github.io/blog/2014/10/23/grepicture.markdown">GRE卡片单词</a></h2>
<hr/>
<h2>layout: post
title: "GRE卡片单词"
date: 2014-10-23 16:17
comments: true
categories: GRE</h2>
<h1>前言</h1>
<p>最近在网上发现一个非常好的，GRE单词的总结的配图版本，花费了一天时间，终于把图片都下载下来了。</p>
<p>仔细回想一下其实挺简单的，但是一开始没找到最简单的方法，想用Python，在人人网渣渣的HTML代码页面爬行了半天，掉到无数的坑里面爬不出来。</p>
<p>今天下午睡了个午觉,用grep,wget一下子搞定。So Easy.</p>
<p>上两个实例:</p>
<p><img alt="tu１" src="/media/wwww/share/study/聚聚/源数据/博客聚合/iphyer.github.io/images/GRE/GREpicture/a.jpg"/></p>
<p>这幅图很好的表达了:</p>
<p>al·lege verb \ə-ˈlej\
: to state without definite proof that someone has done something wrong or illegal</p>
<p>不可靠的宣称的意思。</p>
<p><img alt="tu１" src="/media/wwww/share/study/聚聚/源数据/博客聚合/iphyer.github.io/images/GRE/GREpicture/b.jpg"/></p>
<p>这个图自己理解。不解释了。</p>
<p>&lt;!--more--&gt;</p>
<h1>技术过程</h1>
<p>需要说明，这个作者制作的时候可能有的单词打错了，大家使用的时候注意。</p>
<p>下载地址:<a href="http://pan.baidu.com/s/1o6ogi5k">百度网盘</a></p>
<p>一开始是在Github上找软件的，发现了基于Chrome应用的Renren Album Downloader非常好，但是下载的时候对于文件大小有限制——已知的bug就是对于大于500张照片的情况下会出现崩溃。具体原因不明，而我需要下载的是507张照片，果然崩溃。</p>
<p>然后尝试了Github上的这个Python代码<a href="https://github.com/latifrons/RenrenAlbumDownload">atifrons/RenrenAlbumDownload</a>其实这个Python的代码写得非常好，但是作者是在2012年写得，几年过去了人人进行了很大的更新所以很多方法都不太可靠了。但是作者的这个程序还是可以使用的，作者在登陆函数设计方面非常好，对于验证码也很好的解决了，读了他的源代码收获很大，非常有帮助。可惜的是，不知道为什么只能在每个相册上下载40张图片。我仔细阅读了作者的页面解析部分，是基于正则规则的搜索。非常复杂而且不是针对现在的人人网代码的所以也就放弃了。</p>
<p>这个时候我发现人人可以选择不分页模式浏览相册，这样的话所有的照片都可以看到，而且为了保证点击可以进入看大图，人人设置了超链接所以只要能够把超链接提取出来，我就可以用wget提取了。又回过来看了wget的ｍanul发现wget是支持从预设的文件输入参数的。所以我就使用了这个方法做页面解析了。</p>
<h1>代码细节</h1>
<h2>页面解析</h2>
<p>使用了BeautifulSoup解析，这里原谅我很粗暴的使用了直接输入HTML的办法，因为不想使用urllib了毕竟已经下载了页面。</p>
<pre><code>
from bs4 import BeautifulSoup

html= """
&lt;!DOCTYPE html&gt;
&lt;!-- saved from url=(0065)http://photo.renren.com/photo/230038558/album-938072917?noPager=1 --&gt;
&lt;html xmlns="http://www.w3.org/1999/xhtml" class=" marginRightForPager"&gt;&lt;head&gt;&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;
&lt;meta charset="utf-8"&gt;&lt;meta name="Description" content="人人网 校内是一个真实的社交网络，联络你和你周围的朋友。 加入人人网校内你可以:联络朋友，了解他们的最新动态；和朋友分享相片、音乐和电影；找到老同学，结识新朋友；用照片和日志记录生活,展示自我。"&gt;

....省略很多HTML代码

"""


soup = BeautifulSoup(html)
img=soup.find_all("img")
print img

</code></pre>
<p>这段代码的功能就是把所有的img标签提取出来，在terminal里面使用一下重定向把输出直接输入到一个tem的文件里面。本来还想在这个基础上用Python提取url的，后来发现还是不熟悉Python的正则规则，放弃，直接使用grep和wget吧。</p>
<h2>提取url</h2>
<p>这个提取直接使用grep的正则查找，然后把结果重定向，注意我只查找.jpg结尾的url。</p>
<pre><code>
grep -oP '(?&lt;=")http.*?(jpg)(?=")' tem &gt; url.txt

</code></pre>
<h2>wget下载图片</h2>
<pre><code>
wget -b -i url.txt -c

</code></pre>
<p>最后把所有得到的照片按照名称排个序，把所有main开头的缩略图删掉只留下orginal开头的原图。</p>
<p>至此全部搞定！</p>
<h1>总结</h1>
<p>其实这次可以发现，我一开始就没怎么想用wget下载结果导致最后花费了不少时间，确实很多时候wget这样的小工具还是相当方便的，多多使用。</p>
