<h2><a href="https://github.com/iphyer/iphyer.github.io/blob/master/_posts/2019-04-21-GrokkingDL2.md">仓库源文</a>，<a href="https://iphyer.github.io/blog/2019/04/21/GrokkingDL2">站点原文</a></h2>
<h2>Foreword</h2>
<p>Following the last post, I will continue to summarize key points of Chapter 4 in this blog.</p>
<p>Nearly all supervised learning projects can be divided into 3 parts,</p>
<blockquote><p>predict, compare, learn</p>
</blockquote>
<p>so chapter 3 focus on predicting part and chapter will focus on comparing and learning.</p>
<h2>Chapter 4</h2>
<p>Simple Code and Simple Math but the concept is very important:</p>
<h3>Learning in brief</h3>
<blockquote><p>Learning is adjusting the weight to reduce the error to 0!</p>
<p>With derivatives, you can pick any two variables in any formula, and know how they interact! It is the sensitivity between two variables.</p>
</blockquote>
<h3>Why learning rate alpha</h3>
<blockquote><p>the simplest way to prevent overcorrecting weight updates!</p>
</blockquote>
<p>Not solving the overshooting problem just a way to make weight update smaller.</p>
<h3>Transfer Learning Benefits</h3>
<p>The benefits of transfer learning:</p>
<ul>
<li>Transfer learning needs less training data</li>
<li>Transfer learning makes the learned model generalizes better</li>
<li>Transfer learning makes training process less brittle</li>
<li>Transfer learning makes deep learning easier</li>
</ul>
