<h2><a href="https://github.com/shiina18/shiina18.github.io/blob/master/_posts/2022-01-01-bayesop.md">仓库源文</a>，<a href="https://shiina18.github.io/machine%20learning/2022/01/01/bayesop">站点原文</a></h2>
<hr/>
<p>title: "Bayes 优化简介"
categories:</p>
<ul>
<li>Machine Learning
updated: 2022-02-05
comments: true
mathjax: true</li>
</ul>
<hr/>
<p>首先强烈推荐</p>
<ul>
<li>Frazier, P. I. (2018). <a href="https://arxiv.org/abs/1807.02811">A tutorial on Bayesian optimization</a>. <em>arXiv Preprint arXiv:1807.02811</em>.</li>
</ul>
<p>写得非常清楚. 本文只简单介绍最基本的内容.</p>
<p>考虑优化问题</p>
<p>$$
\max_{x\in A} f(x),
$$</p>
<p>Bayesian optimization is designed for black-box derivative-free global optimization. 黑箱意思是不知道函数 $f$ 的形式和性质 (凸性等), 只能通过输入 $x$ 得到输出 $f(x)$, 另外也不知道导数信息, 目标是求解全局最优.</p>
<p>&lt;!-- more --&gt;</p>
<p>Bayes 优化由两部分组成: 目标函数上的 Bayes 模型, 以及用来决定下一步输入哪个值的 acquisition function. 场景是每次输入 $x$ 得到输出 $f(x)$ 有一定成本 (比如时间成本), 希望试最少的次数得到最大值.</p>
<p><img alt="" src="https://shiina18.github.io/assets/posts/images/20211204105530317_13090.png"/></p>
<h2>Gaussian Process (GP) Regression</h2>
<p>GP regression is a Bayesian statistical approach for modeling functions. 现在有 $k$ 个点上的函数值 $f(x_{1:k}) := (f(x_1), \dots, f(x_k))$, 假设它是服从某个先验分布的随机向量的一个具体观测值. GP 把这个先验分布取为多元正态分布</p>
<p>$$
f(x_{1:k}) \sim \operatorname{Normal} (\mu_0(x_{1:k}), \Sigma_0(x_{1:k}, x_{1:k})),
$$</p>
<p>其中 $\mu_0$ 和 $\Sigma_0$ 自定义 (这里会引入新的超参数, 略). 对于新的点 $x$, 考虑后验分布 (条件分布) $f(x) \mid f(x_{1:k})$, 它也是多元正态分布.</p>
<h2>Acquisition functions</h2>
<p>The most commonly used acquisition function is expected improvement. (此外还有 knowledge gradient, entropy search and predictive entropy search 等.) 要决定下一个点 $x$ 试哪里收益高, EI 根据历史观测, 取期望增益 (相比历史观测最大值) 最高的点, 即</p>
<p>$$
\operatorname{EI}<em>k(x) = \mathbb E_k \max(0, f(x) - \max</em>{j\le k} f(x_j)),
$$</p>
<p>其中 $\mathbb E_k$ 对应的分布为观测 $k$ 个点的后验分布. 然后下一个点 $x_{k+1}$ 取为使这个期望最大的点</p>
<p>$$
x_{k+1} = \operatorname{argmax} \operatorname{EI}_k(x).
$$</p>
<p><img alt="" src="https://shiina18.github.io/assets/posts/images/147071516220142.png"/></p>
<p>总得来说贝叶斯优化把原先的寻找超参数问题转换为在 surrogate model (GP) 上调超参, 求解 acquisition function 最大值的问题.</p>
<h2>Further reading</h2>
<ul>
<li><a href="https://www.reddit.com/r/MachineLearning/comments/fsupfu/d_bayesian_optimization_does_it_work/">Bayesian Optimization: does it work?</a></li>
<li><a href="https://distill.pub/2020/bayesian-optimization/">Exploring Bayesian Optimization</a>. 可以看文章底下收集的一些包.</li>
<li><a href="https://www.zhihu.com/question/33711002">为什么基于贝叶斯优化的自动调参没有大范围使用?</a><ul>
<li>不过之前看到 AlphaGo (Mastering the game of Go without human knowledge) 在某个地方用到了贝叶斯优化 .</li>
</ul>
</li>
<li>Ouyang, Y. (2022, Feb 4). <a href="https://engineering.linkedin.com/blog/2022/using-bayesian-optimization-for-balancing-metrics-in-recommendat">Using Bayesian optimization for balancing metrics in recommendation systems</a>. <em>LinkedIn Engineering</em>.</li>
</ul>
