<h2>原文：<a href="https://shiina18.github.io/machine%20learning/2022/10/16/gpu">深度学习性能瓶颈往往是内存带宽</a></h2>
<hr/>
<p>title: "深度学习性能瓶颈往往是内存带宽"
categories:</p>
<ul>
<li>Machine Learning
tags:
updated:
comments: true
mathjax: false</li>
</ul>
<hr/>
<p>首先是这篇文章</p>
<ul>
<li>Horace He. (2022). <a href="https://horace.io/brrr_intro.html">Making Deep Learning Go Brrrr From First Principles</a></li>
</ul>
<p><strong>内容不再赘述</strong>, 下面是纲要和一些对原文的注解. 虽然文章用 Torch 和 GPU 举例, 但基本原理依然普适.</p>
<p>&lt;details&gt;&lt;summary&gt;&lt;b&gt;注&lt;/b&gt;&lt;font color="deepskyblue"&gt; (Show more &amp;raquo;)&lt;/font&gt;&lt;/summary&gt;
&lt;p&gt;机器之心 &lt;a href="https://zhuanlan.zhihu.com/p/485490532"&gt;编译&lt;/a&gt; 过这篇文章, 但有些错误. 比如只看第一段, 前几句话还行, 但光最后一句就有两个技术错误: (1) 把 in-place 说成内置; (2) 梯度设置为 None 说成 0. 关于第 2 点可以参考 torch 文档 &lt;a href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#use-parameter-grad-none-instead-of-model-zero-grad-or-optimizer-zero-grad"&gt;Use parameter.grad = None instead of model.zero_grad() or optimizer.zero_grad()&lt;/a&gt;.&lt;/p&gt;&lt;/details&gt;</p>
<p>简单介绍了模型耗时可以分为三部分</p>
<ul>
<li>Compute: Time spent on your GPU computing actual floating point operations (FLOPS)</li>
<li>Bandwith: Time spent on moving the data from CPU to GPU, from one node to another, or even from CUDA global memory to CUDA shared memory</li>
<li>Overhead: Everything else</li>
</ul>
<p>&lt;!-- more --&gt;</p>
<p>我们希望最大化利用 GPU 算力, 但实际上内存带宽才是瓶颈. 硬件层面, 每年内存带宽增速不及 FLOPS 增速, 导致这个问题更加严重.</p>
<p>简单起见, 文章只讨论了 GPU 内部数据转移带来的内存带宽开销, 并简单介绍了可以用算子融合降低带宽开销. 文章用 toy example 演示了什么时候受限于带宽, 什么时候瓶颈才是算力.</p>
<ul>
<li><a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf">NVIDIA A100 specs</a></li>
<li>那张三类算子耗时图片出处论文 Data Movement Is All You Need: A Case Study on Optimizing Transformers 是 MLSys 2021 五篇 <a href="https://mlsys.org/Conferences/2021/BestPapers">best papers</a> 之一.</li>
</ul>
<h2>算子融合</h2>
<p>也叫 kernel 融合</p>
<ul>
<li><a href="https://stackoverflow.com/questions/56601075/what-is-a-fused-kernel-or-fused-layer-in-deep-learning">"Kernel" here is for computation kernels</a>.</li>
<li>Torch 博文 <a href="https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/">Optimizing CUDA Recurrent Neural Networks with TorchScript</a> 可以作为算子融合 (主要是逐点运算) 的例子. [<a href="https://zhuanlan.zhihu.com/p/83419913">网友笔记</a>]</li>
<li>Torch 文档 <a href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#fuse-pointwise-operations">fuse pointwise operations</a> 介绍了用户怎么手动融合算子.</li>
<li><a href="https://www.doit.com.cn/p/306236.html">内核融合: GPU 深度学习的 "加速神器"</a> 微软研究院一篇老博文, 不知道为什么官网上被删掉了. 可以读一读 "内核间的数据同步" 这节.</li>
</ul>
<h2>Overhead</h2>
<p>最后文章提到虽然 PyTorch 等框架有额外调度开销, 但因为 CPU 和 GPU 并行, 只要 GPU 任务足够繁重, CPU 完成任务时间短, 那么 overhead 就不会成为瓶颈.</p>
<ul>
<li>Overhead bound 那张手绘图的竖线应当是时间轴, 从上到下.</li>
<li>Torch 文档 <a href="https://pytorch.org/docs/stable/notes/cuda.html#asynchronous-execution">asynchronous-execution</a> 中也有介绍 "By default, GPU operations are asynchronous".</li>
<li>更详细的 Torch 内部调度方法可以参考 <a href="http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/">Let's talk about the PyTorch dispatcher</a>.</li>
<li><a href="https://zhuanlan.zhihu.com/p/41662629">Torch profiler</a></li>
</ul>
<h2>其他</h2>
<p>NVIDIA <a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-getting-started/index.html#operate-guidelines">文档</a> 开头就写了</p>
<blockquote>
<p>Operating In Math-Limited Regime Where Possible</p>
</blockquote>
<p>尽量让计算成为瓶颈.</p>
<p>另外要注意 <a href="https://medium.com/analytics-vidhya/explained-output-of-nvidia-smi-utility-fc4fbee3b124">nvidia-smi</a> 中的 GPU-Util (不要读成 <a href="https://twitter.com/cHHillee/status/1500547396945670144">Volatile GPU-Util</a>) 定义只是时间维度上的</p>
<blockquote>
<p>It indicates the percent of GPU utilization i.e. percent of the time when kernels were using GPU over the sample period. Here, the period could be between 1 to 1/6th second.</p>
</blockquote>
<p>只要时间段内有 <strong>一个</strong> kernel 执行就会算上 util. 最简单的 <a href="https://www.zhihu.com/question/307643863">例子</a>: 写一个 1-block 的循环 kernel 就能把 GPU "利用率" 打到 100%, 但在空间层面上利用率很低.</p>
<ul>
<li>NVIDIA 文档 <a href="https://docs.nvidia.com/deeplearning/performance/index.html">Deep Learning Performance Documentation</a> 介绍了 arithmetic intensity, 以及更详细的介绍包括 GPU Performance Background, Matrix Multiplication Background 等. 其他相关关键词还有 roofline modeling. (TODO 疑问: 为什么增大 batch size 可以提高 arithmetic intensity, 分子和分母应该都是线性增长?)</li>
<li><a href="https://zhuanlan.zhihu.com/p/122943688">FLOPs 与模型推理速度</a> 提供具体的计算机视觉模型的例子.</li>
<li><a href="https://blog.paperspace.com/understanding-tensor-cores/">Understanding Tensor Cores</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/346389176">教你如何继续压榨 GPU 的算力</a> 介绍了 MPS (Multi-Process Service) 模式.</li>
<li><a href="https://www.zhihu.com/question/454589636/answer/1836794871">为什么模型和数据都在 GPU 上, 却打不满 GPU 的使用率?</a></li>
<li><a href="https://www.zhihu.com/question/307643863/answer/2662713663">同一张 GPU 运行多个程序速度会变慢吗?</a> 大概率会变慢. 该答主 <a href="https://www.zhihu.com/people/lychee-82">lychee</a> 写过不少 GPU 底层的文章.</li>
<li><a href="https://zhuanlan.zhihu.com/p/462191421">GPU 内存 (显存) 的理解与基本使用</a></li>
</ul>
<h3>混合精度训练</h3>
<ul>
<li>rumor. (2020). <a href="https://zhuanlan.zhihu.com/p/84219777">一文搞懂神经网络混合精度训练</a></li>
<li><a href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html">Train With Mixed Precision</a>. NVIDIA.</li>
<li><a href="https://developer.nvidia.com/automatic-mixed-precision">Automatic Mixed Precision for Deep Learning</a>. NVIDIA.</li>
<li><a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/performance_improving/amp_cn.html">自动混合精度训练（AMP）-使用文档-PaddlePaddle深度学习平台</a></li>
</ul>
