<h2><a href="https://github.com/shiina18/shiina18.github.io/blob/master/_posts/2019-03-07-svd.md">仓库源文</a>，<a href="https://shiina18.github.io/mathematics/2019/03/07/svd">站点原文</a></h2>
<p>简单复习.</p>
&lt;!-- more --&gt;

<p>考虑 $m\times n$ 的实矩阵 $A$, 秩为 $r$, 记 SVD (singular value decomposition) 的一般形式为</p>
<p>$$A = U\Sigma V',$$</p>
<p>其中 $U=(u_1,\dots,u_m)$, $V=(v_1,\dots,v_n)$ 为正交阵,</p>
<p>$$
\Sigma = \begin{pmatrix} S &amp; O\ O &amp; O  \end{pmatrix},
$$</p>
<p>其中 $S = \text{diag}\{\sigma_1,\dots,\sigma_r\}$, $\sigma_1\ge\cdots\sigma_r&gt; 0$ 是 $A$ 的非零奇异值.</p>
<p>Proof. 由于 $A'A$ 是对称半正定矩阵, 故存在正交对角化</p>
<p>$$
A'A = V\Lambda V',
$$</p>
<p>其中 $V$ 是正交阵, $\Lambda = \text{diag}\{\lambda_1,\dots,\lambda_n\}$, 且特征值非负, 不妨设 $\lambda_1\ge\cdots\ge\lambda_n$.</p>
<p>另外 $r = \text{rank}(A) = \text{rank}(A'A)$. 事实上, $Ax = 0$ 的解都是 $A'Ax = 0$ 的解; 而若 $A'Ay = 0$, 则 $y'A'Ay = 0$ 意味着 $Ay = 0$. 因此只有 $r$ 个非零特征值, 记 <strong>奇异值</strong> $\sigma_j = \sqrt{\lambda_j}$.</p>
<p>下面记</p>
<p>$$
u_j = Av_j / \sigma_j, \quad j=1,\dots, r.
$$</p>
<p>显然 $u_1, \dots, u_r$ 是一组单位正交基, 将它扩张为 $m$ 维的单位正交基, 最后整理即得 SVD. $\square$</p>
<p>Note:</p>
<ul>
<li>最后一段也就是说, 对任意 $A$, 在 $r$ 维空间存在一组正交基, 使得其经过 $A$ 映射后为依然为一组正交基.</li>
<li>当然了, 把 $U$, $V$ 分别写成 $m\times r$ 和 $n\times r$ 矩阵, $\Sigma$ 为对角线元素全为正的对角阵也行.</li>
<li>从 SVD 的形式, $V$ 和 $U$ 表示旋转, 而 $\Sigma$ 表示拉伸, 也就是其几何意义.</li>
<li>从 SVD 的形式, $V$ 是 $A'A$ 的正交特征向量, $U$ 是 $AA'$ 的正交特征向量.</li>
</ul>
<p>把 $A$ 视为 <strong>中心化后</strong> 的样本矩阵, 每一行为一个样本 ($m$ 个样本), 每一列代表一个特征 ($n$ 个特征). 则样本协方差矩阵为 $A'A/m$, 而主成分 (principal component) $Av_j$ 的样本方差为 $\sigma_j/m$. 奇异值 $\sigma_j$ 越大意味着对应的样本方差越大, 我们把方差大视为提供了更多信息.</p>
<p>$A$ 可写为</p>
<p>$$
A = \sum_{k=1}^r \sigma_k u_kv_k',
$$</p>
<p>把带有较少信息的小的奇异值扔掉便达到了压缩 (减少需要存储的东西) 的目的, 比如图像压缩.</p>
<p>PCA (principal component analysis) 降维也是同样的. 不过 PCA 还有若干种不同的 formulation.</p>
<p>从几何意义直观的展示就是, 奇异值大的那个轴数据更散布, 从而方差更大.</p>
<p><img alt="src: https://en.wikipedia.org/wiki/Singular_value_decomposition" src="https://shiina18.github.io/assets/posts/images/20200915163305503_24769.png" title="src: https://en.wikipedia.org/wiki/Singular_value_decomposition"/></p>
<p><img alt="Lenna 照片保留前 k 个奇异值的压缩结果" src="https://shiina18.github.io/assets/posts/images/20200915155342646_5911.png" title="Lenna 照片保留前 k 个奇异值的压缩结果"/></p>
<p>References</p>
<ul>
<li>姚慕生, 吴泉水, 谢启鸿. (2014). 高等代数学 (第三版). 上海: 复旦大学出版社. pp. 414-416.</li>
<li>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). <em>The elements of statistical learning: data mining, inference, and prediction</em>. Springer Science &amp; Business Media. p. 66.</li>
</ul>
<p>Images:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Singular value decomposition - Wikipedia</a></li>
</ul>
