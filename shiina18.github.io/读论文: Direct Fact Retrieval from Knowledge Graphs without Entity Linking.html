<h2><a href="https://github.com/shiina18/shiina18.github.io/blob/master/_posts/2023-12-04-kg.md">仓库源文</a>，<a href="https://shiina18.github.io/machine%20learning/2023/12/04/kg">站点原文</a></h2>
<hr/>
<p>title: "读论文: Direct Fact Retrieval from Knowledge Graphs without Entity Linking"
categories:</p>
<ul>
<li>Machine Learning
tags: NLP
updated: 
comments: true
mathjax: false</li>
</ul>
<hr/>
<p>Baek, J., Aji, A. F., Lehmann, J., &amp; Hwang, S. J. (2023). <a href="https://arxiv.org/abs/2305.12416">Direct Fact Retrieval from Knowledge Graphs without Entity Linking</a>. <em>arXiv preprint arXiv:2305.12416</em>.</p>
<p>简单粗暴的召回 + 排序. 流程是标准的, 粗暴点 ("创新点") 在于直接输入句子与知识库中的东西算相似度. 两句话讲完.</p>
&lt;!-- more --&gt;

<p><img alt="" src="https://shiina18.github.io/assets/posts/images/399773210249686.png"/></p>
<p><img alt="图片来自 sentence-transformers 文档" src="https://shiina18.github.io/assets/posts/images/109281210231260.png" title="图片来自 sentence-transformers 文档"/></p>
<p>召回 <strong>Direct Knowledge Graph Retrieval.</strong> 跳过实体识别+实体链接的步骤, 直接学习 (对比学习) 输入句子和三元组 (也用句子表示) 的相似度. 其中句子和三元组的 embedidng 是分别独立完成的 (三元组 embedding 线下先算好存着, 双塔, 见上图左边). 论文用了 distilbert (66M params).</p>
<p>排序 (精排) <strong>Reranking for Accurate Fact Retrieval.</strong> 先根据相似度召回若干候选三元组, 然后把句子和三元组同时 (拼接在一起, 单塔 point-wise, 见上图右边) 输入另外一个模型得到相似度. 论文用了 MiniLM (22M params).</p>
<p><img alt="" src="https://shiina18.github.io/assets/posts/images/569534814249671.png"/></p>
<p>看论文的效果不加排序的效果不好. 没有开源代码.</p>
<p>For supervised learning experiments, we train all models for 30 epochs, with a batch size of 512 for question answering and 32 for dialogue, and a learning rate of 2e-5.</p>
