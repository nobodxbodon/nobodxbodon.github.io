<h2>原文：<a href="https://shiina18.github.io/2023/05/09/chatdoc">ChatDoc 系列简要</a></h2>
<hr/>
<p>title: "ChatDoc 系列简要"
categories:</p>
<ul>
<li>Machine Learning
updated: 2023-08-03
comments: true
mathjax: false</li>
</ul>
<hr/>
<p>ChatDoc 系列有很多实现, 典型流程如下:</p>
<ul>
<li>解析文档获得文本, 将文本切成小块, 存储各文本块的 embedding 作为索引</li>
<li>获得用户 query 的 embedding, 从文本块 embedding 索引中召回 (kNN) 相关文本块</li>
<li>将 query 和召回的文本块组装成 prompt 调用 LLM 获得答案</li>
</ul>
<p>下面是一些开源项目及其实现方式区别</p>
<p>&lt;!-- more --&gt;</p>
<h2>pdfGPT</h2>
<p><a href="https://github.com/bhaskatripathi/pdfGPT">bhaskatripathi/pdfGPT</a></p>
<p><strong>Embedding</strong></p>
<p>句子 embedding 用了 <a href="https://tfhub.dev/google/universal-sentence-encoder/4">universal-sentence-encoder</a>, 只支持英文.</p>
<p><strong>Retrieval</strong></p>
<p>kNN</p>
<p><strong>Prompt</strong></p>
<pre><code class="language-python">prompt = ""
prompt += 'search results:\n\n'
for c in topn_chunks:
    prompt += c + '\n\n'

prompt += (
    "Instructions: Compose a comprehensive reply to the query using the search results given. "
    "Cite each reference using [ Page Number] notation (every result has this number at the beginning). "
    "Citation should be done at the end of each sentence. If the search results mention multiple subjects "
    "with the same name, create separate answers for each. Only include information found in the results and "
    "don't add any additional information. Make sure the answer is correct and don't output false content. "
    "If the text does not relate to the query, simply state 'Text Not Found in PDF'. Ignore outlier "
    "search results which has nothing to do with the question. Only answer what is asked. The "
    "answer should be short and concise. Answer step-by-step. \n\nQuery: {question}\nAnswer: "
)

prompt += f"Query: {question}\nAnswer:"
</code></pre>
<p>最有趣的点是这个 prompt 让 LLM 回答时每个句末给出来源编号.</p>
<p>观察 <a href="https://www.pandagpt.io/">PandaGPT</a> (非开源) 给出的 sources, 固定是四个, 而且包含无关来源, 可以推断其实现方式应该是给出所有召回文本块的位置. <a href="https://chatdoc.com/">ChatDOC</a> 给出的 sources 数量不固定, 但也有无关来源, 估计也是这么做的, 而且可能是按相关度排序的.</p>
<h2>ChatFiles</h2>
<p><a href="https://github.com/guangzhengli/ChatFiles">guangzhengli/ChatFiles</a></p>
<p>基于 <a href="https://github.com/jerryjliu/llama_index">llama_index</a> 开发, 这个包封装了开头说的 ChatDoc 典型流程.</p>
<p><strong>Embedding</strong></p>
<p>默认调 openai 接口</p>
<p><strong>Retrieval</strong></p>
<p>可以配置多种方式, 默认是 kNN</p>
<p><strong>Prompt</strong></p>
<pre><code class="language-python">DEFAULT_PROMPT = (
    "We have provided context information below: \n"
    "---------------------\n"
    "{context_str}\n"
    "---------------------\n"
    "Given this information, Please answer my question in the same language that I used to ask you.\n"
    "Please answer the question: {query_str}\n"
)
</code></pre>
<p>llama_index 中根据使用场景还有别的 prompt; 如果 prompt 过长 (因为召回的文本块过长), 则会切分成多个 prompts, 后续的 prompt 中加入之前的回答, 并让 LLM 优化之前的回答.</p>
<p>llama_index 的一些原始 prompt</p>
<pre><code class="language-python">DEFAULT_TEXT_QA_PROMPT_TMPL = (
    "Context information is below. \n"
    "---------------------\n"
    "{context_str}"
    "\n---------------------\n"
    "Given the context information and not prior knowledge, "
    "answer the question: {query_str}\n"
)
DEFAULT_REFINE_PROMPT_TMPL = (
    "The original question is as follows: {query_str}\n"
    "We have provided an existing answer: {existing_answer}\n"
    "We have the opportunity to refine the existing answer "
    "(only if needed) with some more context below.\n"
    "------------\n"
    "{context_msg}\n"
    "------------\n"
    "Given the new context, refine the original answer to better "
    "answer the question. "
    "If the context isn't useful, return the original answer."
)

DEFAULT_SUMMARY_PROMPT_TMPL = (
    "Write a summary of the following. Try to use only the "
    "information provided. "
    "Try to include as many key details as possible.\n"
    "\n"
    "\n"
    "{context_str}\n"
    "\n"
    "\n"
    'SUMMARY:"""\n'
)
</code></pre>
<h2>langchain-ChatGLM</h2>
<p><a href="https://github.com/imClumsyPanda/langchain-ChatGLM">imClumsyPanda/langchain-ChatGLM</a></p>
<p>基于 <a href="https://github.com/hwchase17/langchain">langchain</a> 开发, 针对中文优化.</p>
<p><strong>Embedding</strong></p>
<pre><code class="language-python">embedding_model_dict = {
    "ernie-tiny": "nghuyong/ernie-3.0-nano-zh",
    "ernie-base": "nghuyong/ernie-3.0-base-zh",
    "text2vec-base": "shibing624/text2vec-base-chinese",
    "text2vec": "GanymedeNil/text2vec-large-chinese",
}
</code></pre>
<p>langchain 的 HuggingFaceEmbeddings 的默认模型是 sentence-transformers/all-mpnet-base-v2, 其中 "The all-* models are only trained on English. The all means 'all-training-datasets'" (<a href="https://github.com/UKPLab/sentence-transformers/issues/1232">#1232</a>) 所以中文可能不够好.</p>
<p><strong>Prompt</strong></p>
<pre><code class="language-python">PROMPT_TEMPLATE = """已知信息：
{context} 

根据上述已知信息，简洁和专业的来回答用户的问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题” 或 “没有提供足够的相关信息”，不允许在答案中添加编造成分，答案请使用中文。 问题是：{question}"""
</code></pre>
<h2>Miscs</h2>
<p><strong>General prompt design</strong></p>
<p>上面几个例子基本上是这样的</p>
<pre><code>context
instruction
query
</code></pre>
<p><strong>Retrieval</strong></p>
<p>对每个文本块总结或者扩写问题 (问 LLM 这个文本块可以问什么问题), 再把这些信息和文本块一起存储起来, 期待改善召回.</p>
<p><strong>搜索引擎化</strong></p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/641132245">LLM+Embedding构建问答系统的局限性及优化方案</a></li>
<li>2023-06-13 <a href="https://weaviate.io/blog/llms-and-search">Large Language Models and Search</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/664921095">RAG探索之路的血泪史及曙光</a> (我还没看)</li>
</ul>
