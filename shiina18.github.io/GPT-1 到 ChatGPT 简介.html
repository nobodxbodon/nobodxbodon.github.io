<h2>原文：<a href="https://shiina18.github.io/machine%20learning/2022/12/24/gpt">GPT-1 到 ChatGPT 简介</a></h2>
<hr/>
<p>title: "GPT-1 到 ChatGPT 简介"
categories:</p>
<ul>
<li>Machine Learning
tags: NLP
updated:
comments: true
mathjax: false</li>
</ul>
<hr/>
<p>总体时间线参考 <a href="https://lifearchitect.ai/chatgpt/#:~:text=The%20Memo.-,Timeline%20to%20ChatGPT,-Date">这里</a>.</p>
<h2>GPT-1~3</h2>
<h3>GPT-1</h3>
<blockquote>
<p>Our system works in two stages; first we train a transformer model on a very large amount of data in an unsupervised manner — using language modeling as a training signal — then we fine-tune this model on much smaller supervised datasets to help it solve specific tasks.</p>
<p>We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads).</p>
</blockquote>
<p>GPT 全称 generative pre-training, 就是预训练 + 微调. 时间顺序从前到后依次是, GPT-1, BERT, GPT-2.</p>
<p><img alt="来自 BERT 论文" src="https://shiina18.github.io/assets/posts/images/221634209239691.png" title="来自 BERT 论文"/></p>
<p>&lt;!-- more --&gt;</p>
<h3>GPT-2</h3>
<p>GPT-2 is a direct scale-up of GPT, with more than 10X the parameters (1542M vs 117M) and trained on more than 10X the amount of data. Github: <a href="https://github.com/openai/gpt-2">openai/gpt-2</a>.</p>
<p><strong>方法</strong></p>
<p>不再针对单独的任务分别微调</p>
<blockquote>
<p>Learning to perform a single task can be expressed in a probabilistic framework as estimating a conditional distribution p(output | input). Since a general system should be able to perform many different tasks, even for the same input, it should condition not only on the input but also on the task to be performed. That is, it should model p(output | input, task).</p>
<p>For example, a translation training example can be written as the sequence <code>(translate to french, english text, french text)</code>. Likewise, a reading comprehension training example can be written as <code>(answer the question, document, question, answer)</code>.</p>
</blockquote>
<blockquote>
<p>Our speculation is that a language model with sufficient capacity will begin to learn to infer and perform the tasks demonstrated in natural language sequences in order to better predict them, regardless of their method of procurement. <strong>If a language model is able to do this it will be, in effect, performing unsupervised multitask learning.</strong> We test whether this is the case by analyzing the performance of language models in a zero-shot setting on a wide variety of tasks.</p>
</blockquote>
<p><strong>数据</strong></p>
<blockquote>
<p>Our approach motivates building as large and diverse a dataset as possible in order to collect <strong>natural language demonstrations of tasks</strong> in as varied of domains and contexts as possible.</p>
</blockquote>
<p><img alt="" src="https://shiina18.github.io/assets/posts/images/518713610227558.png"/></p>
<h3>GPT-3</h3>
<p>GPT-2 的更大版本 (175 billion 参数, 是 GPT-2 的上百倍).</p>
<blockquote>
<p>For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.</p>
</blockquote>
<p><img alt="" src="https://shiina18.github.io/assets/posts/images/389065012236948.png"/></p>
<p><img alt="" src="https://shiina18.github.io/assets/posts/images/578433412247724.png"/></p>
<p><img alt="模型得够大才玩得起" src="https://shiina18.github.io/assets/posts/images/92663912240393.png" title="模型得够大才玩得起"/></p>
<h3>参考</h3>
<ul>
<li><strong>GPT-1</strong> Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving language understanding by generative pre-training</a>.</li>
<li>OpenAI Blog. (2018). <a href="https://openai.com/blog/language-unsupervised/">Improving Language Understanding with Unsupervised Learning</a></li>
<li><strong>GPT-2</strong> Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever, I. (2019). <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language models are unsupervised multitask learners</a>. <em>OpenAI blog</em>, <em>1</em>(8), 9.</li>
<li>OpenAI Blog. (2019). <a href="https://openai.com/blog/better-language-models/">Better Language Models and Their Implications</a></li>
<li><strong>GPT-3</strong> Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... &amp; Amodei, D. (2020). <a href="https://arxiv.org/pdf/2005.14165.pdf">Language models are few-shot learners</a>. <em>Advances in neural information processing systems</em>, <em>33</em>, 1877-1901.</li>
<li>Priya Shree. (2020). <a href="https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2">The Journey of Open AI GPT models</a></li>
</ul>
<h2>InstrcutGPT (GPT-3.5)</h2>
<p>涉及到的所有模型都是 GPT-3 结构, 参数量大小不同. InstructGPT (1.3B 参数量) is better than GPT-3 at following English instructions.</p>
<blockquote>
<p>But these models (GPT-3) can also generate outputs that are untruthful, toxic, or reflect harmful sentiments. This is in part because GPT-3 is trained to predict the next word on a large dataset of Internet text, rather than to safely perform the language task that the user wants. In other words, these models aren't <strong>aligned</strong> with their users.</p>
</blockquote>
<p>To make our models safer, more helpful, and more aligned, we use an existing technique called reinforcement learning from human feedback (RLHF).</p>
<blockquote>
<p>We hired about 40 contractors... We kept our team of contractors small because this facilitates high-bandwidth communication with a smaller set of contractors who are doing the task full-time.</p>
</blockquote>
<blockquote>
<p>The SFT dataset contains about 13k training prompts (from the API and labeler-written), the RM dataset has 33k training prompts (from the API and labeler-written), and the PPO dataset has 31k training prompts (only from the API).</p>
</blockquote>
<p><img alt="" src="https://shiina18.github.io/assets/posts/images/40600917248186.png"/></p>
<p>解释一下 step 3. 不涉及强化学习的术语, "宏观上" 可以把流程类比到普通深度学习.</p>
<ul>
<li>Policy 是语言模型, 输入文本 (prompt), 输出文本.</li>
<li>Reward model 是 step 2 训练 (pairwise learning to rank) 的模型 (6B 参数量), 输入 prompt 和生成的文本, 输出一个实数以表示生成文本的质量. 地位相当于普通深度学习流程中的损失函数, 只是在当前任务下很难写直接写个函数来评估生成文本的好坏, 所以训练了一个模型来做.<ul>
<li>更准确的说这里损失函数的主项是 reward 的期望, 其他还有些别的正则项.</li>
</ul>
</li>
<li>更新参数. PPO 是强化学习中一种常用优化算法, 类比普通深度学习流程, 把它想象成梯度下降就行.</li>
<li>最后 step 2 和 step 3 不停迭代.</li>
</ul>
<blockquote>
<p>A limitation of this approach is that it introduces an "alignment tax": aligning the models only on customer tasks can make their performance worse on some other academic NLP tasks.</p>
<p>We've found a simple algorithmic change that minimizes this alignment tax: during RL fine-tuning we mix in a small fraction of the original data used to train GPT-3, and train on this data using the normal log likelihood maximization.</p>
</blockquote>
<h3>参考</h3>
<ul>
<li>OpenAI Blog. (2022). <a href="https://openai.com/blog/instruction-following/">Aligning Language Models to Follow Instructions</a></li>
<li><strong>InstructGPT</strong> Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., ... &amp; Lowe, R. (2022). <a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a>. <em>arXiv preprint arXiv:2203.02155</em>.</li>
<li>OpenAI Blog. (2017). <a href="https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/">Learning from Human Preferences</a></li>
<li>Hugging Face Blog. (2022). <a href="https://huggingface.co/blog/rlhf">Illustrating Reinforcement Learning from Human Feedback (RLHF)</a></li>
</ul>
<h2>ChatGPT</h2>
<blockquote>
<p>We trained this model using Reinforcement Learning from Human Feedback (RLHF), <strong>using the same methods as InstructGPT, but with slight differences in the data collection setup</strong>. We trained an initial model using supervised fine-tuning: human AI trainers provided conversations in which they played both sides—the user and an AI assistant. We gave the trainers access to model-written suggestions to help them compose their responses. We mixed this new dialogue dataset with the InstructGPT dataset, which we transformed into a dialogue format.</p>
</blockquote>
<p>数据闭环的成功: <a href="https://mp.weixin.qq.com/s/vKiy2zEPWa9FL4BFJpUgcQ">只有我一个人对 ChatGPT 感到蕉绿吗?</a></p>
<h3>一些用例</h3>
<ul>
<li><a href="https://github.com/f/awesome-chatgpt-prompts">f/awesome-chatgpt-prompts: This repo includes ChatGPT promt curation to use ChatGPT better.</a></li>
<li><a href="https://www.zhihu.com/question/570189639/answer/2788647814">当虚拟机</a></li>
<li><a href="https://share.api.weibo.cn/share/357446941,4843306712043720.html?weibo_id=4843306712043720">实现新的编程语言</a> [<a href="https://twitter.com/tisoga/status/1599347662888882177?s=46&amp;t=ghNYNFoEsrlOJZXLYaWa2w">原推</a>]</li>
<li><a href="https://onetwo.ren/ChatGPT-Magic-Chat/#Index:Index">一些咒语</a></li>
</ul>
<h3>参考</h3>
<ul>
<li>OpenAI Blog. (2022). <a href="https://openai.com/blog/chatgpt/">ChatGPT: Optimizing Language Models for Dialogue</a></li>
<li>张俊林. <a href="https://mp.weixin.qq.com/s/hKnJclVG11H5LbQuKkXMjg">ChatGPT 会取代搜索引擎吗?</a></li>
<li>Datawhale. <a href="https://mp.weixin.qq.com/s/pxAo75C7mimlm3bJMKkrfg">全网唯一, 不忽悠的 ChatGPT</a></li>
</ul>
<p><strong>扩展阅读</strong></p>
<ul>
<li>曹越. <a href="https://www.zhihu.com/question/570189639/answer/2787763735">如何评价 OpenAI 的超级对话模型 ChatGPT?</a></li>
<li>符尧. <a href="https://mp.weixin.qq.com/s/VYv8BRgGnp9ZTuXxaSuFwg">追溯 ChatGPT 各项能力的起源</a>. 可以直接看最后 "五、总结当前阶段 GPT-3.5 的进化历程" 总结图</li>
</ul>
