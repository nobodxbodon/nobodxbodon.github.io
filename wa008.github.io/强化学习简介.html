<h2><a href="https://github.com/wa008/wa008.github.io/blob/master/blog/_posts/2022-11-16-强化学习简介.md">仓库源文</a>，<a href="https://wa008.github.io/2022/11/16/强化学习简介">站点原文</a></h2>
<hr/>
<p>layout: default</p>
<h2>title: 强化学习简介</h2>
<h2>概念</h2>
<p>强化学习是机器学习的一个分支，指模型输出结果后从环境中获取反馈，从而优化自身的一种学习方法。</p>
<p>人类的学习过程就是学习知识，然后根据环境的反馈不断提升自己。最典型的比如开车，可以通过现实的反馈得知当前操作是否合理。再比如数学推理，人类本身就可以证明某个数学结果的真伪，当然这个证明过程或许比较长，但至少是可证明的。</p>
<p>最著名的案例是 DeepMind 的 AlphaGo 击败围棋世界冠军李世石，生成对抗网络GAN也属于强化学习的一种。</p>
<h2>Q-learning</h2>
<p>Q-learning 是不断维护一个 Q[status][action]=value 的二维数据表，value 代表当前 status 下执行 action 能获取到的奖励。</p>
<p>通过不断地学习、试错，来更新这个Q表，使得Q表的取值越来越合理，这样无论对于任意一种状态，都能选择到Q值最大的action进行操作。</p>
<p>实际的更新过程中，会认为当前action的收益 + 下次status的Q值是当前 status-action 的实际Q值，以此作为基准来不断更新当前的预测值，即当前的 Q[status][action]，具体逻辑可以参考<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/2_Q_Learning_maze/RL_brain.py#L40">莫烦大佬的样例</a>。个人理解这种更新逻辑不一定最好，只是为了得到最合理的Q表的一种手段。</p>
<h2>Saras</h2>
<p>Saras 和 Q-learning 比较类似，区别在于，Q-learning 计算实际Q值时，取的是使value最大的action，但实际执行时，下次动作不一定是这个action（因为要保证探索性，下次action有一定的随机性），而对于 Saras，选择的是下次实际动作的action，而不是最大的action，这样就能更好地保证随机性，对需要结果多样性的场景来说更加合适。</p>
<p><strong>Sarsa(lambda)</strong></p>
<p>默认的场景里，当前的状态离最优状态都很远，只有等第一次遇到最优态的时候才能获取到收益，才能真正更新Q值，那么这时候要更新前面多少步的status对应的Q值呢？lambda 就是起这个作用的，lambda 范围[0, 1]，取0时只更新当前步骤的Q值，取1时更新从初始状态到当前状态的所有Q值，当然距离当前状态越远，更新的权值就越小。</p>
<h2>Deep Q Network</h2>
<p>现实很多情况中 status 的状态是无法枚举的，于是就有了 DQN，使用隐式向量作为 status 的表征，Q 表用神经网络模型来替换，从而把Q-learning 和 神经网络融合在一起，解决了 status 无法枚举的问题。DQN 主要有两点值得提一下，1）历史的 Q 表学习样本可以累积，也可以重复使用，累积一段时间之后再去更新模型。2）DQN 造了两个结构一样的模型，分别叫做估计模型和现实模型，估计模型参数不断更新，现实模型隔一段时间会同步估计模型的参数。现实模型的输入的是下一步的status，占有先机，因此不需要持续更新也能创造比较准确率的Q值。原文中交替更新的两个相同结构的模型是DQN的一大亮点，但我还无法理解。。总觉得一起更新的话效果会更好一点。</p>
<p>Double DQN：使用 Q_max_predict (s, a) 来替代 Q_max_target (s, a) ，防止实际结果过大，加快收敛速度。ps：这不就是说明 predict 模型要比 target 模型更好嘛！</p>
<p>Prioritized Experience Replay (DQN)：跟 Sarsa(lambda) 的作用比较类似，能获取到收益的步骤很少，因此要高效利用这个步骤，DQN中，会根据误差（Q预估-Q收益）对样本进行带权采样，当前步骤获取到收益时，预估的误差会很大，那被采样的概率就很高。</p>
<p>Dueling DQN：把Q值分成两部分，status 本身固有的值 和 action-status 的增益。目的是为了让Q值更加符合实际的场景。</p>
<h2>Policy Gradients</h2>
<p>Q-learning 中 action 取值都是离散型的，Policy Gradients 就是适配 action 是连续取值的情况。</p>
<p>这样每次的学习目标就不再是误差，而是每个action对应的 收益了。</p>
<h2>进阶</h2>
<p>Actor Critic：将环境反馈也用神经网络替代，解决  Policy Gradients 只能回合更新，而不能单步更新的问题。</p>
<p>soft-Actor Critic：让Q模型不只关注收益，也关注输出值的熵，提高模型的探索能力。</p>
<p>已经越来越像生成对抗网络了。</p>
<p>很有意思的案例，回头再搞一搞: <a href="https://github.com/Farama-Foundation/Gymnasium">https://github.com/Farama-Foundation/Gymnasium</a></p>
<h2>总结</h2>
<p>强化学习的优势在于，面对业务场景，只要能把场景的状态和验证机制描述清楚，就能利用多次交互来获得最优的模型效果。比如 AlphaGo。还有其他可行的场景吗？比如自动驾驶、机器人、游戏、数学推理。很值得探索。</p>
<p>这也特别贴合人的学习过程。现在文本、视觉、语音等各个小模块的效果已经比较成熟，缺点是解释性不够，用一个无法解释的模型来达成AGI几乎是不现实的，一个可能的解决办法是，用一个可解释、可控的框架把不同的功能模型组合起来，从而达成AGI。这个可解释的框架，会是强化学习吗？</p>
<h2>参考</h2>
<p><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">莫烦教程</a></p>
<p><a href="https://paperswithcode.com/task/decision-making">推理 sota</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/52526801">Soft Actor-Critic论文笔记</a></p>
<p><a href="https://rl-book.com/applications/">rl-book-Applications</a></p>
