<h2><a href="https://github.com/wa008/wa008.github.io/blob/master/blog/_posts/2022-06-26-模型解释性-PDP.md">仓库源文</a>，<a href="https://wa008.github.io/2022/06/26/模型解释性-PDP">站点原文</a></h2>
<hr/>
<p>layout: default</p>
<h2>title: 模型解释性-PDP</h2>
<h1>介绍</h1>
<h3>理想定义</h3>
<p>PDP(Partial Dependence Plot) 局部依赖图，定义单特征和目标之间的关系，即函数 $\hat{f}_X(x_S)$
$$
\hat{f}_S(x<em>S)=E</em>{X_C}[\hat{f}(x_S,X_C)]=\int{\hat{f}(x_S,X_C)dP(X_C)}
$$
$x_S$ 标识当前特征 $S$ 的特征值，$X<em>C$ 表示出了 $S$ 以外的其他特征，$E</em>{X_C}$ 表示给定 $X_S$ 时，模型 $\hat{f}$ 对特征 $X_C$ 的期望。</p>
<h3>约束在特定的数据集上</h3>
<p>求期望需要枚举 $X_C$，但枚举所有值算力消耗过高，而且某些特征取值的组合不一定有意义，因此取特定数据集中的特征值，模型
$$
\hat{f}_S(X<em>S)=\frac{1}{n}\sum</em>{i=1}^n{\hat{f}(x_S, X_C^{(i)})}
$$
当分析的单特征类型为类别型特征，直接看不同取值即可。
至此，就可以根据PDP对模型的单个特征做出人为可理解的解释。</p>
<h3>单值衡量特征重要性</h3>
<p>有了PDP的特征-结果关系函数 $\hat{y}(x)$ 之后，对数值型特征，可以将方差作为特征重要性的衡量。
对类别型特征呢？可以将 $\frac{max(f(x)) - min(f(x))}{4}$ 作为方差的近似。因为在正太分布中，95%的数据都满足  $x {\pm} 2 * var = mean$</p>
<h1>优点</h1>
<p>直观
容易应用
适用于任何黑盒模型</p>
<h1>缺点</h1>
<p>PDP只能做 &lt;=2 个特征的解释，严格说这不是方法的缺点，而是人类的理解能力只能在 &lt;=2 维度上
PDP依赖数据分布，可以加个散点图来解决
有特征独立的假设
某些影响可能会被隐藏：多类数据特征解释性相反，合并在一起就会变现为特征没有作用。其实和特征独立的假设是一个问题。</p>
<h1>应用</h1>
<p><code>scikit-learn.PDPBox</code></p>
<h1>参考</h1>
<p><a href="https://christophm.github.io/interpretable-ml-book/pdp.html">https://christophm.github.io/interpretable-ml-book/pdp.html</a></p>
