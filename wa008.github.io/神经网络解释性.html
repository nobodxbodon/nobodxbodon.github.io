<h2><a href="https://github.com/wa008/wa008.github.io/blob/master/blog/_posts/2022-11-06-神经网络解释性.md">仓库源文</a>，<a href="https://wa008.github.io/2022/11/06/神经网络解释性">站点原文</a></h2>
<hr/>
<p>layout: default</p>
<h2>title: 神经网络解释性</h2>
<p>列举几个解释神经网络的方法。</p>
<h1>Learned Features</h1>
<p>主要针对图像领域，对图像的不同层、channel、unit 进行可视化，解释模型的每个模块分别学到了什么。</p>
<h1>Pixel Attribution (Saliency Maps)</h1>
<p>1、基于模型梯度进行解释</p>
<p>2、对输入（图片像素）添加扰动，观察输出结果的变化</p>
<p>作者建议：尽量少发明新的方法，多关注如何验证各个解释性的好坏。</p>
<h1>Detecting Concepts</h1>
<p>利用模型中间层数据，做一些可解释性的检测，具体没有太看懂。</p>
<p><a href="https://arxiv.org/abs/1711.11279">原论文</a></p>
<h1>Adversarial Examples</h1>
<p>通过生成对抗样本发现模型的缺点。主要在图像领域用的比较多。</p>
<p>对抗样本和实际的应用场景相关性较大，场景是什么，就在对应的场景上提供更多对抗样本，提高对应的解释能力。比如无人驾驶、垃圾邮件检测等等。</p>
<h1>Influential Instances</h1>
<p>剔除一个样本，重新训练模型，看模型预测指标的变化，归因样本和模型的关系。</p>
<p>只使用对模型重要性高的样本做训练。</p>
<p>重训模型成本太高，文中提到针对"有梯度"的模型，也可以提前预估样本剔除对模型参数的影响，从而直接微调模型参数，不需要重训模型，减小该部分的成本。</p>
<h1>小结</h1>
<p>模型解释性整体思路</p>
<ol>
<li>剔除/添加 某个 样本/特征，从而挑选重要的特征/样本，或者观察特征/样本对模型的重要性。</li>
<li>通过模型梯度，来解释</li>
<li>通过可视化模型的中间层，解释模型。</li>
</ol>
<p>但天然存在的一个问题是，模型足够大的时候，这些变化都很微弱，很难观测到比较置信的解释性。</p>
<p>一个可行的思路是将模型参数从小往大做，每一步充分保证解释性，比如词向量，可以用大量的数据集充分验证其效果，从而保证解释。</p>
<p>解释性一个重要的作用是为了辅助达成AGI(通用人工智能)，当然或许可以创造其他解释性更好的技术(非神经网络)实现AGI，科学家们加油！</p>
<p>参考：<a href="https://christophm.github.io/interpretable-ml-book/pixel-attribution.html">https://christophm.github.io/interpretable-ml-book/pixel-attribution.html</a></p>
